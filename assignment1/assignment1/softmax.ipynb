{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.408017\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *Fill this in* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.570945 analytic: 0.570945, relative error: 1.115094e-07\n",
      "numerical: -1.617389 analytic: -1.617389, relative error: 7.358177e-10\n",
      "numerical: -2.535233 analytic: -2.535233, relative error: 1.014519e-08\n",
      "numerical: -1.742513 analytic: -1.742513, relative error: 7.894206e-10\n",
      "numerical: 2.575943 analytic: 2.575943, relative error: 2.237181e-08\n",
      "numerical: 1.560668 analytic: 1.560668, relative error: 2.110130e-08\n",
      "numerical: -1.992912 analytic: -1.992912, relative error: 1.489131e-08\n",
      "numerical: 0.184325 analytic: 0.184325, relative error: 8.159676e-08\n",
      "numerical: 2.360323 analytic: 2.360322, relative error: 2.533928e-08\n",
      "numerical: -4.511496 analytic: -4.511496, relative error: 1.030911e-08\n",
      "numerical: -0.733864 analytic: -0.733864, relative error: 2.068902e-08\n",
      "numerical: 1.081258 analytic: 1.081258, relative error: 5.425535e-08\n",
      "numerical: 0.994804 analytic: 0.994804, relative error: 3.574164e-08\n",
      "numerical: -2.522186 analytic: -2.522186, relative error: 3.771767e-09\n",
      "numerical: 1.702662 analytic: 1.702662, relative error: 5.792814e-09\n",
      "numerical: 2.623360 analytic: 2.623359, relative error: 1.319827e-08\n",
      "numerical: -4.459079 analytic: -4.459079, relative error: 1.808729e-08\n",
      "numerical: 0.006143 analytic: 0.006143, relative error: 8.173825e-09\n",
      "numerical: 3.338153 analytic: 3.338153, relative error: 8.665260e-09\n",
      "numerical: 1.058359 analytic: 1.058359, relative error: 4.287892e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.408017e+00 computed in 0.093786s\n",
      "vectorized loss: 2.408017e+00 computed in 0.328216s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 465.141624\n",
      "iteration 100 / 1500: loss 71.764337\n",
      "iteration 200 / 1500: loss 12.631763\n",
      "iteration 300 / 1500: loss 3.638263\n",
      "iteration 400 / 1500: loss 2.290522\n",
      "iteration 500 / 1500: loss 2.059245\n",
      "iteration 600 / 1500: loss 2.056500\n",
      "iteration 700 / 1500: loss 2.035948\n",
      "iteration 800 / 1500: loss 2.021227\n",
      "iteration 900 / 1500: loss 2.074416\n",
      "iteration 1000 / 1500: loss 2.021201\n",
      "iteration 1100 / 1500: loss 2.042824\n",
      "iteration 1200 / 1500: loss 2.048867\n",
      "iteration 1300 / 1500: loss 2.072608\n",
      "iteration 1400 / 1500: loss 2.074004\n",
      "iteration 0 / 1500: loss 413.867317\n",
      "iteration 100 / 1500: loss 76.897053\n",
      "iteration 200 / 1500: loss 15.601142\n",
      "iteration 300 / 1500: loss 4.523832\n",
      "iteration 400 / 1500: loss 2.467960\n",
      "iteration 500 / 1500: loss 2.103795\n",
      "iteration 600 / 1500: loss 2.054970\n",
      "iteration 700 / 1500: loss 2.045260\n",
      "iteration 800 / 1500: loss 2.050544\n",
      "iteration 900 / 1500: loss 1.967916\n",
      "iteration 1000 / 1500: loss 2.005155\n",
      "iteration 1100 / 1500: loss 2.010484\n",
      "iteration 1200 / 1500: loss 2.080533\n",
      "iteration 1300 / 1500: loss 2.061550\n",
      "iteration 1400 / 1500: loss 2.081251\n",
      "iteration 0 / 1500: loss 558.800627\n",
      "iteration 100 / 1500: loss 59.302853\n",
      "iteration 200 / 1500: loss 7.926625\n",
      "iteration 300 / 1500: loss 2.713968\n",
      "iteration 400 / 1500: loss 2.129985\n",
      "iteration 500 / 1500: loss 2.044036\n",
      "iteration 600 / 1500: loss 2.103885\n",
      "iteration 700 / 1500: loss 2.068805\n",
      "iteration 800 / 1500: loss 2.119068\n",
      "iteration 900 / 1500: loss 2.056265\n",
      "iteration 1000 / 1500: loss 2.117402\n",
      "iteration 1100 / 1500: loss 1.992017\n",
      "iteration 1200 / 1500: loss 2.086915\n",
      "iteration 1300 / 1500: loss 2.059089\n",
      "iteration 1400 / 1500: loss 2.087551\n",
      "iteration 0 / 1500: loss 1531.691885\n",
      "iteration 100 / 1500: loss 4.792443\n",
      "iteration 200 / 1500: loss 2.146307\n",
      "iteration 300 / 1500: loss 2.174615\n",
      "iteration 400 / 1500: loss 2.176429\n",
      "iteration 500 / 1500: loss 2.164530\n",
      "iteration 600 / 1500: loss 2.151890\n",
      "iteration 700 / 1500: loss 2.108727\n",
      "iteration 800 / 1500: loss 2.129471\n",
      "iteration 900 / 1500: loss 2.109256\n",
      "iteration 1000 / 1500: loss 2.158574\n",
      "iteration 1100 / 1500: loss 2.167375\n",
      "iteration 1200 / 1500: loss 2.110204\n",
      "iteration 1300 / 1500: loss 2.160641\n",
      "iteration 1400 / 1500: loss 2.148132\n",
      "iteration 0 / 1500: loss 1476.172030\n",
      "iteration 100 / 1500: loss 4.931485\n",
      "iteration 200 / 1500: loss 2.157821\n",
      "iteration 300 / 1500: loss 2.100538\n",
      "iteration 400 / 1500: loss 2.115541\n",
      "iteration 500 / 1500: loss 2.127270\n",
      "iteration 600 / 1500: loss 2.118704\n",
      "iteration 700 / 1500: loss 2.121213\n",
      "iteration 800 / 1500: loss 2.156906\n",
      "iteration 900 / 1500: loss 2.113404\n",
      "iteration 1000 / 1500: loss 2.116068\n",
      "iteration 1100 / 1500: loss 2.170677\n",
      "iteration 1200 / 1500: loss 2.165802\n",
      "iteration 1300 / 1500: loss 2.169228\n",
      "iteration 1400 / 1500: loss 2.108244\n",
      "iteration 0 / 1500: loss 803.812902\n",
      "iteration 100 / 1500: loss 30.108266\n",
      "iteration 200 / 1500: loss 3.059862\n",
      "iteration 300 / 1500: loss 2.155097\n",
      "iteration 400 / 1500: loss 2.087212\n",
      "iteration 500 / 1500: loss 2.011732\n",
      "iteration 600 / 1500: loss 2.103937\n",
      "iteration 700 / 1500: loss 2.052337\n",
      "iteration 800 / 1500: loss 2.077262\n",
      "iteration 900 / 1500: loss 2.072511\n",
      "iteration 1000 / 1500: loss 2.062512\n",
      "iteration 1100 / 1500: loss 2.110440\n",
      "iteration 1200 / 1500: loss 2.071244\n",
      "iteration 1300 / 1500: loss 2.130657\n",
      "iteration 1400 / 1500: loss 2.090432\n",
      "iteration 0 / 1500: loss 981.271114\n",
      "iteration 100 / 1500: loss 19.496857\n",
      "iteration 200 / 1500: loss 2.411979\n",
      "iteration 300 / 1500: loss 2.113871\n",
      "iteration 400 / 1500: loss 2.095828\n",
      "iteration 500 / 1500: loss 2.101199\n",
      "iteration 600 / 1500: loss 2.116934\n",
      "iteration 700 / 1500: loss 2.115711\n",
      "iteration 800 / 1500: loss 2.050027\n",
      "iteration 900 / 1500: loss 2.117803\n",
      "iteration 1000 / 1500: loss 2.160727\n",
      "iteration 1100 / 1500: loss 2.122259\n",
      "iteration 1200 / 1500: loss 2.106027\n",
      "iteration 1300 / 1500: loss 2.131785\n",
      "iteration 1400 / 1500: loss 2.098068\n",
      "iteration 0 / 1500: loss 1067.341413\n",
      "iteration 100 / 1500: loss 15.069890\n",
      "iteration 200 / 1500: loss 2.191428\n",
      "iteration 300 / 1500: loss 2.082541\n",
      "iteration 400 / 1500: loss 2.129001\n",
      "iteration 500 / 1500: loss 2.147363\n",
      "iteration 600 / 1500: loss 2.117434\n",
      "iteration 700 / 1500: loss 2.058464\n",
      "iteration 800 / 1500: loss 2.081382\n",
      "iteration 900 / 1500: loss 2.143466\n",
      "iteration 1000 / 1500: loss 2.121441\n",
      "iteration 1100 / 1500: loss 2.081202\n",
      "iteration 1200 / 1500: loss 2.051437\n",
      "iteration 1300 / 1500: loss 2.094599\n",
      "iteration 1400 / 1500: loss 2.172487\n",
      "iteration 0 / 1500: loss 1468.505863\n",
      "iteration 100 / 1500: loss 5.655125\n",
      "iteration 200 / 1500: loss 2.141083\n",
      "iteration 300 / 1500: loss 2.164330\n",
      "iteration 400 / 1500: loss 2.125501\n",
      "iteration 500 / 1500: loss 2.167625\n",
      "iteration 600 / 1500: loss 2.150519\n",
      "iteration 700 / 1500: loss 2.123930\n",
      "iteration 800 / 1500: loss 2.124503\n",
      "iteration 900 / 1500: loss 2.165216\n",
      "iteration 1000 / 1500: loss 2.122662\n",
      "iteration 1100 / 1500: loss 2.126240\n",
      "iteration 1200 / 1500: loss 2.166110\n",
      "iteration 1300 / 1500: loss 2.127693\n",
      "iteration 1400 / 1500: loss 2.098886\n",
      "iteration 0 / 1500: loss 475.388296\n",
      "iteration 100 / 1500: loss 70.253089\n",
      "iteration 200 / 1500: loss 11.998369\n",
      "iteration 300 / 1500: loss 3.479369\n",
      "iteration 400 / 1500: loss 2.311512\n",
      "iteration 500 / 1500: loss 2.113706\n",
      "iteration 600 / 1500: loss 2.039235\n",
      "iteration 700 / 1500: loss 2.066703\n",
      "iteration 800 / 1500: loss 2.074642\n",
      "iteration 900 / 1500: loss 2.062712\n",
      "iteration 1000 / 1500: loss 1.973221\n",
      "iteration 1100 / 1500: loss 2.084453\n",
      "iteration 1200 / 1500: loss 2.113131\n",
      "iteration 1300 / 1500: loss 2.052445\n",
      "iteration 1400 / 1500: loss 2.043684\n",
      "iteration 0 / 1500: loss 330.467522\n",
      "iteration 100 / 1500: loss 88.029723\n",
      "iteration 200 / 1500: loss 24.622651\n",
      "iteration 300 / 1500: loss 7.990301\n",
      "iteration 400 / 1500: loss 3.594275\n",
      "iteration 500 / 1500: loss 2.341804\n",
      "iteration 600 / 1500: loss 2.069429\n",
      "iteration 700 / 1500: loss 2.000370\n",
      "iteration 800 / 1500: loss 2.011130\n",
      "iteration 900 / 1500: loss 1.993508\n",
      "iteration 1000 / 1500: loss 2.034797\n",
      "iteration 1100 / 1500: loss 1.993175\n",
      "iteration 1200 / 1500: loss 2.017955\n",
      "iteration 1300 / 1500: loss 1.974969\n",
      "iteration 1400 / 1500: loss 2.021067\n",
      "iteration 0 / 1500: loss 360.921178\n",
      "iteration 100 / 1500: loss 84.497704\n",
      "iteration 200 / 1500: loss 21.197783\n",
      "iteration 300 / 1500: loss 6.456639\n",
      "iteration 400 / 1500: loss 3.000802\n",
      "iteration 500 / 1500: loss 2.251843\n",
      "iteration 600 / 1500: loss 2.082646\n",
      "iteration 700 / 1500: loss 2.045714\n",
      "iteration 800 / 1500: loss 2.063317\n",
      "iteration 900 / 1500: loss 2.007285\n",
      "iteration 1000 / 1500: loss 2.025908\n",
      "iteration 1100 / 1500: loss 2.051371\n",
      "iteration 1200 / 1500: loss 1.991452\n",
      "iteration 1300 / 1500: loss 2.003733\n",
      "iteration 1400 / 1500: loss 1.950004\n",
      "iteration 0 / 1500: loss 648.820130\n",
      "iteration 100 / 1500: loss 46.165767\n",
      "iteration 200 / 1500: loss 5.106922\n",
      "iteration 300 / 1500: loss 2.268772\n",
      "iteration 400 / 1500: loss 2.043368\n",
      "iteration 500 / 1500: loss 2.059824\n",
      "iteration 600 / 1500: loss 2.044856\n",
      "iteration 700 / 1500: loss 2.015312\n",
      "iteration 800 / 1500: loss 2.010152\n",
      "iteration 900 / 1500: loss 2.031830\n",
      "iteration 1000 / 1500: loss 2.098366\n",
      "iteration 1100 / 1500: loss 2.083625\n",
      "iteration 1200 / 1500: loss 2.003991\n",
      "iteration 1300 / 1500: loss 2.099890\n",
      "iteration 1400 / 1500: loss 2.106072\n",
      "iteration 0 / 1500: loss 401.414516\n",
      "iteration 100 / 1500: loss 80.144210\n",
      "iteration 200 / 1500: loss 17.378998\n",
      "iteration 300 / 1500: loss 5.092360\n",
      "iteration 400 / 1500: loss 2.574451\n",
      "iteration 500 / 1500: loss 2.154522\n",
      "iteration 600 / 1500: loss 2.063319\n",
      "iteration 700 / 1500: loss 2.017688\n",
      "iteration 800 / 1500: loss 2.059727\n",
      "iteration 900 / 1500: loss 1.981397\n",
      "iteration 1000 / 1500: loss 2.020866\n",
      "iteration 1100 / 1500: loss 1.991595\n",
      "iteration 1200 / 1500: loss 2.006545\n",
      "iteration 1300 / 1500: loss 2.031410\n",
      "iteration 1400 / 1500: loss 2.018729\n",
      "iteration 0 / 1500: loss 1088.897499\n",
      "iteration 100 / 1500: loss 13.956724\n",
      "iteration 200 / 1500: loss 2.216593\n",
      "iteration 300 / 1500: loss 2.137513\n",
      "iteration 400 / 1500: loss 2.116234\n",
      "iteration 500 / 1500: loss 2.144210\n",
      "iteration 600 / 1500: loss 2.162606\n",
      "iteration 700 / 1500: loss 2.154637\n",
      "iteration 800 / 1500: loss 2.126417\n",
      "iteration 900 / 1500: loss 2.108143\n",
      "iteration 1000 / 1500: loss 2.173867\n",
      "iteration 1100 / 1500: loss 2.100202\n",
      "iteration 1200 / 1500: loss 2.090231\n",
      "iteration 1300 / 1500: loss 2.071784\n",
      "iteration 1400 / 1500: loss 2.137135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 386.538409\n",
      "iteration 100 / 1500: loss 80.194411\n",
      "iteration 200 / 1500: loss 17.904511\n",
      "iteration 300 / 1500: loss 5.344200\n",
      "iteration 400 / 1500: loss 2.657766\n",
      "iteration 500 / 1500: loss 2.135625\n",
      "iteration 600 / 1500: loss 2.061461\n",
      "iteration 700 / 1500: loss 2.040541\n",
      "iteration 800 / 1500: loss 2.042464\n",
      "iteration 900 / 1500: loss 1.963175\n",
      "iteration 1000 / 1500: loss 2.029368\n",
      "iteration 1100 / 1500: loss 2.036958\n",
      "iteration 1200 / 1500: loss 2.046577\n",
      "iteration 1300 / 1500: loss 1.996702\n",
      "iteration 1400 / 1500: loss 2.083128\n",
      "iteration 0 / 1500: loss 1252.038466\n",
      "iteration 100 / 1500: loss 9.021513\n",
      "iteration 200 / 1500: loss 2.140554\n",
      "iteration 300 / 1500: loss 2.169614\n",
      "iteration 400 / 1500: loss 2.107457\n",
      "iteration 500 / 1500: loss 2.124314\n",
      "iteration 600 / 1500: loss 2.141996\n",
      "iteration 700 / 1500: loss 2.123387\n",
      "iteration 800 / 1500: loss 2.074613\n",
      "iteration 900 / 1500: loss 2.145805\n",
      "iteration 1000 / 1500: loss 2.090773\n",
      "iteration 1100 / 1500: loss 2.077120\n",
      "iteration 1200 / 1500: loss 2.129850\n",
      "iteration 1300 / 1500: loss 2.103285\n",
      "iteration 1400 / 1500: loss 2.145961\n",
      "iteration 0 / 1500: loss 1201.197428\n",
      "iteration 100 / 1500: loss 10.273220\n",
      "iteration 200 / 1500: loss 2.154556\n",
      "iteration 300 / 1500: loss 2.144822\n",
      "iteration 400 / 1500: loss 2.122863\n",
      "iteration 500 / 1500: loss 2.119400\n",
      "iteration 600 / 1500: loss 2.106045\n",
      "iteration 700 / 1500: loss 2.112166\n",
      "iteration 800 / 1500: loss 2.098863\n",
      "iteration 900 / 1500: loss 2.146769\n",
      "iteration 1000 / 1500: loss 2.116389\n",
      "iteration 1100 / 1500: loss 2.090436\n",
      "iteration 1200 / 1500: loss 2.117744\n",
      "iteration 1300 / 1500: loss 2.116811\n",
      "iteration 1400 / 1500: loss 2.149676\n",
      "iteration 0 / 1500: loss 383.334241\n",
      "iteration 100 / 1500: loss 80.060234\n",
      "iteration 200 / 1500: loss 17.992048\n",
      "iteration 300 / 1500: loss 5.358383\n",
      "iteration 400 / 1500: loss 2.676868\n",
      "iteration 500 / 1500: loss 2.130549\n",
      "iteration 600 / 1500: loss 2.037644\n",
      "iteration 700 / 1500: loss 2.008293\n",
      "iteration 800 / 1500: loss 2.022860\n",
      "iteration 900 / 1500: loss 2.080307\n",
      "iteration 1000 / 1500: loss 2.084737\n",
      "iteration 1100 / 1500: loss 1.994806\n",
      "iteration 1200 / 1500: loss 1.998262\n",
      "iteration 1300 / 1500: loss 1.984841\n",
      "iteration 1400 / 1500: loss 2.059200\n",
      "iteration 0 / 1500: loss 1282.066586\n",
      "iteration 100 / 1500: loss 8.618667\n",
      "iteration 200 / 1500: loss 2.148115\n",
      "iteration 300 / 1500: loss 2.126788\n",
      "iteration 400 / 1500: loss 2.152081\n",
      "iteration 500 / 1500: loss 2.131664\n",
      "iteration 600 / 1500: loss 2.148447\n",
      "iteration 700 / 1500: loss 2.073311\n",
      "iteration 800 / 1500: loss 2.175894\n",
      "iteration 900 / 1500: loss 2.117362\n",
      "iteration 1000 / 1500: loss 2.094041\n",
      "iteration 1100 / 1500: loss 2.101327\n",
      "iteration 1200 / 1500: loss 2.106412\n",
      "iteration 1300 / 1500: loss 2.137952\n",
      "iteration 1400 / 1500: loss 2.168017\n",
      "iteration 0 / 1500: loss 466.357756\n",
      "iteration 100 / 1500: loss 376.605132\n",
      "iteration 200 / 1500: loss 304.611330\n",
      "iteration 300 / 1500: loss 246.441787\n",
      "iteration 400 / 1500: loss 199.530353\n",
      "iteration 500 / 1500: loss 161.878501\n",
      "iteration 600 / 1500: loss 131.111334\n",
      "iteration 700 / 1500: loss 106.629150\n",
      "iteration 800 / 1500: loss 86.462170\n",
      "iteration 900 / 1500: loss 70.318060\n",
      "iteration 1000 / 1500: loss 57.299990\n",
      "iteration 1100 / 1500: loss 46.654107\n",
      "iteration 1200 / 1500: loss 38.099531\n",
      "iteration 1300 / 1500: loss 31.226663\n",
      "iteration 1400 / 1500: loss 25.588893\n",
      "iteration 0 / 1500: loss 414.860263\n",
      "iteration 100 / 1500: loss 342.209223\n",
      "iteration 200 / 1500: loss 282.707458\n",
      "iteration 300 / 1500: loss 233.455638\n",
      "iteration 400 / 1500: loss 192.967407\n",
      "iteration 500 / 1500: loss 159.505879\n",
      "iteration 600 / 1500: loss 132.053845\n",
      "iteration 700 / 1500: loss 109.422418\n",
      "iteration 800 / 1500: loss 90.750118\n",
      "iteration 900 / 1500: loss 75.169053\n",
      "iteration 1000 / 1500: loss 62.517539\n",
      "iteration 1100 / 1500: loss 51.854662\n",
      "iteration 1200 / 1500: loss 43.170754\n",
      "iteration 1300 / 1500: loss 35.966611\n",
      "iteration 1400 / 1500: loss 30.049901\n",
      "iteration 0 / 1500: loss 557.985936\n",
      "iteration 100 / 1500: loss 431.963026\n",
      "iteration 200 / 1500: loss 334.713843\n",
      "iteration 300 / 1500: loss 259.737223\n",
      "iteration 400 / 1500: loss 201.284713\n",
      "iteration 500 / 1500: loss 156.363228\n",
      "iteration 600 / 1500: loss 121.632691\n",
      "iteration 700 / 1500: loss 94.599403\n",
      "iteration 800 / 1500: loss 73.745076\n",
      "iteration 900 / 1500: loss 57.402667\n",
      "iteration 1000 / 1500: loss 45.114649\n",
      "iteration 1100 / 1500: loss 35.400869\n",
      "iteration 1200 / 1500: loss 27.848738\n",
      "iteration 1300 / 1500: loss 22.075601\n",
      "iteration 1400 / 1500: loss 17.487851\n",
      "iteration 0 / 1500: loss 1516.467188\n",
      "iteration 100 / 1500: loss 745.411566\n",
      "iteration 200 / 1500: loss 366.995817\n",
      "iteration 300 / 1500: loss 181.368228\n",
      "iteration 400 / 1500: loss 90.208357\n",
      "iteration 500 / 1500: loss 45.374873\n",
      "iteration 600 / 1500: loss 23.332681\n",
      "iteration 700 / 1500: loss 12.550658\n",
      "iteration 800 / 1500: loss 7.246544\n",
      "iteration 900 / 1500: loss 4.678544\n",
      "iteration 1000 / 1500: loss 3.431962\n",
      "iteration 1100 / 1500: loss 2.719216\n",
      "iteration 1200 / 1500: loss 2.414100\n",
      "iteration 1300 / 1500: loss 2.294155\n",
      "iteration 1400 / 1500: loss 2.230101\n",
      "iteration 0 / 1500: loss 1505.413056\n",
      "iteration 100 / 1500: loss 749.007279\n",
      "iteration 200 / 1500: loss 373.108990\n",
      "iteration 300 / 1500: loss 186.322912\n",
      "iteration 400 / 1500: loss 93.642244\n",
      "iteration 500 / 1500: loss 47.655433\n",
      "iteration 600 / 1500: loss 24.715439\n",
      "iteration 700 / 1500: loss 13.338629\n",
      "iteration 800 / 1500: loss 7.709315\n",
      "iteration 900 / 1500: loss 4.821406\n",
      "iteration 1000 / 1500: loss 3.500594\n",
      "iteration 1100 / 1500: loss 2.810965\n",
      "iteration 1200 / 1500: loss 2.520311\n",
      "iteration 1300 / 1500: loss 2.335122\n",
      "iteration 1400 / 1500: loss 2.214993\n",
      "iteration 0 / 1500: loss 800.905572\n",
      "iteration 100 / 1500: loss 549.792016\n",
      "iteration 200 / 1500: loss 378.159660\n",
      "iteration 300 / 1500: loss 260.037346\n",
      "iteration 400 / 1500: loss 179.048064\n",
      "iteration 500 / 1500: loss 123.703198\n",
      "iteration 600 / 1500: loss 85.498414\n",
      "iteration 700 / 1500: loss 59.340641\n",
      "iteration 800 / 1500: loss 41.363358\n",
      "iteration 900 / 1500: loss 29.141515\n",
      "iteration 1000 / 1500: loss 20.591671\n",
      "iteration 1100 / 1500: loss 14.802597\n",
      "iteration 1200 / 1500: loss 10.839330\n",
      "iteration 1300 / 1500: loss 8.054948\n",
      "iteration 1400 / 1500: loss 6.190788\n",
      "iteration 0 / 1500: loss 958.567449\n",
      "iteration 100 / 1500: loss 610.703579\n",
      "iteration 200 / 1500: loss 389.439113\n",
      "iteration 300 / 1500: loss 248.713468\n",
      "iteration 400 / 1500: loss 159.058119\n",
      "iteration 500 / 1500: loss 102.015581\n",
      "iteration 600 / 1500: loss 65.695180\n",
      "iteration 700 / 1500: loss 42.626592\n",
      "iteration 800 / 1500: loss 27.890085\n",
      "iteration 900 / 1500: loss 18.502008\n",
      "iteration 1000 / 1500: loss 12.576120\n",
      "iteration 1100 / 1500: loss 8.818174\n",
      "iteration 1200 / 1500: loss 6.366675\n",
      "iteration 1300 / 1500: loss 4.837195\n",
      "iteration 1400 / 1500: loss 3.818335\n",
      "iteration 0 / 1500: loss 1083.494845\n",
      "iteration 100 / 1500: loss 661.006500\n",
      "iteration 200 / 1500: loss 404.248742\n",
      "iteration 300 / 1500: loss 247.226134\n",
      "iteration 400 / 1500: loss 151.831750\n",
      "iteration 500 / 1500: loss 93.499710\n",
      "iteration 600 / 1500: loss 57.788838\n",
      "iteration 700 / 1500: loss 36.108204\n",
      "iteration 800 / 1500: loss 22.825198\n",
      "iteration 900 / 1500: loss 14.772320\n",
      "iteration 1000 / 1500: loss 9.856756\n",
      "iteration 1100 / 1500: loss 6.833918\n",
      "iteration 1200 / 1500: loss 4.944817\n",
      "iteration 1300 / 1500: loss 3.818888\n",
      "iteration 1400 / 1500: loss 3.207889\n",
      "iteration 0 / 1500: loss 1454.182980\n",
      "iteration 100 / 1500: loss 743.071249\n",
      "iteration 200 / 1500: loss 380.296388\n",
      "iteration 300 / 1500: loss 194.939540\n",
      "iteration 400 / 1500: loss 100.523573\n",
      "iteration 500 / 1500: loss 52.366337\n",
      "iteration 600 / 1500: loss 27.785993\n",
      "iteration 700 / 1500: loss 15.241720\n",
      "iteration 800 / 1500: loss 8.729430\n",
      "iteration 900 / 1500: loss 5.516926\n",
      "iteration 1000 / 1500: loss 3.847987\n",
      "iteration 1100 / 1500: loss 3.023527\n",
      "iteration 1200 / 1500: loss 2.553992\n",
      "iteration 1300 / 1500: loss 2.433120\n",
      "iteration 1400 / 1500: loss 2.282804\n",
      "iteration 0 / 1500: loss 468.711628\n",
      "iteration 100 / 1500: loss 376.807766\n",
      "iteration 200 / 1500: loss 303.922168\n",
      "iteration 300 / 1500: loss 244.747502\n",
      "iteration 400 / 1500: loss 197.387527\n",
      "iteration 500 / 1500: loss 159.149394\n",
      "iteration 600 / 1500: loss 128.499670\n",
      "iteration 700 / 1500: loss 103.795446\n",
      "iteration 800 / 1500: loss 83.844948\n",
      "iteration 900 / 1500: loss 67.768096\n",
      "iteration 1000 / 1500: loss 54.951569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 1500: loss 44.796487\n",
      "iteration 1200 / 1500: loss 36.354498\n",
      "iteration 1300 / 1500: loss 29.655584\n",
      "iteration 1400 / 1500: loss 24.249464\n",
      "iteration 0 / 1500: loss 328.150049\n",
      "iteration 100 / 1500: loss 281.583609\n",
      "iteration 200 / 1500: loss 242.504576\n",
      "iteration 300 / 1500: loss 208.786560\n",
      "iteration 400 / 1500: loss 180.073477\n",
      "iteration 500 / 1500: loss 154.910228\n",
      "iteration 600 / 1500: loss 133.469389\n",
      "iteration 700 / 1500: loss 115.256714\n",
      "iteration 800 / 1500: loss 99.314385\n",
      "iteration 900 / 1500: loss 85.823883\n",
      "iteration 1000 / 1500: loss 74.087375\n",
      "iteration 1100 / 1500: loss 64.109755\n",
      "iteration 1200 / 1500: loss 55.308014\n",
      "iteration 1300 / 1500: loss 47.857940\n",
      "iteration 1400 / 1500: loss 41.512305\n",
      "iteration 0 / 1500: loss 364.917134\n",
      "iteration 100 / 1500: loss 309.185646\n",
      "iteration 200 / 1500: loss 262.135751\n",
      "iteration 300 / 1500: loss 222.451481\n",
      "iteration 400 / 1500: loss 188.802951\n",
      "iteration 500 / 1500: loss 160.599791\n",
      "iteration 600 / 1500: loss 136.426179\n",
      "iteration 700 / 1500: loss 116.050307\n",
      "iteration 800 / 1500: loss 98.714938\n",
      "iteration 900 / 1500: loss 84.103048\n",
      "iteration 1000 / 1500: loss 71.609759\n",
      "iteration 1100 / 1500: loss 61.118048\n",
      "iteration 1200 / 1500: loss 52.118132\n",
      "iteration 1300 / 1500: loss 44.523336\n",
      "iteration 1400 / 1500: loss 38.042211\n",
      "iteration 0 / 1500: loss 655.181320\n",
      "iteration 100 / 1500: loss 484.926873\n",
      "iteration 200 / 1500: loss 359.479493\n",
      "iteration 300 / 1500: loss 266.343749\n",
      "iteration 400 / 1500: loss 197.564058\n",
      "iteration 500 / 1500: loss 146.716834\n",
      "iteration 600 / 1500: loss 109.125982\n",
      "iteration 700 / 1500: loss 81.280909\n",
      "iteration 800 / 1500: loss 60.720879\n",
      "iteration 900 / 1500: loss 45.329612\n",
      "iteration 1000 / 1500: loss 34.141311\n",
      "iteration 1100 / 1500: loss 25.816170\n",
      "iteration 1200 / 1500: loss 19.626320\n",
      "iteration 1300 / 1500: loss 15.017127\n",
      "iteration 1400 / 1500: loss 11.676075\n",
      "iteration 0 / 1500: loss 399.182589\n",
      "iteration 100 / 1500: loss 332.060819\n",
      "iteration 200 / 1500: loss 276.442335\n",
      "iteration 300 / 1500: loss 230.493820\n",
      "iteration 400 / 1500: loss 192.249376\n",
      "iteration 500 / 1500: loss 160.266972\n",
      "iteration 600 / 1500: loss 133.781308\n",
      "iteration 700 / 1500: loss 111.773530\n",
      "iteration 800 / 1500: loss 93.398983\n",
      "iteration 900 / 1500: loss 78.102672\n",
      "iteration 1000 / 1500: loss 65.259900\n",
      "iteration 1100 / 1500: loss 54.811886\n",
      "iteration 1200 / 1500: loss 45.878448\n",
      "iteration 1300 / 1500: loss 38.459969\n",
      "iteration 1400 / 1500: loss 32.395758\n",
      "iteration 0 / 1500: loss 1104.410427\n",
      "iteration 100 / 1500: loss 666.550534\n",
      "iteration 200 / 1500: loss 402.221333\n",
      "iteration 300 / 1500: loss 243.509463\n",
      "iteration 400 / 1500: loss 147.760131\n",
      "iteration 500 / 1500: loss 89.868378\n",
      "iteration 600 / 1500: loss 55.021270\n",
      "iteration 700 / 1500: loss 33.985723\n",
      "iteration 800 / 1500: loss 21.344587\n",
      "iteration 900 / 1500: loss 13.734214\n",
      "iteration 1000 / 1500: loss 9.133048\n",
      "iteration 1100 / 1500: loss 6.267090\n",
      "iteration 1200 / 1500: loss 4.562940\n",
      "iteration 1300 / 1500: loss 3.747071\n",
      "iteration 1400 / 1500: loss 3.066265\n",
      "iteration 0 / 1500: loss 391.876211\n",
      "iteration 100 / 1500: loss 327.025310\n",
      "iteration 200 / 1500: loss 273.687204\n",
      "iteration 300 / 1500: loss 228.790557\n",
      "iteration 400 / 1500: loss 191.896513\n",
      "iteration 500 / 1500: loss 160.583944\n",
      "iteration 600 / 1500: loss 134.381594\n",
      "iteration 700 / 1500: loss 112.715774\n",
      "iteration 800 / 1500: loss 94.632152\n",
      "iteration 900 / 1500: loss 79.434733\n",
      "iteration 1000 / 1500: loss 66.595161\n",
      "iteration 1100 / 1500: loss 56.019757\n",
      "iteration 1200 / 1500: loss 47.298911\n",
      "iteration 1300 / 1500: loss 39.818802\n",
      "iteration 1400 / 1500: loss 33.612040\n",
      "iteration 0 / 1500: loss 1257.152753\n",
      "iteration 100 / 1500: loss 703.771513\n",
      "iteration 200 / 1500: loss 394.178168\n",
      "iteration 300 / 1500: loss 221.441873\n",
      "iteration 400 / 1500: loss 124.861504\n",
      "iteration 500 / 1500: loss 70.785332\n",
      "iteration 600 / 1500: loss 40.508930\n",
      "iteration 700 / 1500: loss 23.535203\n",
      "iteration 800 / 1500: loss 14.110184\n",
      "iteration 900 / 1500: loss 8.872964\n",
      "iteration 1000 / 1500: loss 5.850629\n",
      "iteration 1100 / 1500: loss 4.242909\n",
      "iteration 1200 / 1500: loss 3.302968\n",
      "iteration 1300 / 1500: loss 2.781726\n",
      "iteration 1400 / 1500: loss 2.547790\n",
      "iteration 0 / 1500: loss 1199.742703\n",
      "iteration 100 / 1500: loss 687.281486\n",
      "iteration 200 / 1500: loss 394.410191\n",
      "iteration 300 / 1500: loss 226.315199\n",
      "iteration 400 / 1500: loss 130.551658\n",
      "iteration 500 / 1500: loss 75.489865\n",
      "iteration 600 / 1500: loss 44.178949\n",
      "iteration 700 / 1500: loss 26.203574\n",
      "iteration 800 / 1500: loss 15.917554\n",
      "iteration 900 / 1500: loss 10.007137\n",
      "iteration 1000 / 1500: loss 6.672830\n",
      "iteration 1100 / 1500: loss 4.684498\n",
      "iteration 1200 / 1500: loss 3.573075\n",
      "iteration 1300 / 1500: loss 2.916678\n",
      "iteration 1400 / 1500: loss 2.611157\n",
      "iteration 0 / 1500: loss 383.378549\n",
      "iteration 100 / 1500: loss 320.029816\n",
      "iteration 200 / 1500: loss 267.817196\n",
      "iteration 300 / 1500: loss 224.459413\n",
      "iteration 400 / 1500: loss 187.778435\n",
      "iteration 500 / 1500: loss 157.357358\n",
      "iteration 600 / 1500: loss 132.134882\n",
      "iteration 700 / 1500: loss 110.635941\n",
      "iteration 800 / 1500: loss 92.947286\n",
      "iteration 900 / 1500: loss 78.180693\n",
      "iteration 1000 / 1500: loss 65.762450\n",
      "iteration 1100 / 1500: loss 55.284411\n",
      "iteration 1200 / 1500: loss 46.643441\n",
      "iteration 1300 / 1500: loss 39.228709\n",
      "iteration 1400 / 1500: loss 33.268413\n",
      "iteration 0 / 1500: loss 1277.251329\n",
      "iteration 100 / 1500: loss 708.293820\n",
      "iteration 200 / 1500: loss 393.143362\n",
      "iteration 300 / 1500: loss 218.470817\n",
      "iteration 400 / 1500: loss 121.879639\n",
      "iteration 500 / 1500: loss 68.460895\n",
      "iteration 600 / 1500: loss 38.820558\n",
      "iteration 700 / 1500: loss 22.515410\n",
      "iteration 800 / 1500: loss 13.344351\n",
      "iteration 900 / 1500: loss 8.397325\n",
      "iteration 1000 / 1500: loss 5.546071\n",
      "iteration 1100 / 1500: loss 4.071202\n",
      "iteration 1200 / 1500: loss 3.177968\n",
      "iteration 1300 / 1500: loss 2.674023\n",
      "iteration 1400 / 1500: loss 2.414295\n",
      "iteration 0 / 1500: loss 466.269965\n",
      "iteration 100 / 1500: loss 399.257667\n",
      "iteration 200 / 1500: loss 341.923460\n",
      "iteration 300 / 1500: loss 292.816219\n",
      "iteration 400 / 1500: loss 250.872737\n",
      "iteration 500 / 1500: loss 214.844636\n",
      "iteration 600 / 1500: loss 184.504682\n",
      "iteration 700 / 1500: loss 158.057695\n",
      "iteration 800 / 1500: loss 135.746813\n",
      "iteration 900 / 1500: loss 116.431569\n",
      "iteration 1000 / 1500: loss 99.850605\n",
      "iteration 1100 / 1500: loss 85.794311\n",
      "iteration 1200 / 1500: loss 73.837627\n",
      "iteration 1300 / 1500: loss 63.443571\n",
      "iteration 1400 / 1500: loss 54.649177\n",
      "iteration 0 / 1500: loss 418.035532\n",
      "iteration 100 / 1500: loss 362.952244\n",
      "iteration 200 / 1500: loss 315.589513\n",
      "iteration 300 / 1500: loss 274.485379\n",
      "iteration 400 / 1500: loss 238.690272\n",
      "iteration 500 / 1500: loss 207.780787\n",
      "iteration 600 / 1500: loss 180.869665\n",
      "iteration 700 / 1500: loss 157.487506\n",
      "iteration 800 / 1500: loss 137.123849\n",
      "iteration 900 / 1500: loss 119.608307\n",
      "iteration 1000 / 1500: loss 104.039203\n",
      "iteration 1100 / 1500: loss 90.797041\n",
      "iteration 1200 / 1500: loss 79.020891\n",
      "iteration 1300 / 1500: loss 69.094477\n",
      "iteration 1400 / 1500: loss 60.298201\n",
      "iteration 0 / 1500: loss 554.466561\n",
      "iteration 100 / 1500: loss 460.108794\n",
      "iteration 200 / 1500: loss 381.688385\n",
      "iteration 300 / 1500: loss 317.166965\n",
      "iteration 400 / 1500: loss 263.358999\n",
      "iteration 500 / 1500: loss 219.071503\n",
      "iteration 600 / 1500: loss 182.109927\n",
      "iteration 700 / 1500: loss 151.306353\n",
      "iteration 800 / 1500: loss 125.901855\n",
      "iteration 900 / 1500: loss 104.777513\n",
      "iteration 1000 / 1500: loss 87.342703\n",
      "iteration 1100 / 1500: loss 72.766290\n",
      "iteration 1200 / 1500: loss 60.724076\n",
      "iteration 1300 / 1500: loss 50.768970\n",
      "iteration 1400 / 1500: loss 42.566450\n",
      "iteration 0 / 1500: loss 1524.926257\n",
      "iteration 100 / 1500: loss 908.194751\n",
      "iteration 200 / 1500: loss 541.591982\n",
      "iteration 300 / 1500: loss 323.049507\n",
      "iteration 400 / 1500: loss 193.134191\n",
      "iteration 500 / 1500: loss 115.756639\n",
      "iteration 600 / 1500: loss 69.838510\n",
      "iteration 700 / 1500: loss 42.414065\n",
      "iteration 800 / 1500: loss 26.120438\n",
      "iteration 900 / 1500: loss 16.470794\n",
      "iteration 1000 / 1500: loss 10.613640\n",
      "iteration 1100 / 1500: loss 7.221509\n",
      "iteration 1200 / 1500: loss 5.146812\n",
      "iteration 1300 / 1500: loss 3.949388\n",
      "iteration 1400 / 1500: loss 3.208713\n",
      "iteration 0 / 1500: loss 1510.934877\n",
      "iteration 100 / 1500: loss 907.510257\n",
      "iteration 200 / 1500: loss 545.269634\n",
      "iteration 300 / 1500: loss 328.054144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 1500: loss 197.550995\n",
      "iteration 500 / 1500: loss 119.402378\n",
      "iteration 600 / 1500: loss 72.538809\n",
      "iteration 700 / 1500: loss 44.338224\n",
      "iteration 800 / 1500: loss 27.448390\n",
      "iteration 900 / 1500: loss 17.325965\n",
      "iteration 1000 / 1500: loss 11.270908\n",
      "iteration 1100 / 1500: loss 7.566033\n",
      "iteration 1200 / 1500: loss 5.421515\n",
      "iteration 1300 / 1500: loss 4.090965\n",
      "iteration 1400 / 1500: loss 3.310174\n",
      "iteration 0 / 1500: loss 823.596987\n",
      "iteration 100 / 1500: loss 625.851513\n",
      "iteration 200 / 1500: loss 475.498422\n",
      "iteration 300 / 1500: loss 361.331254\n",
      "iteration 400 / 1500: loss 275.036707\n",
      "iteration 500 / 1500: loss 209.473586\n",
      "iteration 600 / 1500: loss 159.753395\n",
      "iteration 700 / 1500: loss 121.790926\n",
      "iteration 800 / 1500: loss 92.985679\n",
      "iteration 900 / 1500: loss 71.196878\n",
      "iteration 1000 / 1500: loss 54.516976\n",
      "iteration 1100 / 1500: loss 41.886513\n",
      "iteration 1200 / 1500: loss 32.385171\n",
      "iteration 1300 / 1500: loss 25.093805\n",
      "iteration 1400 / 1500: loss 19.554321\n",
      "iteration 0 / 1500: loss 983.109085\n",
      "iteration 100 / 1500: loss 707.288535\n",
      "iteration 200 / 1500: loss 509.030907\n",
      "iteration 300 / 1500: loss 366.530905\n",
      "iteration 400 / 1500: loss 264.053788\n",
      "iteration 500 / 1500: loss 190.651829\n",
      "iteration 600 / 1500: loss 137.651146\n",
      "iteration 700 / 1500: loss 99.625378\n",
      "iteration 800 / 1500: loss 72.177587\n",
      "iteration 900 / 1500: loss 52.540737\n",
      "iteration 1000 / 1500: loss 38.337426\n",
      "iteration 1100 / 1500: loss 28.167031\n",
      "iteration 1200 / 1500: loss 20.852810\n",
      "iteration 1300 / 1500: loss 15.635289\n",
      "iteration 1400 / 1500: loss 11.760640\n",
      "iteration 0 / 1500: loss 1067.042760\n",
      "iteration 100 / 1500: loss 744.533870\n",
      "iteration 200 / 1500: loss 519.412809\n",
      "iteration 300 / 1500: loss 362.423586\n",
      "iteration 400 / 1500: loss 253.353611\n",
      "iteration 500 / 1500: loss 177.268402\n",
      "iteration 600 / 1500: loss 124.149566\n",
      "iteration 700 / 1500: loss 87.141673\n",
      "iteration 800 / 1500: loss 61.504208\n",
      "iteration 900 / 1500: loss 43.441444\n",
      "iteration 1000 / 1500: loss 30.981749\n",
      "iteration 1100 / 1500: loss 22.270818\n",
      "iteration 1200 / 1500: loss 16.153203\n",
      "iteration 1300 / 1500: loss 11.918544\n",
      "iteration 1400 / 1500: loss 8.950369\n",
      "iteration 0 / 1500: loss 1458.342290\n",
      "iteration 100 / 1500: loss 893.055056\n",
      "iteration 200 / 1500: loss 546.963142\n",
      "iteration 300 / 1500: loss 335.463991\n",
      "iteration 400 / 1500: loss 206.087779\n",
      "iteration 500 / 1500: loss 126.839065\n",
      "iteration 600 / 1500: loss 78.487664\n",
      "iteration 700 / 1500: loss 48.935198\n",
      "iteration 800 / 1500: loss 30.715423\n",
      "iteration 900 / 1500: loss 19.639917\n",
      "iteration 1000 / 1500: loss 12.803729\n",
      "iteration 1100 / 1500: loss 8.654381\n",
      "iteration 1200 / 1500: loss 6.113013\n",
      "iteration 1300 / 1500: loss 4.587467\n",
      "iteration 1400 / 1500: loss 3.603653\n",
      "iteration 0 / 1500: loss 467.239174\n",
      "iteration 100 / 1500: loss 398.611683\n",
      "iteration 200 / 1500: loss 339.884989\n",
      "iteration 300 / 1500: loss 290.259139\n",
      "iteration 400 / 1500: loss 247.869980\n",
      "iteration 500 / 1500: loss 211.821571\n",
      "iteration 600 / 1500: loss 180.995927\n",
      "iteration 700 / 1500: loss 154.680370\n",
      "iteration 800 / 1500: loss 132.426768\n",
      "iteration 900 / 1500: loss 113.243346\n",
      "iteration 1000 / 1500: loss 96.781915\n",
      "iteration 1100 / 1500: loss 83.040932\n",
      "iteration 1200 / 1500: loss 70.999198\n",
      "iteration 1300 / 1500: loss 60.865358\n",
      "iteration 1400 / 1500: loss 52.293593\n",
      "iteration 0 / 1500: loss 326.927294\n",
      "iteration 100 / 1500: loss 292.285998\n",
      "iteration 200 / 1500: loss 261.818841\n",
      "iteration 300 / 1500: loss 234.818745\n",
      "iteration 400 / 1500: loss 210.583098\n",
      "iteration 500 / 1500: loss 189.012297\n",
      "iteration 600 / 1500: loss 169.385334\n",
      "iteration 700 / 1500: loss 152.132503\n",
      "iteration 800 / 1500: loss 136.248663\n",
      "iteration 900 / 1500: loss 122.291926\n",
      "iteration 1000 / 1500: loss 109.858114\n",
      "iteration 1100 / 1500: loss 98.607650\n",
      "iteration 1200 / 1500: loss 88.633193\n",
      "iteration 1300 / 1500: loss 79.467072\n",
      "iteration 1400 / 1500: loss 71.581281\n",
      "iteration 0 / 1500: loss 361.837594\n",
      "iteration 100 / 1500: loss 320.151182\n",
      "iteration 200 / 1500: loss 283.605151\n",
      "iteration 300 / 1500: loss 251.722174\n",
      "iteration 400 / 1500: loss 222.852382\n",
      "iteration 500 / 1500: loss 198.277371\n",
      "iteration 600 / 1500: loss 175.654776\n",
      "iteration 700 / 1500: loss 156.017087\n",
      "iteration 800 / 1500: loss 138.378335\n",
      "iteration 900 / 1500: loss 122.956916\n",
      "iteration 1000 / 1500: loss 109.341784\n",
      "iteration 1100 / 1500: loss 97.055150\n",
      "iteration 1200 / 1500: loss 86.267761\n",
      "iteration 1300 / 1500: loss 76.720668\n",
      "iteration 1400 / 1500: loss 68.237152\n",
      "iteration 0 / 1500: loss 646.972973\n",
      "iteration 100 / 1500: loss 518.699551\n",
      "iteration 200 / 1500: loss 416.594795\n",
      "iteration 300 / 1500: loss 334.477119\n",
      "iteration 400 / 1500: loss 268.812969\n",
      "iteration 500 / 1500: loss 216.012216\n",
      "iteration 600 / 1500: loss 173.735658\n",
      "iteration 700 / 1500: loss 139.964278\n",
      "iteration 800 / 1500: loss 112.626976\n",
      "iteration 900 / 1500: loss 90.910811\n",
      "iteration 1000 / 1500: loss 73.470469\n",
      "iteration 1100 / 1500: loss 59.299227\n",
      "iteration 1200 / 1500: loss 47.862845\n",
      "iteration 1300 / 1500: loss 38.829504\n",
      "iteration 1400 / 1500: loss 31.612800\n",
      "iteration 0 / 1500: loss 394.655286\n",
      "iteration 100 / 1500: loss 345.625895\n",
      "iteration 200 / 1500: loss 302.188890\n",
      "iteration 300 / 1500: loss 264.182588\n",
      "iteration 400 / 1500: loss 231.873763\n",
      "iteration 500 / 1500: loss 202.745988\n",
      "iteration 600 / 1500: loss 177.566915\n",
      "iteration 700 / 1500: loss 155.578475\n",
      "iteration 800 / 1500: loss 136.236651\n",
      "iteration 900 / 1500: loss 119.473123\n",
      "iteration 1000 / 1500: loss 104.742118\n",
      "iteration 1100 / 1500: loss 91.850190\n",
      "iteration 1200 / 1500: loss 80.582790\n",
      "iteration 1300 / 1500: loss 70.844979\n",
      "iteration 1400 / 1500: loss 62.089197\n",
      "iteration 0 / 1500: loss 1087.703769\n",
      "iteration 100 / 1500: loss 753.155331\n",
      "iteration 200 / 1500: loss 520.576009\n",
      "iteration 300 / 1500: loss 360.443744\n",
      "iteration 400 / 1500: loss 249.821127\n",
      "iteration 500 / 1500: loss 173.216680\n",
      "iteration 600 / 1500: loss 120.441444\n",
      "iteration 700 / 1500: loss 83.939176\n",
      "iteration 800 / 1500: loss 58.656487\n",
      "iteration 900 / 1500: loss 41.281460\n",
      "iteration 1000 / 1500: loss 29.110812\n",
      "iteration 1100 / 1500: loss 20.814366\n",
      "iteration 1200 / 1500: loss 15.012194\n",
      "iteration 1300 / 1500: loss 11.043105\n",
      "iteration 1400 / 1500: loss 8.298760\n",
      "iteration 0 / 1500: loss 388.178143\n",
      "iteration 100 / 1500: loss 340.162320\n",
      "iteration 200 / 1500: loss 298.632303\n",
      "iteration 300 / 1500: loss 262.027314\n",
      "iteration 400 / 1500: loss 230.163609\n",
      "iteration 500 / 1500: loss 202.092471\n",
      "iteration 600 / 1500: loss 177.322340\n",
      "iteration 700 / 1500: loss 155.828481\n",
      "iteration 800 / 1500: loss 136.754671\n",
      "iteration 900 / 1500: loss 120.613789\n",
      "iteration 1000 / 1500: loss 105.813868\n",
      "iteration 1100 / 1500: loss 93.125798\n",
      "iteration 1200 / 1500: loss 81.892757\n",
      "iteration 1300 / 1500: loss 72.144701\n",
      "iteration 1400 / 1500: loss 63.549832\n",
      "iteration 0 / 1500: loss 1261.636555\n",
      "iteration 100 / 1500: loss 825.980650\n",
      "iteration 200 / 1500: loss 541.059832\n",
      "iteration 300 / 1500: loss 354.562232\n",
      "iteration 400 / 1500: loss 232.766457\n",
      "iteration 500 / 1500: loss 153.008772\n",
      "iteration 600 / 1500: loss 100.751013\n",
      "iteration 700 / 1500: loss 66.691126\n",
      "iteration 800 / 1500: loss 44.385016\n",
      "iteration 900 / 1500: loss 29.800861\n",
      "iteration 1000 / 1500: loss 20.164936\n",
      "iteration 1100 / 1500: loss 13.987137\n",
      "iteration 1200 / 1500: loss 9.861275\n",
      "iteration 1300 / 1500: loss 7.266673\n",
      "iteration 1400 / 1500: loss 5.442704\n",
      "iteration 0 / 1500: loss 1191.165485\n",
      "iteration 100 / 1500: loss 793.273012\n",
      "iteration 200 / 1500: loss 528.086917\n",
      "iteration 300 / 1500: loss 352.157313\n",
      "iteration 400 / 1500: loss 235.076922\n",
      "iteration 500 / 1500: loss 156.919870\n",
      "iteration 600 / 1500: loss 105.152488\n",
      "iteration 700 / 1500: loss 70.720736\n",
      "iteration 800 / 1500: loss 47.741451\n",
      "iteration 900 / 1500: loss 32.411870\n",
      "iteration 1000 / 1500: loss 22.286927\n",
      "iteration 1100 / 1500: loss 15.575647\n",
      "iteration 1200 / 1500: loss 11.077290\n",
      "iteration 1300 / 1500: loss 7.999686\n",
      "iteration 1400 / 1500: loss 6.022066\n",
      "iteration 0 / 1500: loss 386.430560\n",
      "iteration 100 / 1500: loss 338.988222\n",
      "iteration 200 / 1500: loss 297.553370\n",
      "iteration 300 / 1500: loss 261.425210\n",
      "iteration 400 / 1500: loss 229.483469\n",
      "iteration 500 / 1500: loss 201.821735\n",
      "iteration 600 / 1500: loss 177.558067\n",
      "iteration 700 / 1500: loss 155.915260\n",
      "iteration 800 / 1500: loss 137.323949\n",
      "iteration 900 / 1500: loss 120.742920\n",
      "iteration 1000 / 1500: loss 106.059678\n",
      "iteration 1100 / 1500: loss 93.447369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 82.206401\n",
      "iteration 1300 / 1500: loss 72.416933\n",
      "iteration 1400 / 1500: loss 63.751812\n",
      "iteration 0 / 1500: loss 1276.943487\n",
      "iteration 100 / 1500: loss 829.691296\n",
      "iteration 200 / 1500: loss 539.360399\n",
      "iteration 300 / 1500: loss 351.072029\n",
      "iteration 400 / 1500: loss 228.754151\n",
      "iteration 500 / 1500: loss 149.378256\n",
      "iteration 600 / 1500: loss 97.615947\n",
      "iteration 700 / 1500: loss 64.219220\n",
      "iteration 800 / 1500: loss 42.401956\n",
      "iteration 900 / 1500: loss 28.322956\n",
      "iteration 1000 / 1500: loss 19.141009\n",
      "iteration 1100 / 1500: loss 13.195598\n",
      "iteration 1200 / 1500: loss 9.322232\n",
      "iteration 1300 / 1500: loss 6.810778\n",
      "iteration 1400 / 1500: loss 5.151649\n",
      "iteration 0 / 1500: loss 459.886836\n",
      "iteration 100 / 1500: loss 307.887136\n",
      "iteration 200 / 1500: loss 207.041721\n",
      "iteration 300 / 1500: loss 139.195435\n",
      "iteration 400 / 1500: loss 93.840702\n",
      "iteration 500 / 1500: loss 63.629594\n",
      "iteration 600 / 1500: loss 43.326212\n",
      "iteration 700 / 1500: loss 29.673342\n",
      "iteration 800 / 1500: loss 20.546654\n",
      "iteration 900 / 1500: loss 14.432748\n",
      "iteration 1000 / 1500: loss 10.437581\n",
      "iteration 1100 / 1500: loss 7.633529\n",
      "iteration 1200 / 1500: loss 5.850107\n",
      "iteration 1300 / 1500: loss 4.579648\n",
      "iteration 1400 / 1500: loss 3.783819\n",
      "iteration 0 / 1500: loss 414.098001\n",
      "iteration 100 / 1500: loss 288.701678\n",
      "iteration 200 / 1500: loss 201.636379\n",
      "iteration 300 / 1500: loss 141.173394\n",
      "iteration 400 / 1500: loss 98.851425\n",
      "iteration 500 / 1500: loss 69.454212\n",
      "iteration 600 / 1500: loss 49.089046\n",
      "iteration 700 / 1500: loss 34.790320\n",
      "iteration 800 / 1500: loss 24.891574\n",
      "iteration 900 / 1500: loss 17.989436\n",
      "iteration 1000 / 1500: loss 13.181591\n",
      "iteration 1100 / 1500: loss 9.762767\n",
      "iteration 1200 / 1500: loss 7.453809\n",
      "iteration 1300 / 1500: loss 5.805489\n",
      "iteration 1400 / 1500: loss 4.684024\n",
      "iteration 0 / 1500: loss 559.366197\n",
      "iteration 100 / 1500: loss 345.267009\n",
      "iteration 200 / 1500: loss 214.094248\n",
      "iteration 300 / 1500: loss 132.864304\n",
      "iteration 400 / 1500: loss 83.036333\n",
      "iteration 500 / 1500: loss 52.043255\n",
      "iteration 600 / 1500: loss 32.930772\n",
      "iteration 700 / 1500: loss 21.110514\n",
      "iteration 800 / 1500: loss 13.895423\n",
      "iteration 900 / 1500: loss 9.335114\n",
      "iteration 1000 / 1500: loss 6.567515\n",
      "iteration 1100 / 1500: loss 4.835085\n",
      "iteration 1200 / 1500: loss 3.765559\n",
      "iteration 1300 / 1500: loss 3.061086\n",
      "iteration 1400 / 1500: loss 2.731732\n",
      "iteration 0 / 1500: loss 1559.466197\n",
      "iteration 100 / 1500: loss 409.790963\n",
      "iteration 200 / 1500: loss 108.900813\n",
      "iteration 300 / 1500: loss 30.056872\n",
      "iteration 400 / 1500: loss 9.441341\n",
      "iteration 500 / 1500: loss 4.032888\n",
      "iteration 600 / 1500: loss 2.602221\n",
      "iteration 700 / 1500: loss 2.269500\n",
      "iteration 800 / 1500: loss 2.211879\n",
      "iteration 900 / 1500: loss 2.109911\n",
      "iteration 1000 / 1500: loss 2.155340\n",
      "iteration 1100 / 1500: loss 2.119337\n",
      "iteration 1200 / 1500: loss 2.150887\n",
      "iteration 1300 / 1500: loss 2.163418\n",
      "iteration 1400 / 1500: loss 2.146969\n",
      "iteration 0 / 1500: loss 1521.849139\n",
      "iteration 100 / 1500: loss 408.386447\n",
      "iteration 200 / 1500: loss 110.811958\n",
      "iteration 300 / 1500: loss 31.193023\n",
      "iteration 400 / 1500: loss 9.920193\n",
      "iteration 500 / 1500: loss 4.209379\n",
      "iteration 600 / 1500: loss 2.683627\n",
      "iteration 700 / 1500: loss 2.303320\n",
      "iteration 800 / 1500: loss 2.212706\n",
      "iteration 900 / 1500: loss 2.152408\n",
      "iteration 1000 / 1500: loss 2.144449\n",
      "iteration 1100 / 1500: loss 2.163624\n",
      "iteration 1200 / 1500: loss 2.170043\n",
      "iteration 1300 / 1500: loss 2.167253\n",
      "iteration 1400 / 1500: loss 2.126893\n",
      "iteration 0 / 1500: loss 798.430732\n",
      "iteration 100 / 1500: loss 393.160962\n",
      "iteration 200 / 1500: loss 194.560970\n",
      "iteration 300 / 1500: loss 96.771215\n",
      "iteration 400 / 1500: loss 48.747765\n",
      "iteration 500 / 1500: loss 24.993738\n",
      "iteration 600 / 1500: loss 13.376947\n",
      "iteration 700 / 1500: loss 7.678732\n",
      "iteration 800 / 1500: loss 4.864251\n",
      "iteration 900 / 1500: loss 3.432813\n",
      "iteration 1000 / 1500: loss 2.723469\n",
      "iteration 1100 / 1500: loss 2.486538\n",
      "iteration 1200 / 1500: loss 2.247739\n",
      "iteration 1300 / 1500: loss 2.161597\n",
      "iteration 1400 / 1500: loss 2.138118\n",
      "iteration 0 / 1500: loss 973.640959\n",
      "iteration 100 / 1500: loss 416.513919\n",
      "iteration 200 / 1500: loss 178.965460\n",
      "iteration 300 / 1500: loss 77.628933\n",
      "iteration 400 / 1500: loss 34.334332\n",
      "iteration 500 / 1500: loss 15.956581\n",
      "iteration 600 / 1500: loss 7.974966\n",
      "iteration 700 / 1500: loss 4.619103\n",
      "iteration 800 / 1500: loss 3.163370\n",
      "iteration 900 / 1500: loss 2.549961\n",
      "iteration 1000 / 1500: loss 2.326906\n",
      "iteration 1100 / 1500: loss 2.173691\n",
      "iteration 1200 / 1500: loss 2.094111\n",
      "iteration 1300 / 1500: loss 2.132516\n",
      "iteration 1400 / 1500: loss 2.124404\n",
      "iteration 0 / 1500: loss 1053.811310\n",
      "iteration 100 / 1500: loss 416.669804\n",
      "iteration 200 / 1500: loss 165.500151\n",
      "iteration 300 / 1500: loss 66.553504\n",
      "iteration 400 / 1500: loss 27.536274\n",
      "iteration 500 / 1500: loss 12.167340\n",
      "iteration 600 / 1500: loss 6.008320\n",
      "iteration 700 / 1500: loss 3.654636\n",
      "iteration 800 / 1500: loss 2.696612\n",
      "iteration 900 / 1500: loss 2.393964\n",
      "iteration 1000 / 1500: loss 2.226362\n",
      "iteration 1100 / 1500: loss 2.184866\n",
      "iteration 1200 / 1500: loss 2.092971\n",
      "iteration 1300 / 1500: loss 2.141126\n",
      "iteration 1400 / 1500: loss 2.112627\n",
      "iteration 0 / 1500: loss 1440.184641\n",
      "iteration 100 / 1500: loss 406.670165\n",
      "iteration 200 / 1500: loss 115.962027\n",
      "iteration 300 / 1500: loss 34.201546\n",
      "iteration 400 / 1500: loss 11.139826\n",
      "iteration 500 / 1500: loss 4.708181\n",
      "iteration 600 / 1500: loss 2.811553\n",
      "iteration 700 / 1500: loss 2.320359\n",
      "iteration 800 / 1500: loss 2.174107\n",
      "iteration 900 / 1500: loss 2.142802\n",
      "iteration 1000 / 1500: loss 2.206935\n",
      "iteration 1100 / 1500: loss 2.158437\n",
      "iteration 1200 / 1500: loss 2.112015\n",
      "iteration 1300 / 1500: loss 2.170402\n",
      "iteration 1400 / 1500: loss 2.166608\n",
      "iteration 0 / 1500: loss 473.412796\n",
      "iteration 100 / 1500: loss 313.806152\n",
      "iteration 200 / 1500: loss 209.071304\n",
      "iteration 300 / 1500: loss 139.525242\n",
      "iteration 400 / 1500: loss 93.257780\n",
      "iteration 500 / 1500: loss 62.539541\n",
      "iteration 600 / 1500: loss 42.328429\n",
      "iteration 700 / 1500: loss 28.829698\n",
      "iteration 800 / 1500: loss 19.847563\n",
      "iteration 900 / 1500: loss 13.820641\n",
      "iteration 1000 / 1500: loss 9.878170\n",
      "iteration 1100 / 1500: loss 7.243978\n",
      "iteration 1200 / 1500: loss 5.546283\n",
      "iteration 1300 / 1500: loss 4.360390\n",
      "iteration 1400 / 1500: loss 3.556311\n",
      "iteration 0 / 1500: loss 325.354096\n",
      "iteration 100 / 1500: loss 244.841460\n",
      "iteration 200 / 1500: loss 184.959725\n",
      "iteration 300 / 1500: loss 139.502164\n",
      "iteration 400 / 1500: loss 105.729704\n",
      "iteration 500 / 1500: loss 80.211685\n",
      "iteration 600 / 1500: loss 60.789309\n",
      "iteration 700 / 1500: loss 46.328732\n",
      "iteration 800 / 1500: loss 35.387627\n",
      "iteration 900 / 1500: loss 27.246735\n",
      "iteration 1000 / 1500: loss 20.972778\n",
      "iteration 1100 / 1500: loss 16.261885\n",
      "iteration 1200 / 1500: loss 12.729378\n",
      "iteration 1300 / 1500: loss 10.099671\n",
      "iteration 1400 / 1500: loss 8.131668\n",
      "iteration 0 / 1500: loss 357.952662\n",
      "iteration 100 / 1500: loss 262.359943\n",
      "iteration 200 / 1500: loss 192.796818\n",
      "iteration 300 / 1500: loss 141.732558\n",
      "iteration 400 / 1500: loss 104.424939\n",
      "iteration 500 / 1500: loss 77.074922\n",
      "iteration 600 / 1500: loss 56.973904\n",
      "iteration 700 / 1500: loss 42.355156\n",
      "iteration 800 / 1500: loss 31.701792\n",
      "iteration 900 / 1500: loss 23.634834\n",
      "iteration 1000 / 1500: loss 17.924080\n",
      "iteration 1100 / 1500: loss 13.626917\n",
      "iteration 1200 / 1500: loss 10.659152\n",
      "iteration 1300 / 1500: loss 8.400294\n",
      "iteration 1400 / 1500: loss 6.607194\n",
      "iteration 0 / 1500: loss 656.727400\n",
      "iteration 100 / 1500: loss 372.399848\n",
      "iteration 200 / 1500: loss 211.976171\n",
      "iteration 300 / 1500: loss 121.127433\n",
      "iteration 400 / 1500: loss 69.621388\n",
      "iteration 500 / 1500: loss 40.258375\n",
      "iteration 600 / 1500: loss 23.746540\n",
      "iteration 700 / 1500: loss 14.434876\n",
      "iteration 800 / 1500: loss 9.011175\n",
      "iteration 900 / 1500: loss 6.036131\n",
      "iteration 1000 / 1500: loss 4.352102\n",
      "iteration 1100 / 1500: loss 3.319011\n",
      "iteration 1200 / 1500: loss 2.765221\n",
      "iteration 1300 / 1500: loss 2.493361\n",
      "iteration 1400 / 1500: loss 2.346719\n",
      "iteration 0 / 1500: loss 399.282206\n",
      "iteration 100 / 1500: loss 282.169502\n",
      "iteration 200 / 1500: loss 200.283629\n",
      "iteration 300 / 1500: loss 142.309727\n",
      "iteration 400 / 1500: loss 101.420247\n",
      "iteration 500 / 1500: loss 72.264636\n",
      "iteration 600 / 1500: loss 51.822629\n",
      "iteration 700 / 1500: loss 37.327533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1500: loss 27.040730\n",
      "iteration 900 / 1500: loss 19.700838\n",
      "iteration 1000 / 1500: loss 14.505218\n",
      "iteration 1100 / 1500: loss 10.895844\n",
      "iteration 1200 / 1500: loss 8.347107\n",
      "iteration 1300 / 1500: loss 6.488674\n",
      "iteration 1400 / 1500: loss 5.137226\n",
      "iteration 0 / 1500: loss 1083.137127\n",
      "iteration 100 / 1500: loss 418.664669\n",
      "iteration 200 / 1500: loss 162.734116\n",
      "iteration 300 / 1500: loss 64.045548\n",
      "iteration 400 / 1500: loss 25.939529\n",
      "iteration 500 / 1500: loss 11.262644\n",
      "iteration 600 / 1500: loss 5.656433\n",
      "iteration 700 / 1500: loss 3.511372\n",
      "iteration 800 / 1500: loss 2.655100\n",
      "iteration 900 / 1500: loss 2.322970\n",
      "iteration 1000 / 1500: loss 2.210775\n",
      "iteration 1100 / 1500: loss 2.123310\n",
      "iteration 1200 / 1500: loss 2.135362\n",
      "iteration 1300 / 1500: loss 2.101918\n",
      "iteration 1400 / 1500: loss 2.098697\n",
      "iteration 0 / 1500: loss 384.704082\n",
      "iteration 100 / 1500: loss 274.295439\n",
      "iteration 200 / 1500: loss 195.759427\n",
      "iteration 300 / 1500: loss 140.484851\n",
      "iteration 400 / 1500: loss 100.846646\n",
      "iteration 500 / 1500: loss 72.582592\n",
      "iteration 600 / 1500: loss 52.389643\n",
      "iteration 700 / 1500: loss 37.796737\n",
      "iteration 800 / 1500: loss 27.600711\n",
      "iteration 900 / 1500: loss 20.336330\n",
      "iteration 1000 / 1500: loss 15.120692\n",
      "iteration 1100 / 1500: loss 11.267563\n",
      "iteration 1200 / 1500: loss 8.746038\n",
      "iteration 1300 / 1500: loss 6.731952\n",
      "iteration 1400 / 1500: loss 5.366194\n",
      "iteration 0 / 1500: loss 1251.215341\n",
      "iteration 100 / 1500: loss 420.110864\n",
      "iteration 200 / 1500: loss 141.897968\n",
      "iteration 300 / 1500: loss 48.991424\n",
      "iteration 400 / 1500: loss 17.765458\n",
      "iteration 500 / 1500: loss 7.373062\n",
      "iteration 600 / 1500: loss 3.845383\n",
      "iteration 700 / 1500: loss 2.695974\n",
      "iteration 800 / 1500: loss 2.314828\n",
      "iteration 900 / 1500: loss 2.205601\n",
      "iteration 1000 / 1500: loss 2.177712\n",
      "iteration 1100 / 1500: loss 2.127220\n",
      "iteration 1200 / 1500: loss 2.124746\n",
      "iteration 1300 / 1500: loss 2.127040\n",
      "iteration 1400 / 1500: loss 2.144239\n",
      "iteration 0 / 1500: loss 1213.715963\n",
      "iteration 100 / 1500: loss 424.994060\n",
      "iteration 200 / 1500: loss 149.702798\n",
      "iteration 300 / 1500: loss 53.686873\n",
      "iteration 400 / 1500: loss 20.112197\n",
      "iteration 500 / 1500: loss 8.406261\n",
      "iteration 600 / 1500: loss 4.308887\n",
      "iteration 700 / 1500: loss 2.888264\n",
      "iteration 800 / 1500: loss 2.381347\n",
      "iteration 900 / 1500: loss 2.201113\n",
      "iteration 1000 / 1500: loss 2.135232\n",
      "iteration 1100 / 1500: loss 2.121870\n",
      "iteration 1200 / 1500: loss 2.125004\n",
      "iteration 1300 / 1500: loss 2.143962\n",
      "iteration 1400 / 1500: loss 2.150850\n",
      "iteration 0 / 1500: loss 388.979474\n",
      "iteration 100 / 1500: loss 277.377599\n",
      "iteration 200 / 1500: loss 198.554403\n",
      "iteration 300 / 1500: loss 142.539683\n",
      "iteration 400 / 1500: loss 102.364841\n",
      "iteration 500 / 1500: loss 73.782422\n",
      "iteration 600 / 1500: loss 53.244249\n",
      "iteration 700 / 1500: loss 38.787860\n",
      "iteration 800 / 1500: loss 28.218138\n",
      "iteration 900 / 1500: loss 20.826821\n",
      "iteration 1000 / 1500: loss 15.388113\n",
      "iteration 1100 / 1500: loss 11.659936\n",
      "iteration 1200 / 1500: loss 8.839935\n",
      "iteration 1300 / 1500: loss 6.989627\n",
      "iteration 1400 / 1500: loss 5.558317\n",
      "iteration 0 / 1500: loss 1271.542359\n",
      "iteration 100 / 1500: loss 418.413404\n",
      "iteration 200 / 1500: loss 138.672605\n",
      "iteration 300 / 1500: loss 47.039795\n",
      "iteration 400 / 1500: loss 16.866882\n",
      "iteration 500 / 1500: loss 6.969572\n",
      "iteration 600 / 1500: loss 3.700448\n",
      "iteration 700 / 1500: loss 2.633930\n",
      "iteration 800 / 1500: loss 2.313243\n",
      "iteration 900 / 1500: loss 2.180910\n",
      "iteration 1000 / 1500: loss 2.148220\n",
      "iteration 1100 / 1500: loss 2.121191\n",
      "iteration 1200 / 1500: loss 2.129131\n",
      "iteration 1300 / 1500: loss 2.112210\n",
      "iteration 1400 / 1500: loss 2.125381\n",
      "iteration 0 / 1500: loss 461.402581\n",
      "iteration 100 / 1500: loss 26.842245\n",
      "iteration 200 / 1500: loss 3.362942\n",
      "iteration 300 / 1500: loss 2.175111\n",
      "iteration 400 / 1500: loss 2.135926\n",
      "iteration 500 / 1500: loss 2.025308\n",
      "iteration 600 / 1500: loss 2.027440\n",
      "iteration 700 / 1500: loss 2.023084\n",
      "iteration 800 / 1500: loss 2.065082\n",
      "iteration 900 / 1500: loss 1.976733\n",
      "iteration 1000 / 1500: loss 2.126529\n",
      "iteration 1100 / 1500: loss 2.098147\n",
      "iteration 1200 / 1500: loss 2.047376\n",
      "iteration 1300 / 1500: loss 2.050296\n",
      "iteration 1400 / 1500: loss 2.012142\n",
      "iteration 0 / 1500: loss 419.646825\n",
      "iteration 100 / 1500: loss 31.976056\n",
      "iteration 200 / 1500: loss 4.200406\n",
      "iteration 300 / 1500: loss 2.171626\n",
      "iteration 400 / 1500: loss 2.074630\n",
      "iteration 500 / 1500: loss 2.115144\n",
      "iteration 600 / 1500: loss 2.038247\n",
      "iteration 700 / 1500: loss 1.968272\n",
      "iteration 800 / 1500: loss 1.991071\n",
      "iteration 900 / 1500: loss 2.027615\n",
      "iteration 1000 / 1500: loss 2.044058\n",
      "iteration 1100 / 1500: loss 1.991702\n",
      "iteration 1200 / 1500: loss 2.108892\n",
      "iteration 1300 / 1500: loss 2.062428\n",
      "iteration 1400 / 1500: loss 1.985031\n",
      "iteration 0 / 1500: loss 558.506526\n",
      "iteration 100 / 1500: loss 18.651966\n",
      "iteration 200 / 1500: loss 2.530547\n",
      "iteration 300 / 1500: loss 2.151051\n",
      "iteration 400 / 1500: loss 1.999737\n",
      "iteration 500 / 1500: loss 2.038515\n",
      "iteration 600 / 1500: loss 2.065107\n",
      "iteration 700 / 1500: loss 2.070652\n",
      "iteration 800 / 1500: loss 2.031587\n",
      "iteration 900 / 1500: loss 2.046815\n",
      "iteration 1000 / 1500: loss 2.044175\n",
      "iteration 1100 / 1500: loss 2.057806\n",
      "iteration 1200 / 1500: loss 2.061978\n",
      "iteration 1300 / 1500: loss 2.017283\n",
      "iteration 1400 / 1500: loss 2.090646\n",
      "iteration 0 / 1500: loss 1514.463205\n",
      "iteration 100 / 1500: loss 2.213846\n",
      "iteration 200 / 1500: loss 2.129114\n",
      "iteration 300 / 1500: loss 2.119241\n",
      "iteration 400 / 1500: loss 2.192524\n",
      "iteration 500 / 1500: loss 2.133738\n",
      "iteration 600 / 1500: loss 2.165729\n",
      "iteration 700 / 1500: loss 2.092438\n",
      "iteration 800 / 1500: loss 2.132090\n",
      "iteration 900 / 1500: loss 2.135451\n",
      "iteration 1000 / 1500: loss 2.192020\n",
      "iteration 1100 / 1500: loss 2.127667\n",
      "iteration 1200 / 1500: loss 2.146555\n",
      "iteration 1300 / 1500: loss 2.177031\n",
      "iteration 1400 / 1500: loss 2.141909\n",
      "iteration 0 / 1500: loss 1505.763214\n",
      "iteration 100 / 1500: loss 2.244542\n",
      "iteration 200 / 1500: loss 2.133484\n",
      "iteration 300 / 1500: loss 2.147334\n",
      "iteration 400 / 1500: loss 2.144524\n",
      "iteration 500 / 1500: loss 2.160941\n",
      "iteration 600 / 1500: loss 2.132589\n",
      "iteration 700 / 1500: loss 2.124553\n",
      "iteration 800 / 1500: loss 2.172680\n",
      "iteration 900 / 1500: loss 2.157465\n",
      "iteration 1000 / 1500: loss 2.129411\n",
      "iteration 1100 / 1500: loss 2.154676\n",
      "iteration 1200 / 1500: loss 2.172622\n",
      "iteration 1300 / 1500: loss 2.205507\n",
      "iteration 1400 / 1500: loss 2.126165\n",
      "iteration 0 / 1500: loss 817.680938\n",
      "iteration 100 / 1500: loss 6.656608\n",
      "iteration 200 / 1500: loss 2.109547\n",
      "iteration 300 / 1500: loss 2.081749\n",
      "iteration 400 / 1500: loss 2.091867\n",
      "iteration 500 / 1500: loss 2.080732\n",
      "iteration 600 / 1500: loss 2.089505\n",
      "iteration 700 / 1500: loss 2.084274\n",
      "iteration 800 / 1500: loss 2.078551\n",
      "iteration 900 / 1500: loss 2.118616\n",
      "iteration 1000 / 1500: loss 2.107063\n",
      "iteration 1100 / 1500: loss 2.081341\n",
      "iteration 1200 / 1500: loss 2.068446\n",
      "iteration 1300 / 1500: loss 2.135335\n",
      "iteration 1400 / 1500: loss 2.080757\n",
      "iteration 0 / 1500: loss 970.310014\n",
      "iteration 100 / 1500: loss 3.970072\n",
      "iteration 200 / 1500: loss 2.143684\n",
      "iteration 300 / 1500: loss 2.064217\n",
      "iteration 400 / 1500: loss 2.103431\n",
      "iteration 500 / 1500: loss 2.153303\n",
      "iteration 600 / 1500: loss 2.082678\n",
      "iteration 700 / 1500: loss 2.113575\n",
      "iteration 800 / 1500: loss 2.127212\n",
      "iteration 900 / 1500: loss 2.121150\n",
      "iteration 1000 / 1500: loss 2.126232\n",
      "iteration 1100 / 1500: loss 2.139642\n",
      "iteration 1200 / 1500: loss 2.117754\n",
      "iteration 1300 / 1500: loss 2.139162\n",
      "iteration 1400 / 1500: loss 2.173820\n",
      "iteration 0 / 1500: loss 1056.036405\n",
      "iteration 100 / 1500: loss 3.265053\n",
      "iteration 200 / 1500: loss 2.123931\n",
      "iteration 300 / 1500: loss 2.099722\n",
      "iteration 400 / 1500: loss 2.087692\n",
      "iteration 500 / 1500: loss 2.155909\n",
      "iteration 600 / 1500: loss 2.111415\n",
      "iteration 700 / 1500: loss 2.081940\n",
      "iteration 800 / 1500: loss 2.102115\n",
      "iteration 900 / 1500: loss 2.158925\n",
      "iteration 1000 / 1500: loss 2.063761\n",
      "iteration 1100 / 1500: loss 2.141559\n",
      "iteration 1200 / 1500: loss 2.081893\n",
      "iteration 1300 / 1500: loss 2.153025\n",
      "iteration 1400 / 1500: loss 2.132135\n",
      "iteration 0 / 1500: loss 1446.617806\n",
      "iteration 100 / 1500: loss 2.262392\n",
      "iteration 200 / 1500: loss 2.159423\n",
      "iteration 300 / 1500: loss 2.172046\n",
      "iteration 400 / 1500: loss 2.149400\n",
      "iteration 500 / 1500: loss 2.145507\n",
      "iteration 600 / 1500: loss 2.158110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 1500: loss 2.143390\n",
      "iteration 800 / 1500: loss 2.155587\n",
      "iteration 900 / 1500: loss 2.105802\n",
      "iteration 1000 / 1500: loss 2.198149\n",
      "iteration 1100 / 1500: loss 2.125957\n",
      "iteration 1200 / 1500: loss 2.092602\n",
      "iteration 1300 / 1500: loss 2.153229\n",
      "iteration 1400 / 1500: loss 2.143311\n",
      "iteration 0 / 1500: loss 471.346198\n",
      "iteration 100 / 1500: loss 25.743547\n",
      "iteration 200 / 1500: loss 3.305027\n",
      "iteration 300 / 1500: loss 2.105051\n",
      "iteration 400 / 1500: loss 1.990944\n",
      "iteration 500 / 1500: loss 2.023193\n",
      "iteration 600 / 1500: loss 1.943573\n",
      "iteration 700 / 1500: loss 2.023658\n",
      "iteration 800 / 1500: loss 2.037428\n",
      "iteration 900 / 1500: loss 2.026086\n",
      "iteration 1000 / 1500: loss 1.974479\n",
      "iteration 1100 / 1500: loss 2.082682\n",
      "iteration 1200 / 1500: loss 2.062853\n",
      "iteration 1300 / 1500: loss 2.000115\n",
      "iteration 1400 / 1500: loss 2.015029\n",
      "iteration 0 / 1500: loss 328.537217\n",
      "iteration 100 / 1500: loss 43.578112\n",
      "iteration 200 / 1500: loss 7.376676\n",
      "iteration 300 / 1500: loss 2.782266\n",
      "iteration 400 / 1500: loss 2.054833\n",
      "iteration 500 / 1500: loss 2.041933\n",
      "iteration 600 / 1500: loss 2.019391\n",
      "iteration 700 / 1500: loss 1.991661\n",
      "iteration 800 / 1500: loss 1.914781\n",
      "iteration 900 / 1500: loss 2.043932\n",
      "iteration 1000 / 1500: loss 1.962325\n",
      "iteration 1100 / 1500: loss 1.979174\n",
      "iteration 1200 / 1500: loss 2.005236\n",
      "iteration 1300 / 1500: loss 2.054144\n",
      "iteration 1400 / 1500: loss 2.118543\n",
      "iteration 0 / 1500: loss 360.155105\n",
      "iteration 100 / 1500: loss 39.298150\n",
      "iteration 200 / 1500: loss 5.898252\n",
      "iteration 300 / 1500: loss 2.418061\n",
      "iteration 400 / 1500: loss 2.095490\n",
      "iteration 500 / 1500: loss 2.035827\n",
      "iteration 600 / 1500: loss 2.029746\n",
      "iteration 700 / 1500: loss 2.035279\n",
      "iteration 800 / 1500: loss 2.031694\n",
      "iteration 900 / 1500: loss 2.007812\n",
      "iteration 1000 / 1500: loss 1.970685\n",
      "iteration 1100 / 1500: loss 2.027151\n",
      "iteration 1200 / 1500: loss 2.120516\n",
      "iteration 1300 / 1500: loss 2.001043\n",
      "iteration 1400 / 1500: loss 2.011811\n",
      "iteration 0 / 1500: loss 654.648975\n",
      "iteration 100 / 1500: loss 12.354492\n",
      "iteration 200 / 1500: loss 2.197122\n",
      "iteration 300 / 1500: loss 2.113715\n",
      "iteration 400 / 1500: loss 2.086002\n",
      "iteration 500 / 1500: loss 2.122820\n",
      "iteration 600 / 1500: loss 2.072212\n",
      "iteration 700 / 1500: loss 2.094850\n",
      "iteration 800 / 1500: loss 1.996140\n",
      "iteration 900 / 1500: loss 2.055627\n",
      "iteration 1000 / 1500: loss 2.105147\n",
      "iteration 1100 / 1500: loss 2.024963\n",
      "iteration 1200 / 1500: loss 2.094773\n",
      "iteration 1300 / 1500: loss 2.093667\n",
      "iteration 1400 / 1500: loss 2.103090\n",
      "iteration 0 / 1500: loss 393.686285\n",
      "iteration 100 / 1500: loss 33.511357\n",
      "iteration 200 / 1500: loss 4.472945\n",
      "iteration 300 / 1500: loss 2.227657\n",
      "iteration 400 / 1500: loss 2.077916\n",
      "iteration 500 / 1500: loss 2.042049\n",
      "iteration 600 / 1500: loss 2.048403\n",
      "iteration 700 / 1500: loss 2.104253\n",
      "iteration 800 / 1500: loss 1.956311\n",
      "iteration 900 / 1500: loss 1.980621\n",
      "iteration 1000 / 1500: loss 2.065672\n",
      "iteration 1100 / 1500: loss 1.979748\n",
      "iteration 1200 / 1500: loss 1.985285\n",
      "iteration 1300 / 1500: loss 2.003174\n",
      "iteration 1400 / 1500: loss 2.090031\n",
      "iteration 0 / 1500: loss 1098.782840\n",
      "iteration 100 / 1500: loss 3.096040\n",
      "iteration 200 / 1500: loss 2.049949\n",
      "iteration 300 / 1500: loss 2.094924\n",
      "iteration 400 / 1500: loss 2.168524\n",
      "iteration 500 / 1500: loss 2.133825\n",
      "iteration 600 / 1500: loss 2.063519\n",
      "iteration 700 / 1500: loss 2.116513\n",
      "iteration 800 / 1500: loss 2.119829\n",
      "iteration 900 / 1500: loss 2.159315\n",
      "iteration 1000 / 1500: loss 2.116392\n",
      "iteration 1100 / 1500: loss 2.126311\n",
      "iteration 1200 / 1500: loss 2.122779\n",
      "iteration 1300 / 1500: loss 2.133398\n",
      "iteration 1400 / 1500: loss 2.124977\n",
      "iteration 0 / 1500: loss 388.506633\n",
      "iteration 100 / 1500: loss 35.024464\n",
      "iteration 200 / 1500: loss 4.870832\n",
      "iteration 300 / 1500: loss 2.313010\n",
      "iteration 400 / 1500: loss 2.021252\n",
      "iteration 500 / 1500: loss 2.033466\n",
      "iteration 600 / 1500: loss 1.976818\n",
      "iteration 700 / 1500: loss 1.981224\n",
      "iteration 800 / 1500: loss 1.957829\n",
      "iteration 900 / 1500: loss 2.049038\n",
      "iteration 1000 / 1500: loss 2.009046\n",
      "iteration 1100 / 1500: loss 1.978098\n",
      "iteration 1200 / 1500: loss 2.009068\n",
      "iteration 1300 / 1500: loss 2.056505\n",
      "iteration 1400 / 1500: loss 1.985422\n",
      "iteration 0 / 1500: loss 1268.003672\n",
      "iteration 100 / 1500: loss 2.492351\n",
      "iteration 200 / 1500: loss 2.166299\n",
      "iteration 300 / 1500: loss 2.142160\n",
      "iteration 400 / 1500: loss 2.167731\n",
      "iteration 500 / 1500: loss 2.152249\n",
      "iteration 600 / 1500: loss 2.097559\n",
      "iteration 700 / 1500: loss 2.128445\n",
      "iteration 800 / 1500: loss 2.136793\n",
      "iteration 900 / 1500: loss 2.098950\n",
      "iteration 1000 / 1500: loss 2.095659\n",
      "iteration 1100 / 1500: loss 2.159867\n",
      "iteration 1200 / 1500: loss 2.128007\n",
      "iteration 1300 / 1500: loss 2.149450\n",
      "iteration 1400 / 1500: loss 2.167645\n",
      "iteration 0 / 1500: loss 1197.369196\n",
      "iteration 100 / 1500: loss 2.639649\n",
      "iteration 200 / 1500: loss 2.130844\n",
      "iteration 300 / 1500: loss 2.122905\n",
      "iteration 400 / 1500: loss 2.087098\n",
      "iteration 500 / 1500: loss 2.064598\n",
      "iteration 600 / 1500: loss 2.103480\n",
      "iteration 700 / 1500: loss 2.138646\n",
      "iteration 800 / 1500: loss 2.127673\n",
      "iteration 900 / 1500: loss 2.149247\n",
      "iteration 1000 / 1500: loss 2.082266\n",
      "iteration 1100 / 1500: loss 2.099916\n",
      "iteration 1200 / 1500: loss 2.181587\n",
      "iteration 1300 / 1500: loss 2.058024\n",
      "iteration 1400 / 1500: loss 2.146850\n",
      "iteration 0 / 1500: loss 386.255845\n",
      "iteration 100 / 1500: loss 35.224911\n",
      "iteration 200 / 1500: loss 4.873396\n",
      "iteration 300 / 1500: loss 2.363557\n",
      "iteration 400 / 1500: loss 2.038242\n",
      "iteration 500 / 1500: loss 2.032664\n",
      "iteration 600 / 1500: loss 2.068437\n",
      "iteration 700 / 1500: loss 2.062545\n",
      "iteration 800 / 1500: loss 2.055914\n",
      "iteration 900 / 1500: loss 2.086290\n",
      "iteration 1000 / 1500: loss 2.059164\n",
      "iteration 1100 / 1500: loss 2.046231\n",
      "iteration 1200 / 1500: loss 1.979819\n",
      "iteration 1300 / 1500: loss 2.056255\n",
      "iteration 1400 / 1500: loss 2.062664\n",
      "iteration 0 / 1500: loss 1277.112442\n",
      "iteration 100 / 1500: loss 2.432318\n",
      "iteration 200 / 1500: loss 2.130636\n",
      "iteration 300 / 1500: loss 2.127161\n",
      "iteration 400 / 1500: loss 2.086763\n",
      "iteration 500 / 1500: loss 2.145182\n",
      "iteration 600 / 1500: loss 2.136103\n",
      "iteration 700 / 1500: loss 2.124635\n",
      "iteration 800 / 1500: loss 2.091083\n",
      "iteration 900 / 1500: loss 2.114890\n",
      "iteration 1000 / 1500: loss 2.140096\n",
      "iteration 1100 / 1500: loss 2.170205\n",
      "iteration 1200 / 1500: loss 2.174691\n",
      "iteration 1300 / 1500: loss 2.072528\n",
      "iteration 1400 / 1500: loss 2.096360\n",
      "iteration 0 / 1500: loss 456.593155\n",
      "iteration 100 / 1500: loss 136.558085\n",
      "iteration 200 / 1500: loss 41.895930\n",
      "iteration 300 / 1500: loss 13.869220\n",
      "iteration 400 / 1500: loss 5.582756\n",
      "iteration 500 / 1500: loss 3.130463\n",
      "iteration 600 / 1500: loss 2.316115\n",
      "iteration 700 / 1500: loss 2.128685\n",
      "iteration 800 / 1500: loss 2.130109\n",
      "iteration 900 / 1500: loss 2.083950\n",
      "iteration 1000 / 1500: loss 2.036846\n",
      "iteration 1100 / 1500: loss 2.023370\n",
      "iteration 1200 / 1500: loss 2.044157\n",
      "iteration 1300 / 1500: loss 2.074460\n",
      "iteration 1400 / 1500: loss 2.073049\n",
      "iteration 0 / 1500: loss 418.184415\n",
      "iteration 100 / 1500: loss 140.758803\n",
      "iteration 200 / 1500: loss 48.378344\n",
      "iteration 300 / 1500: loss 17.552277\n",
      "iteration 400 / 1500: loss 7.267224\n",
      "iteration 500 / 1500: loss 3.822621\n",
      "iteration 600 / 1500: loss 2.531703\n",
      "iteration 700 / 1500: loss 2.217376\n",
      "iteration 800 / 1500: loss 2.063697\n",
      "iteration 900 / 1500: loss 2.077945\n",
      "iteration 1000 / 1500: loss 2.057887\n",
      "iteration 1100 / 1500: loss 2.029479\n",
      "iteration 1200 / 1500: loss 2.031464\n",
      "iteration 1300 / 1500: loss 2.083331\n",
      "iteration 1400 / 1500: loss 2.039647\n",
      "iteration 0 / 1500: loss 548.969212\n",
      "iteration 100 / 1500: loss 128.803364\n",
      "iteration 200 / 1500: loss 31.519413\n",
      "iteration 300 / 1500: loss 8.815604\n",
      "iteration 400 / 1500: loss 3.574249\n",
      "iteration 500 / 1500: loss 2.466649\n",
      "iteration 600 / 1500: loss 2.160118\n",
      "iteration 700 / 1500: loss 2.110412\n",
      "iteration 800 / 1500: loss 2.127386\n",
      "iteration 900 / 1500: loss 2.040854\n",
      "iteration 1000 / 1500: loss 2.077867\n",
      "iteration 1100 / 1500: loss 1.999850\n",
      "iteration 1200 / 1500: loss 2.099408\n",
      "iteration 1300 / 1500: loss 2.061242\n",
      "iteration 1400 / 1500: loss 2.089901\n",
      "iteration 0 / 1500: loss 1527.240389\n",
      "iteration 100 / 1500: loss 27.847224\n",
      "iteration 200 / 1500: loss 2.559674\n",
      "iteration 300 / 1500: loss 2.170396\n",
      "iteration 400 / 1500: loss 2.143851\n",
      "iteration 500 / 1500: loss 2.115591\n",
      "iteration 600 / 1500: loss 2.059258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 1500: loss 2.140336\n",
      "iteration 800 / 1500: loss 2.159854\n",
      "iteration 900 / 1500: loss 2.161145\n",
      "iteration 1000 / 1500: loss 2.162881\n",
      "iteration 1100 / 1500: loss 2.084754\n",
      "iteration 1200 / 1500: loss 2.134308\n",
      "iteration 1300 / 1500: loss 2.149704\n",
      "iteration 1400 / 1500: loss 2.168620\n",
      "iteration 0 / 1500: loss 1491.620631\n",
      "iteration 100 / 1500: loss 28.849983\n",
      "iteration 200 / 1500: loss 2.649782\n",
      "iteration 300 / 1500: loss 2.144349\n",
      "iteration 400 / 1500: loss 2.125499\n",
      "iteration 500 / 1500: loss 2.130553\n",
      "iteration 600 / 1500: loss 2.160416\n",
      "iteration 700 / 1500: loss 2.113277\n",
      "iteration 800 / 1500: loss 2.127344\n",
      "iteration 900 / 1500: loss 2.126075\n",
      "iteration 1000 / 1500: loss 2.147172\n",
      "iteration 1100 / 1500: loss 2.158216\n",
      "iteration 1200 / 1500: loss 2.101125\n",
      "iteration 1300 / 1500: loss 2.210756\n",
      "iteration 1400 / 1500: loss 2.139317\n",
      "iteration 0 / 1500: loss 817.899592\n",
      "iteration 100 / 1500: loss 96.581557\n",
      "iteration 200 / 1500: loss 13.038004\n",
      "iteration 300 / 1500: loss 3.339408\n",
      "iteration 400 / 1500: loss 2.192921\n",
      "iteration 500 / 1500: loss 2.146171\n",
      "iteration 600 / 1500: loss 2.097926\n",
      "iteration 700 / 1500: loss 2.115911\n",
      "iteration 800 / 1500: loss 2.137419\n",
      "iteration 900 / 1500: loss 2.107511\n",
      "iteration 1000 / 1500: loss 2.086746\n",
      "iteration 1100 / 1500: loss 2.078382\n",
      "iteration 1200 / 1500: loss 2.135003\n",
      "iteration 1300 / 1500: loss 2.127888\n",
      "iteration 1400 / 1500: loss 2.122703\n",
      "iteration 0 / 1500: loss 977.481300\n",
      "iteration 100 / 1500: loss 75.410933\n",
      "iteration 200 / 1500: loss 7.685142\n",
      "iteration 300 / 1500: loss 2.493351\n",
      "iteration 400 / 1500: loss 2.207560\n",
      "iteration 500 / 1500: loss 2.066147\n",
      "iteration 600 / 1500: loss 2.077022\n",
      "iteration 700 / 1500: loss 2.099016\n",
      "iteration 800 / 1500: loss 2.048984\n",
      "iteration 900 / 1500: loss 2.123923\n",
      "iteration 1000 / 1500: loss 2.170606\n",
      "iteration 1100 / 1500: loss 2.078240\n",
      "iteration 1200 / 1500: loss 2.051963\n",
      "iteration 1300 / 1500: loss 2.141423\n",
      "iteration 1400 / 1500: loss 2.137021\n",
      "iteration 0 / 1500: loss 1068.403938\n",
      "iteration 100 / 1500: loss 64.959121\n",
      "iteration 200 / 1500: loss 5.835652\n",
      "iteration 300 / 1500: loss 2.355128\n",
      "iteration 400 / 1500: loss 2.138648\n",
      "iteration 500 / 1500: loss 2.096814\n",
      "iteration 600 / 1500: loss 2.116790\n",
      "iteration 700 / 1500: loss 2.057045\n",
      "iteration 800 / 1500: loss 2.095231\n",
      "iteration 900 / 1500: loss 2.194956\n",
      "iteration 1000 / 1500: loss 2.136226\n",
      "iteration 1100 / 1500: loss 2.135206\n",
      "iteration 1200 / 1500: loss 2.093840\n",
      "iteration 1300 / 1500: loss 2.127177\n",
      "iteration 1400 / 1500: loss 2.147992\n",
      "iteration 0 / 1500: loss 1453.620784\n",
      "iteration 100 / 1500: loss 32.500884\n",
      "iteration 200 / 1500: loss 2.787416\n",
      "iteration 300 / 1500: loss 2.126396\n",
      "iteration 400 / 1500: loss 2.169876\n",
      "iteration 500 / 1500: loss 2.144539\n",
      "iteration 600 / 1500: loss 2.144097\n",
      "iteration 700 / 1500: loss 2.175029\n",
      "iteration 800 / 1500: loss 2.127861\n",
      "iteration 900 / 1500: loss 2.148825\n",
      "iteration 1000 / 1500: loss 2.150636\n",
      "iteration 1100 / 1500: loss 2.120830\n",
      "iteration 1200 / 1500: loss 2.074433\n",
      "iteration 1300 / 1500: loss 2.131035\n",
      "iteration 1400 / 1500: loss 2.134483\n",
      "iteration 0 / 1500: loss 470.193626\n",
      "iteration 100 / 1500: loss 136.762845\n",
      "iteration 200 / 1500: loss 40.979006\n",
      "iteration 300 / 1500: loss 13.290961\n",
      "iteration 400 / 1500: loss 5.310402\n",
      "iteration 500 / 1500: loss 3.014141\n",
      "iteration 600 / 1500: loss 2.263238\n",
      "iteration 700 / 1500: loss 2.150278\n",
      "iteration 800 / 1500: loss 2.160815\n",
      "iteration 900 / 1500: loss 2.104403\n",
      "iteration 1000 / 1500: loss 2.059602\n",
      "iteration 1100 / 1500: loss 2.045690\n",
      "iteration 1200 / 1500: loss 2.064211\n",
      "iteration 1300 / 1500: loss 1.972467\n",
      "iteration 1400 / 1500: loss 2.102960\n",
      "iteration 0 / 1500: loss 327.252137\n",
      "iteration 100 / 1500: loss 139.173084\n",
      "iteration 200 / 1500: loss 60.247456\n",
      "iteration 300 / 1500: loss 26.737118\n",
      "iteration 400 / 1500: loss 12.479095\n",
      "iteration 500 / 1500: loss 6.467420\n",
      "iteration 600 / 1500: loss 3.843939\n",
      "iteration 700 / 1500: loss 2.741233\n",
      "iteration 800 / 1500: loss 2.314997\n",
      "iteration 900 / 1500: loss 2.188863\n",
      "iteration 1000 / 1500: loss 2.035147\n",
      "iteration 1100 / 1500: loss 2.063789\n",
      "iteration 1200 / 1500: loss 1.956116\n",
      "iteration 1300 / 1500: loss 2.055906\n",
      "iteration 1400 / 1500: loss 1.959763\n",
      "iteration 0 / 1500: loss 362.941438\n",
      "iteration 100 / 1500: loss 142.094003\n",
      "iteration 200 / 1500: loss 56.702561\n",
      "iteration 300 / 1500: loss 23.292161\n",
      "iteration 400 / 1500: loss 10.339655\n",
      "iteration 500 / 1500: loss 5.265521\n",
      "iteration 600 / 1500: loss 3.254634\n",
      "iteration 700 / 1500: loss 2.498438\n",
      "iteration 800 / 1500: loss 2.176513\n",
      "iteration 900 / 1500: loss 2.088007\n",
      "iteration 1000 / 1500: loss 2.067823\n",
      "iteration 1100 / 1500: loss 2.050665\n",
      "iteration 1200 / 1500: loss 1.953491\n",
      "iteration 1300 / 1500: loss 2.055109\n",
      "iteration 1400 / 1500: loss 2.052398\n",
      "iteration 0 / 1500: loss 641.759042\n",
      "iteration 100 / 1500: loss 116.184028\n",
      "iteration 200 / 1500: loss 22.411113\n",
      "iteration 300 / 1500: loss 5.658049\n",
      "iteration 400 / 1500: loss 2.744300\n",
      "iteration 500 / 1500: loss 2.144333\n",
      "iteration 600 / 1500: loss 2.089507\n",
      "iteration 700 / 1500: loss 2.085343\n",
      "iteration 800 / 1500: loss 2.037486\n",
      "iteration 900 / 1500: loss 2.108065\n",
      "iteration 1000 / 1500: loss 2.140763\n",
      "iteration 1100 / 1500: loss 2.033721\n",
      "iteration 1200 / 1500: loss 2.053989\n",
      "iteration 1300 / 1500: loss 2.060380\n",
      "iteration 1400 / 1500: loss 2.015736\n",
      "iteration 0 / 1500: loss 392.170283\n",
      "iteration 100 / 1500: loss 138.344375\n",
      "iteration 200 / 1500: loss 49.891705\n",
      "iteration 300 / 1500: loss 18.936394\n",
      "iteration 400 / 1500: loss 7.875309\n",
      "iteration 500 / 1500: loss 4.176359\n",
      "iteration 600 / 1500: loss 2.697413\n",
      "iteration 700 / 1500: loss 2.235919\n",
      "iteration 800 / 1500: loss 2.225871\n",
      "iteration 900 / 1500: loss 2.077287\n",
      "iteration 1000 / 1500: loss 2.079585\n",
      "iteration 1100 / 1500: loss 2.016255\n",
      "iteration 1200 / 1500: loss 2.017142\n",
      "iteration 1300 / 1500: loss 2.101060\n",
      "iteration 1400 / 1500: loss 2.059713\n",
      "iteration 0 / 1500: loss 1087.414445\n",
      "iteration 100 / 1500: loss 61.822129\n",
      "iteration 200 / 1500: loss 5.383418\n",
      "iteration 300 / 1500: loss 2.313943\n",
      "iteration 400 / 1500: loss 2.153563\n",
      "iteration 500 / 1500: loss 2.103430\n",
      "iteration 600 / 1500: loss 2.127900\n",
      "iteration 700 / 1500: loss 2.127390\n",
      "iteration 800 / 1500: loss 2.181894\n",
      "iteration 900 / 1500: loss 2.121446\n",
      "iteration 1000 / 1500: loss 2.173010\n",
      "iteration 1100 / 1500: loss 2.097021\n",
      "iteration 1200 / 1500: loss 2.111163\n",
      "iteration 1300 / 1500: loss 2.090812\n",
      "iteration 1400 / 1500: loss 2.101516\n",
      "iteration 0 / 1500: loss 397.134000\n",
      "iteration 100 / 1500: loss 142.907622\n",
      "iteration 200 / 1500: loss 52.485289\n",
      "iteration 300 / 1500: loss 20.244156\n",
      "iteration 400 / 1500: loss 8.565644\n",
      "iteration 500 / 1500: loss 4.391080\n",
      "iteration 600 / 1500: loss 2.875601\n",
      "iteration 700 / 1500: loss 2.346537\n",
      "iteration 800 / 1500: loss 2.083782\n",
      "iteration 900 / 1500: loss 2.095056\n",
      "iteration 1000 / 1500: loss 2.032388\n",
      "iteration 1100 / 1500: loss 2.088696\n",
      "iteration 1200 / 1500: loss 2.029503\n",
      "iteration 1300 / 1500: loss 2.004131\n",
      "iteration 1400 / 1500: loss 2.042709\n",
      "iteration 0 / 1500: loss 1263.932945\n",
      "iteration 100 / 1500: loss 47.105709\n",
      "iteration 200 / 1500: loss 3.701996\n",
      "iteration 300 / 1500: loss 2.167729\n",
      "iteration 400 / 1500: loss 2.138735\n",
      "iteration 500 / 1500: loss 2.131107\n",
      "iteration 600 / 1500: loss 2.103300\n",
      "iteration 700 / 1500: loss 2.098540\n",
      "iteration 800 / 1500: loss 2.116764\n",
      "iteration 900 / 1500: loss 2.112784\n",
      "iteration 1000 / 1500: loss 2.069776\n",
      "iteration 1100 / 1500: loss 2.175165\n",
      "iteration 1200 / 1500: loss 2.130525\n",
      "iteration 1300 / 1500: loss 2.138266\n",
      "iteration 1400 / 1500: loss 2.179538\n",
      "iteration 0 / 1500: loss 1195.104626\n",
      "iteration 100 / 1500: loss 50.649389\n",
      "iteration 200 / 1500: loss 4.055502\n",
      "iteration 300 / 1500: loss 2.172950\n",
      "iteration 400 / 1500: loss 2.080368\n",
      "iteration 500 / 1500: loss 2.128885\n",
      "iteration 600 / 1500: loss 2.108343\n",
      "iteration 700 / 1500: loss 2.122603\n",
      "iteration 800 / 1500: loss 2.119755\n",
      "iteration 900 / 1500: loss 2.170550\n",
      "iteration 1000 / 1500: loss 2.130948\n",
      "iteration 1100 / 1500: loss 2.104573\n",
      "iteration 1200 / 1500: loss 2.093265\n",
      "iteration 1300 / 1500: loss 2.118918\n",
      "iteration 1400 / 1500: loss 2.133544\n",
      "iteration 0 / 1500: loss 394.676202\n",
      "iteration 100 / 1500: loss 142.760529\n",
      "iteration 200 / 1500: loss 52.869067\n",
      "iteration 300 / 1500: loss 20.396525\n",
      "iteration 400 / 1500: loss 8.707457\n",
      "iteration 500 / 1500: loss 4.479237\n",
      "iteration 600 / 1500: loss 2.939232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 1500: loss 2.325464\n",
      "iteration 800 / 1500: loss 2.133478\n",
      "iteration 900 / 1500: loss 2.071034\n",
      "iteration 1000 / 1500: loss 2.012263\n",
      "iteration 1100 / 1500: loss 2.023958\n",
      "iteration 1200 / 1500: loss 1.989983\n",
      "iteration 1300 / 1500: loss 2.087782\n",
      "iteration 1400 / 1500: loss 1.937252\n",
      "iteration 0 / 1500: loss 1279.678270\n",
      "iteration 100 / 1500: loss 45.105973\n",
      "iteration 200 / 1500: loss 3.564248\n",
      "iteration 300 / 1500: loss 2.155382\n",
      "iteration 400 / 1500: loss 2.141421\n",
      "iteration 500 / 1500: loss 2.156353\n",
      "iteration 600 / 1500: loss 2.127325\n",
      "iteration 700 / 1500: loss 2.077129\n",
      "iteration 800 / 1500: loss 2.132773\n",
      "iteration 900 / 1500: loss 2.165169\n",
      "iteration 1000 / 1500: loss 2.139610\n",
      "iteration 1100 / 1500: loss 2.093238\n",
      "iteration 1200 / 1500: loss 2.145720\n",
      "iteration 1300 / 1500: loss 2.163099\n",
      "iteration 1400 / 1500: loss 2.177082\n",
      "iteration 0 / 1500: loss 464.314553\n",
      "iteration 100 / 1500: loss 53.779940\n",
      "iteration 200 / 1500: loss 7.901611\n",
      "iteration 300 / 1500: loss 2.702032\n",
      "iteration 400 / 1500: loss 2.125437\n",
      "iteration 500 / 1500: loss 2.132366\n",
      "iteration 600 / 1500: loss 2.074827\n",
      "iteration 700 / 1500: loss 2.049861\n",
      "iteration 800 / 1500: loss 2.005249\n",
      "iteration 900 / 1500: loss 2.014361\n",
      "iteration 1000 / 1500: loss 2.055705\n",
      "iteration 1100 / 1500: loss 2.077855\n",
      "iteration 1200 / 1500: loss 2.024165\n",
      "iteration 1300 / 1500: loss 2.112238\n",
      "iteration 1400 / 1500: loss 2.009702\n",
      "iteration 0 / 1500: loss 417.688753\n",
      "iteration 100 / 1500: loss 59.630040\n",
      "iteration 200 / 1500: loss 10.117355\n",
      "iteration 300 / 1500: loss 3.110738\n",
      "iteration 400 / 1500: loss 2.150121\n",
      "iteration 500 / 1500: loss 2.007977\n",
      "iteration 600 / 1500: loss 2.007940\n",
      "iteration 700 / 1500: loss 1.973602\n",
      "iteration 800 / 1500: loss 2.120754\n",
      "iteration 900 / 1500: loss 2.022787\n",
      "iteration 1000 / 1500: loss 2.060933\n",
      "iteration 1100 / 1500: loss 1.962314\n",
      "iteration 1200 / 1500: loss 2.059184\n",
      "iteration 1300 / 1500: loss 2.054526\n",
      "iteration 1400 / 1500: loss 2.006324\n",
      "iteration 0 / 1500: loss 556.244413\n",
      "iteration 100 / 1500: loss 41.892195\n",
      "iteration 200 / 1500: loss 4.930182\n",
      "iteration 300 / 1500: loss 2.298419\n",
      "iteration 400 / 1500: loss 2.126363\n",
      "iteration 500 / 1500: loss 2.078082\n",
      "iteration 600 / 1500: loss 2.039225\n",
      "iteration 700 / 1500: loss 2.093370\n",
      "iteration 800 / 1500: loss 2.091274\n",
      "iteration 900 / 1500: loss 2.112229\n",
      "iteration 1000 / 1500: loss 2.054301\n",
      "iteration 1100 / 1500: loss 2.065565\n",
      "iteration 1200 / 1500: loss 1.985058\n",
      "iteration 1300 / 1500: loss 2.026604\n",
      "iteration 1400 / 1500: loss 2.040106\n",
      "iteration 0 / 1500: loss 1541.748602\n",
      "iteration 100 / 1500: loss 3.070274\n",
      "iteration 200 / 1500: loss 2.194070\n",
      "iteration 300 / 1500: loss 2.156633\n",
      "iteration 400 / 1500: loss 2.167292\n",
      "iteration 500 / 1500: loss 2.135856\n",
      "iteration 600 / 1500: loss 2.113146\n",
      "iteration 700 / 1500: loss 2.168073\n",
      "iteration 800 / 1500: loss 2.145624\n",
      "iteration 900 / 1500: loss 2.113920\n",
      "iteration 1000 / 1500: loss 2.186745\n",
      "iteration 1100 / 1500: loss 2.124564\n",
      "iteration 1200 / 1500: loss 2.162245\n",
      "iteration 1300 / 1500: loss 2.171770\n",
      "iteration 1400 / 1500: loss 2.122569\n",
      "iteration 0 / 1500: loss 1522.596672\n",
      "iteration 100 / 1500: loss 3.228481\n",
      "iteration 200 / 1500: loss 2.164816\n",
      "iteration 300 / 1500: loss 2.160398\n",
      "iteration 400 / 1500: loss 2.117080\n",
      "iteration 500 / 1500: loss 2.154998\n",
      "iteration 600 / 1500: loss 2.169571\n",
      "iteration 700 / 1500: loss 2.187309\n",
      "iteration 800 / 1500: loss 2.139204\n",
      "iteration 900 / 1500: loss 2.134361\n",
      "iteration 1000 / 1500: loss 2.118176\n",
      "iteration 1100 / 1500: loss 2.059424\n",
      "iteration 1200 / 1500: loss 2.100439\n",
      "iteration 1300 / 1500: loss 2.125716\n",
      "iteration 1400 / 1500: loss 2.203145\n",
      "iteration 0 / 1500: loss 808.736398\n",
      "iteration 100 / 1500: loss 18.676846\n",
      "iteration 200 / 1500: loss 2.482006\n",
      "iteration 300 / 1500: loss 2.115448\n",
      "iteration 400 / 1500: loss 2.144949\n",
      "iteration 500 / 1500: loss 2.130888\n",
      "iteration 600 / 1500: loss 2.012208\n",
      "iteration 700 / 1500: loss 2.064772\n",
      "iteration 800 / 1500: loss 2.046846\n",
      "iteration 900 / 1500: loss 2.093807\n",
      "iteration 1000 / 1500: loss 2.083618\n",
      "iteration 1100 / 1500: loss 2.041119\n",
      "iteration 1200 / 1500: loss 2.047775\n",
      "iteration 1300 / 1500: loss 2.143306\n",
      "iteration 1400 / 1500: loss 2.103581\n",
      "iteration 0 / 1500: loss 964.448722\n",
      "iteration 100 / 1500: loss 11.127227\n",
      "iteration 200 / 1500: loss 2.178411\n",
      "iteration 300 / 1500: loss 2.105347\n",
      "iteration 400 / 1500: loss 2.066720\n",
      "iteration 500 / 1500: loss 2.076355\n",
      "iteration 600 / 1500: loss 2.095043\n",
      "iteration 700 / 1500: loss 2.137006\n",
      "iteration 800 / 1500: loss 2.114294\n",
      "iteration 900 / 1500: loss 2.107059\n",
      "iteration 1000 / 1500: loss 2.111102\n",
      "iteration 1100 / 1500: loss 2.090016\n",
      "iteration 1200 / 1500: loss 2.149722\n",
      "iteration 1300 / 1500: loss 2.131392\n",
      "iteration 1400 / 1500: loss 2.084629\n",
      "iteration 0 / 1500: loss 1065.906846\n",
      "iteration 100 / 1500: loss 8.482190\n",
      "iteration 200 / 1500: loss 2.098655\n",
      "iteration 300 / 1500: loss 2.130596\n",
      "iteration 400 / 1500: loss 2.139976\n",
      "iteration 500 / 1500: loss 2.111218\n",
      "iteration 600 / 1500: loss 2.093190\n",
      "iteration 700 / 1500: loss 2.134193\n",
      "iteration 800 / 1500: loss 2.074298\n",
      "iteration 900 / 1500: loss 2.099469\n",
      "iteration 1000 / 1500: loss 2.144650\n",
      "iteration 1100 / 1500: loss 2.086933\n",
      "iteration 1200 / 1500: loss 2.113617\n",
      "iteration 1300 / 1500: loss 2.101046\n",
      "iteration 1400 / 1500: loss 2.073040\n",
      "iteration 0 / 1500: loss 1434.009219\n",
      "iteration 100 / 1500: loss 3.435676\n",
      "iteration 200 / 1500: loss 2.161198\n",
      "iteration 300 / 1500: loss 2.075803\n",
      "iteration 400 / 1500: loss 2.115864\n",
      "iteration 500 / 1500: loss 2.180392\n",
      "iteration 600 / 1500: loss 2.158762\n",
      "iteration 700 / 1500: loss 2.141490\n",
      "iteration 800 / 1500: loss 2.158408\n",
      "iteration 900 / 1500: loss 2.105155\n",
      "iteration 1000 / 1500: loss 2.118051\n",
      "iteration 1100 / 1500: loss 2.158745\n",
      "iteration 1200 / 1500: loss 2.185707\n",
      "iteration 1300 / 1500: loss 2.158896\n",
      "iteration 1400 / 1500: loss 2.158487\n",
      "iteration 0 / 1500: loss 465.615298\n",
      "iteration 100 / 1500: loss 51.339708\n",
      "iteration 200 / 1500: loss 7.287622\n",
      "iteration 300 / 1500: loss 2.658554\n",
      "iteration 400 / 1500: loss 2.108631\n",
      "iteration 500 / 1500: loss 1.956231\n",
      "iteration 600 / 1500: loss 2.025773\n",
      "iteration 700 / 1500: loss 2.022819\n",
      "iteration 800 / 1500: loss 2.072427\n",
      "iteration 900 / 1500: loss 2.035202\n",
      "iteration 1000 / 1500: loss 2.091995\n",
      "iteration 1100 / 1500: loss 2.057501\n",
      "iteration 1200 / 1500: loss 2.033117\n",
      "iteration 1300 / 1500: loss 2.057504\n",
      "iteration 1400 / 1500: loss 2.034308\n",
      "iteration 0 / 1500: loss 324.766525\n",
      "iteration 100 / 1500: loss 70.403982\n",
      "iteration 200 / 1500: loss 16.568689\n",
      "iteration 300 / 1500: loss 5.177132\n",
      "iteration 400 / 1500: loss 2.604933\n",
      "iteration 500 / 1500: loss 2.120818\n",
      "iteration 600 / 1500: loss 2.032226\n",
      "iteration 700 / 1500: loss 2.065836\n",
      "iteration 800 / 1500: loss 2.039710\n",
      "iteration 900 / 1500: loss 2.037031\n",
      "iteration 1000 / 1500: loss 2.004439\n",
      "iteration 1100 / 1500: loss 2.009618\n",
      "iteration 1200 / 1500: loss 2.086814\n",
      "iteration 1300 / 1500: loss 1.955465\n",
      "iteration 1400 / 1500: loss 1.998464\n",
      "iteration 0 / 1500: loss 359.801967\n",
      "iteration 100 / 1500: loss 67.418493\n",
      "iteration 200 / 1500: loss 14.067286\n",
      "iteration 300 / 1500: loss 4.128814\n",
      "iteration 400 / 1500: loss 2.493243\n",
      "iteration 500 / 1500: loss 2.102973\n",
      "iteration 600 / 1500: loss 2.053152\n",
      "iteration 700 / 1500: loss 2.036320\n",
      "iteration 800 / 1500: loss 1.974309\n",
      "iteration 900 / 1500: loss 1.998580\n",
      "iteration 1000 / 1500: loss 1.979277\n",
      "iteration 1100 / 1500: loss 1.988291\n",
      "iteration 1200 / 1500: loss 1.996500\n",
      "iteration 1300 / 1500: loss 2.038179\n",
      "iteration 1400 / 1500: loss 1.971901\n",
      "iteration 0 / 1500: loss 654.096823\n",
      "iteration 100 / 1500: loss 31.300176\n",
      "iteration 200 / 1500: loss 3.397828\n",
      "iteration 300 / 1500: loss 2.171526\n",
      "iteration 400 / 1500: loss 2.095464\n",
      "iteration 500 / 1500: loss 2.043865\n",
      "iteration 600 / 1500: loss 2.059786\n",
      "iteration 700 / 1500: loss 2.089299\n",
      "iteration 800 / 1500: loss 2.120781\n",
      "iteration 900 / 1500: loss 2.090480\n",
      "iteration 1000 / 1500: loss 2.028146\n",
      "iteration 1100 / 1500: loss 2.057816\n",
      "iteration 1200 / 1500: loss 2.090262\n",
      "iteration 1300 / 1500: loss 2.023338\n",
      "iteration 1400 / 1500: loss 2.084568\n",
      "iteration 0 / 1500: loss 394.916507\n",
      "iteration 100 / 1500: loss 61.527804\n",
      "iteration 200 / 1500: loss 11.116747\n",
      "iteration 300 / 1500: loss 3.402833\n",
      "iteration 400 / 1500: loss 2.288862\n",
      "iteration 500 / 1500: loss 2.023990\n",
      "iteration 600 / 1500: loss 1.996688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 1500: loss 2.061249\n",
      "iteration 800 / 1500: loss 2.050243\n",
      "iteration 900 / 1500: loss 2.010694\n",
      "iteration 1000 / 1500: loss 2.007229\n",
      "iteration 1100 / 1500: loss 1.983037\n",
      "iteration 1200 / 1500: loss 2.041401\n",
      "iteration 1300 / 1500: loss 2.052842\n",
      "iteration 1400 / 1500: loss 2.058935\n",
      "iteration 0 / 1500: loss 1077.201258\n",
      "iteration 100 / 1500: loss 7.886822\n",
      "iteration 200 / 1500: loss 2.166288\n",
      "iteration 300 / 1500: loss 2.078379\n",
      "iteration 400 / 1500: loss 2.040595\n",
      "iteration 500 / 1500: loss 2.104125\n",
      "iteration 600 / 1500: loss 2.141316\n",
      "iteration 700 / 1500: loss 2.099673\n",
      "iteration 800 / 1500: loss 2.050359\n",
      "iteration 900 / 1500: loss 2.159205\n",
      "iteration 1000 / 1500: loss 2.078322\n",
      "iteration 1100 / 1500: loss 2.173327\n",
      "iteration 1200 / 1500: loss 2.072011\n",
      "iteration 1300 / 1500: loss 2.101738\n",
      "iteration 1400 / 1500: loss 2.123763\n",
      "iteration 0 / 1500: loss 385.016713\n",
      "iteration 100 / 1500: loss 62.466174\n",
      "iteration 200 / 1500: loss 11.663674\n",
      "iteration 300 / 1500: loss 3.568399\n",
      "iteration 400 / 1500: loss 2.297022\n",
      "iteration 500 / 1500: loss 2.050911\n",
      "iteration 600 / 1500: loss 2.055273\n",
      "iteration 700 / 1500: loss 2.052302\n",
      "iteration 800 / 1500: loss 2.045256\n",
      "iteration 900 / 1500: loss 2.004885\n",
      "iteration 1000 / 1500: loss 2.067824\n",
      "iteration 1100 / 1500: loss 2.025918\n",
      "iteration 1200 / 1500: loss 2.054715\n",
      "iteration 1300 / 1500: loss 2.076317\n",
      "iteration 1400 / 1500: loss 2.056079\n",
      "iteration 0 / 1500: loss 1262.046083\n",
      "iteration 100 / 1500: loss 5.179864\n",
      "iteration 200 / 1500: loss 2.128352\n",
      "iteration 300 / 1500: loss 2.095536\n",
      "iteration 400 / 1500: loss 2.104403\n",
      "iteration 500 / 1500: loss 2.128423\n",
      "iteration 600 / 1500: loss 2.123810\n",
      "iteration 700 / 1500: loss 2.139763\n",
      "iteration 800 / 1500: loss 2.105425\n",
      "iteration 900 / 1500: loss 2.048422\n",
      "iteration 1000 / 1500: loss 2.113774\n",
      "iteration 1100 / 1500: loss 2.133765\n",
      "iteration 1200 / 1500: loss 2.149365\n",
      "iteration 1300 / 1500: loss 2.118758\n",
      "iteration 1400 / 1500: loss 2.112461\n",
      "iteration 0 / 1500: loss 1205.717404\n",
      "iteration 100 / 1500: loss 5.826773\n",
      "iteration 200 / 1500: loss 2.087875\n",
      "iteration 300 / 1500: loss 2.126287\n",
      "iteration 400 / 1500: loss 2.107293\n",
      "iteration 500 / 1500: loss 2.134880\n",
      "iteration 600 / 1500: loss 2.132604\n",
      "iteration 700 / 1500: loss 2.144633\n",
      "iteration 800 / 1500: loss 2.118311\n",
      "iteration 900 / 1500: loss 2.104966\n",
      "iteration 1000 / 1500: loss 2.164978\n",
      "iteration 1100 / 1500: loss 2.136826\n",
      "iteration 1200 / 1500: loss 2.068910\n",
      "iteration 1300 / 1500: loss 2.096087\n",
      "iteration 1400 / 1500: loss 2.141204\n",
      "iteration 0 / 1500: loss 393.494827\n",
      "iteration 100 / 1500: loss 64.364070\n",
      "iteration 200 / 1500: loss 12.026497\n",
      "iteration 300 / 1500: loss 3.583282\n",
      "iteration 400 / 1500: loss 2.299827\n",
      "iteration 500 / 1500: loss 2.049727\n",
      "iteration 600 / 1500: loss 2.032892\n",
      "iteration 700 / 1500: loss 2.060928\n",
      "iteration 800 / 1500: loss 1.989444\n",
      "iteration 900 / 1500: loss 2.016152\n",
      "iteration 1000 / 1500: loss 2.028268\n",
      "iteration 1100 / 1500: loss 2.058632\n",
      "iteration 1200 / 1500: loss 2.002798\n",
      "iteration 1300 / 1500: loss 2.047097\n",
      "iteration 1400 / 1500: loss 2.008842\n",
      "iteration 0 / 1500: loss 1274.171467\n",
      "iteration 100 / 1500: loss 4.933664\n",
      "iteration 200 / 1500: loss 2.156053\n",
      "iteration 300 / 1500: loss 2.085927\n",
      "iteration 400 / 1500: loss 2.118028\n",
      "iteration 500 / 1500: loss 2.140122\n",
      "iteration 600 / 1500: loss 2.120532\n",
      "iteration 700 / 1500: loss 2.091596\n",
      "iteration 800 / 1500: loss 2.128054\n",
      "iteration 900 / 1500: loss 2.138927\n",
      "iteration 1000 / 1500: loss 2.146621\n",
      "iteration 1100 / 1500: loss 2.155495\n",
      "iteration 1200 / 1500: loss 2.116368\n",
      "iteration 1300 / 1500: loss 2.108847\n",
      "iteration 1400 / 1500: loss 2.110047\n",
      "iteration 0 / 1500: loss 462.373496\n",
      "iteration 100 / 1500: loss 297.123348\n",
      "iteration 200 / 1500: loss 191.658231\n",
      "iteration 300 / 1500: loss 123.991834\n",
      "iteration 400 / 1500: loss 80.364757\n",
      "iteration 500 / 1500: loss 52.513739\n",
      "iteration 600 / 1500: loss 34.418537\n",
      "iteration 700 / 1500: loss 22.818535\n",
      "iteration 800 / 1500: loss 15.494170\n",
      "iteration 900 / 1500: loss 10.611504\n",
      "iteration 1000 / 1500: loss 7.591368\n",
      "iteration 1100 / 1500: loss 5.640992\n",
      "iteration 1200 / 1500: loss 4.326962\n",
      "iteration 1300 / 1500: loss 3.538664\n",
      "iteration 1400 / 1500: loss 3.002341\n",
      "iteration 0 / 1500: loss 422.242947\n",
      "iteration 100 / 1500: loss 282.859242\n",
      "iteration 200 / 1500: loss 190.596567\n",
      "iteration 300 / 1500: loss 128.465161\n",
      "iteration 400 / 1500: loss 86.942624\n",
      "iteration 500 / 1500: loss 59.087903\n",
      "iteration 600 / 1500: loss 40.341216\n",
      "iteration 700 / 1500: loss 27.788437\n",
      "iteration 800 / 1500: loss 19.376495\n",
      "iteration 900 / 1500: loss 13.606550\n",
      "iteration 1000 / 1500: loss 9.837216\n",
      "iteration 1100 / 1500: loss 7.247821\n",
      "iteration 1200 / 1500: loss 5.529027\n",
      "iteration 1300 / 1500: loss 4.396785\n",
      "iteration 1400 / 1500: loss 3.547650\n",
      "iteration 0 / 1500: loss 553.650439\n",
      "iteration 100 / 1500: loss 325.849052\n",
      "iteration 200 / 1500: loss 192.092665\n",
      "iteration 300 / 1500: loss 113.877445\n",
      "iteration 400 / 1500: loss 67.833524\n",
      "iteration 500 / 1500: loss 40.799066\n",
      "iteration 600 / 1500: loss 24.819652\n",
      "iteration 700 / 1500: loss 15.502415\n",
      "iteration 800 / 1500: loss 9.871781\n",
      "iteration 900 / 1500: loss 6.707279\n",
      "iteration 1000 / 1500: loss 4.759611\n",
      "iteration 1100 / 1500: loss 3.638557\n",
      "iteration 1200 / 1500: loss 2.931578\n",
      "iteration 1300 / 1500: loss 2.638552\n",
      "iteration 1400 / 1500: loss 2.386953\n",
      "iteration 0 / 1500: loss 1537.088015\n",
      "iteration 100 / 1500: loss 352.390814\n",
      "iteration 200 / 1500: loss 82.044451\n",
      "iteration 300 / 1500: loss 20.413665\n",
      "iteration 400 / 1500: loss 6.307446\n",
      "iteration 500 / 1500: loss 3.075983\n",
      "iteration 600 / 1500: loss 2.385729\n",
      "iteration 700 / 1500: loss 2.227176\n",
      "iteration 800 / 1500: loss 2.127317\n",
      "iteration 900 / 1500: loss 2.140835\n",
      "iteration 1000 / 1500: loss 2.104414\n",
      "iteration 1100 / 1500: loss 2.122495\n",
      "iteration 1200 / 1500: loss 2.142925\n",
      "iteration 1300 / 1500: loss 2.163866\n",
      "iteration 1400 / 1500: loss 2.130258\n",
      "iteration 0 / 1500: loss 1512.729380\n",
      "iteration 100 / 1500: loss 354.969060\n",
      "iteration 200 / 1500: loss 84.541236\n",
      "iteration 300 / 1500: loss 21.400823\n",
      "iteration 400 / 1500: loss 6.663766\n",
      "iteration 500 / 1500: loss 3.170592\n",
      "iteration 600 / 1500: loss 2.386179\n",
      "iteration 700 / 1500: loss 2.252053\n",
      "iteration 800 / 1500: loss 2.121539\n",
      "iteration 900 / 1500: loss 2.153975\n",
      "iteration 1000 / 1500: loss 2.115489\n",
      "iteration 1100 / 1500: loss 2.129630\n",
      "iteration 1200 / 1500: loss 2.168401\n",
      "iteration 1300 / 1500: loss 2.131097\n",
      "iteration 1400 / 1500: loss 2.171940\n",
      "iteration 0 / 1500: loss 813.005069\n",
      "iteration 100 / 1500: loss 372.662536\n",
      "iteration 200 / 1500: loss 171.738786\n",
      "iteration 300 / 1500: loss 79.681375\n",
      "iteration 400 / 1500: loss 37.692784\n",
      "iteration 500 / 1500: loss 18.386893\n",
      "iteration 600 / 1500: loss 9.587030\n",
      "iteration 700 / 1500: loss 5.475828\n",
      "iteration 800 / 1500: loss 3.597938\n",
      "iteration 900 / 1500: loss 2.791807\n",
      "iteration 1000 / 1500: loss 2.400903\n",
      "iteration 1100 / 1500: loss 2.217726\n",
      "iteration 1200 / 1500: loss 2.193151\n",
      "iteration 1300 / 1500: loss 2.157150\n",
      "iteration 1400 / 1500: loss 2.099370\n",
      "iteration 0 / 1500: loss 985.611348\n",
      "iteration 100 / 1500: loss 386.196155\n",
      "iteration 200 / 1500: loss 152.364889\n",
      "iteration 300 / 1500: loss 60.868267\n",
      "iteration 400 / 1500: loss 25.127888\n",
      "iteration 500 / 1500: loss 11.117860\n",
      "iteration 600 / 1500: loss 5.598009\n",
      "iteration 700 / 1500: loss 3.479980\n",
      "iteration 800 / 1500: loss 2.699747\n",
      "iteration 900 / 1500: loss 2.342642\n",
      "iteration 1000 / 1500: loss 2.178502\n",
      "iteration 1100 / 1500: loss 2.117931\n",
      "iteration 1200 / 1500: loss 2.120303\n",
      "iteration 1300 / 1500: loss 2.123529\n",
      "iteration 1400 / 1500: loss 2.107695\n",
      "iteration 0 / 1500: loss 1069.114989\n",
      "iteration 100 / 1500: loss 383.925943\n",
      "iteration 200 / 1500: loss 138.990910\n",
      "iteration 300 / 1500: loss 51.163814\n",
      "iteration 400 / 1500: loss 19.675184\n",
      "iteration 500 / 1500: loss 8.401727\n",
      "iteration 600 / 1500: loss 4.348913\n",
      "iteration 700 / 1500: loss 2.946214\n",
      "iteration 800 / 1500: loss 2.365521\n",
      "iteration 900 / 1500: loss 2.236582\n",
      "iteration 1000 / 1500: loss 2.130253\n",
      "iteration 1100 / 1500: loss 2.074006\n",
      "iteration 1200 / 1500: loss 2.134252\n",
      "iteration 1300 / 1500: loss 2.125003\n",
      "iteration 1400 / 1500: loss 2.119319\n",
      "iteration 0 / 1500: loss 1444.107740\n",
      "iteration 100 / 1500: loss 357.834165\n",
      "iteration 200 / 1500: loss 89.998548\n",
      "iteration 300 / 1500: loss 23.793449\n",
      "iteration 400 / 1500: loss 7.501243\n",
      "iteration 500 / 1500: loss 3.414165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 1500: loss 2.469053\n",
      "iteration 700 / 1500: loss 2.217037\n",
      "iteration 800 / 1500: loss 2.163509\n",
      "iteration 900 / 1500: loss 2.108792\n",
      "iteration 1000 / 1500: loss 2.124409\n",
      "iteration 1100 / 1500: loss 2.119258\n",
      "iteration 1200 / 1500: loss 2.128372\n",
      "iteration 1300 / 1500: loss 2.131754\n",
      "iteration 1400 / 1500: loss 2.135079\n",
      "iteration 0 / 1500: loss 470.663227\n",
      "iteration 100 / 1500: loss 299.536576\n",
      "iteration 200 / 1500: loss 191.097903\n",
      "iteration 300 / 1500: loss 122.405194\n",
      "iteration 400 / 1500: loss 78.827096\n",
      "iteration 500 / 1500: loss 50.953348\n",
      "iteration 600 / 1500: loss 33.089452\n",
      "iteration 700 / 1500: loss 21.877747\n",
      "iteration 800 / 1500: loss 14.707193\n",
      "iteration 900 / 1500: loss 10.067150\n",
      "iteration 1000 / 1500: loss 7.170521\n",
      "iteration 1100 / 1500: loss 5.354899\n",
      "iteration 1200 / 1500: loss 4.128990\n",
      "iteration 1300 / 1500: loss 3.398532\n",
      "iteration 1400 / 1500: loss 2.855702\n",
      "iteration 0 / 1500: loss 327.226003\n",
      "iteration 100 / 1500: loss 238.723572\n",
      "iteration 200 / 1500: loss 175.465707\n",
      "iteration 300 / 1500: loss 128.583838\n",
      "iteration 400 / 1500: loss 94.882713\n",
      "iteration 500 / 1500: loss 70.024438\n",
      "iteration 600 / 1500: loss 51.801482\n",
      "iteration 700 / 1500: loss 38.405023\n",
      "iteration 800 / 1500: loss 28.557320\n",
      "iteration 900 / 1500: loss 21.528146\n",
      "iteration 1000 / 1500: loss 16.258918\n",
      "iteration 1100 / 1500: loss 12.474317\n",
      "iteration 1200 / 1500: loss 9.665907\n",
      "iteration 1300 / 1500: loss 7.641635\n",
      "iteration 1400 / 1500: loss 6.121627\n",
      "iteration 0 / 1500: loss 361.938184\n",
      "iteration 100 / 1500: loss 256.746733\n",
      "iteration 200 / 1500: loss 182.645596\n",
      "iteration 300 / 1500: loss 130.306031\n",
      "iteration 400 / 1500: loss 93.351860\n",
      "iteration 500 / 1500: loss 66.761031\n",
      "iteration 600 / 1500: loss 47.991413\n",
      "iteration 700 / 1500: loss 34.631848\n",
      "iteration 800 / 1500: loss 25.336860\n",
      "iteration 900 / 1500: loss 18.589411\n",
      "iteration 1000 / 1500: loss 13.821368\n",
      "iteration 1100 / 1500: loss 10.299605\n",
      "iteration 1200 / 1500: loss 7.902915\n",
      "iteration 1300 / 1500: loss 6.349712\n",
      "iteration 1400 / 1500: loss 5.058074\n",
      "iteration 0 / 1500: loss 653.943123\n",
      "iteration 100 / 1500: loss 350.092949\n",
      "iteration 200 / 1500: loss 188.121098\n",
      "iteration 300 / 1500: loss 101.647547\n",
      "iteration 400 / 1500: loss 55.277501\n",
      "iteration 500 / 1500: loss 30.530923\n",
      "iteration 600 / 1500: loss 17.230643\n",
      "iteration 700 / 1500: loss 10.216914\n",
      "iteration 800 / 1500: loss 6.446620\n",
      "iteration 900 / 1500: loss 4.352225\n",
      "iteration 1000 / 1500: loss 3.303643\n",
      "iteration 1100 / 1500: loss 2.702415\n",
      "iteration 1200 / 1500: loss 2.382204\n",
      "iteration 1300 / 1500: loss 2.262141\n",
      "iteration 1400 / 1500: loss 2.176090\n",
      "iteration 0 / 1500: loss 401.805466\n",
      "iteration 100 / 1500: loss 274.926179\n",
      "iteration 200 / 1500: loss 188.279975\n",
      "iteration 300 / 1500: loss 129.358601\n",
      "iteration 400 / 1500: loss 88.932474\n",
      "iteration 500 / 1500: loss 61.400111\n",
      "iteration 600 / 1500: loss 42.641339\n",
      "iteration 700 / 1500: loss 29.796941\n",
      "iteration 800 / 1500: loss 20.962491\n",
      "iteration 900 / 1500: loss 15.055901\n",
      "iteration 1000 / 1500: loss 10.978172\n",
      "iteration 1100 / 1500: loss 8.110008\n",
      "iteration 1200 / 1500: loss 6.104572\n",
      "iteration 1300 / 1500: loss 4.812333\n",
      "iteration 1400 / 1500: loss 3.963644\n",
      "iteration 0 / 1500: loss 1095.278992\n",
      "iteration 100 / 1500: loss 383.907840\n",
      "iteration 200 / 1500: loss 135.308841\n",
      "iteration 300 / 1500: loss 48.827623\n",
      "iteration 400 / 1500: loss 18.478979\n",
      "iteration 500 / 1500: loss 7.841457\n",
      "iteration 600 / 1500: loss 4.108350\n",
      "iteration 700 / 1500: loss 2.810700\n",
      "iteration 800 / 1500: loss 2.326401\n",
      "iteration 900 / 1500: loss 2.191313\n",
      "iteration 1000 / 1500: loss 2.193265\n",
      "iteration 1100 / 1500: loss 2.126675\n",
      "iteration 1200 / 1500: loss 2.106148\n",
      "iteration 1300 / 1500: loss 2.143234\n",
      "iteration 1400 / 1500: loss 2.123376\n",
      "iteration 0 / 1500: loss 389.895161\n",
      "iteration 100 / 1500: loss 268.334732\n",
      "iteration 200 / 1500: loss 185.422874\n",
      "iteration 300 / 1500: loss 128.155996\n",
      "iteration 400 / 1500: loss 89.095150\n",
      "iteration 500 / 1500: loss 61.981618\n",
      "iteration 600 / 1500: loss 43.335440\n",
      "iteration 700 / 1500: loss 30.461523\n",
      "iteration 800 / 1500: loss 21.682610\n",
      "iteration 900 / 1500: loss 15.542036\n",
      "iteration 1000 / 1500: loss 11.355633\n",
      "iteration 1100 / 1500: loss 8.462915\n",
      "iteration 1200 / 1500: loss 6.458754\n",
      "iteration 1300 / 1500: loss 5.046837\n",
      "iteration 1400 / 1500: loss 4.231641\n",
      "iteration 0 / 1500: loss 1254.864664\n",
      "iteration 100 / 1500: loss 376.098490\n",
      "iteration 200 / 1500: loss 113.816614\n",
      "iteration 300 / 1500: loss 35.521523\n",
      "iteration 400 / 1500: loss 12.148615\n",
      "iteration 500 / 1500: loss 5.084225\n",
      "iteration 600 / 1500: loss 3.014602\n",
      "iteration 700 / 1500: loss 2.404293\n",
      "iteration 800 / 1500: loss 2.223929\n",
      "iteration 900 / 1500: loss 2.155227\n",
      "iteration 1000 / 1500: loss 2.127248\n",
      "iteration 1100 / 1500: loss 2.146197\n",
      "iteration 1200 / 1500: loss 2.115039\n",
      "iteration 1300 / 1500: loss 2.155663\n",
      "iteration 1400 / 1500: loss 2.096908\n",
      "iteration 0 / 1500: loss 1200.708532\n",
      "iteration 100 / 1500: loss 377.389261\n",
      "iteration 200 / 1500: loss 119.691286\n",
      "iteration 300 / 1500: loss 38.996631\n",
      "iteration 400 / 1500: loss 13.673530\n",
      "iteration 500 / 1500: loss 5.748637\n",
      "iteration 600 / 1500: loss 3.288985\n",
      "iteration 700 / 1500: loss 2.521396\n",
      "iteration 800 / 1500: loss 2.295561\n",
      "iteration 900 / 1500: loss 2.204352\n",
      "iteration 1000 / 1500: loss 2.164465\n",
      "iteration 1100 / 1500: loss 2.135487\n",
      "iteration 1200 / 1500: loss 2.076613\n",
      "iteration 1300 / 1500: loss 2.148379\n",
      "iteration 1400 / 1500: loss 2.070697\n",
      "iteration 0 / 1500: loss 388.292601\n",
      "iteration 100 / 1500: loss 268.133543\n",
      "iteration 200 / 1500: loss 185.520847\n",
      "iteration 300 / 1500: loss 128.881610\n",
      "iteration 400 / 1500: loss 89.596442\n",
      "iteration 500 / 1500: loss 62.496523\n",
      "iteration 600 / 1500: loss 43.844642\n",
      "iteration 700 / 1500: loss 30.884047\n",
      "iteration 800 / 1500: loss 21.916112\n",
      "iteration 900 / 1500: loss 15.773332\n",
      "iteration 1000 / 1500: loss 11.473030\n",
      "iteration 1100 / 1500: loss 8.676144\n",
      "iteration 1200 / 1500: loss 6.477757\n",
      "iteration 1300 / 1500: loss 5.096435\n",
      "iteration 1400 / 1500: loss 4.163888\n",
      "iteration 0 / 1500: loss 1268.118847\n",
      "iteration 100 / 1500: loss 372.345577\n",
      "iteration 200 / 1500: loss 110.419403\n",
      "iteration 300 / 1500: loss 33.799166\n",
      "iteration 400 / 1500: loss 11.456245\n",
      "iteration 500 / 1500: loss 4.833456\n",
      "iteration 600 / 1500: loss 2.958618\n",
      "iteration 700 / 1500: loss 2.401920\n",
      "iteration 800 / 1500: loss 2.204602\n",
      "iteration 900 / 1500: loss 2.168023\n",
      "iteration 1000 / 1500: loss 2.108757\n",
      "iteration 1100 / 1500: loss 2.122999\n",
      "iteration 1200 / 1500: loss 2.120296\n",
      "iteration 1300 / 1500: loss 2.203500\n",
      "iteration 1400 / 1500: loss 2.121024\n",
      "iteration 0 / 1500: loss 464.415244\n",
      "iteration 100 / 1500: loss 96.638888\n",
      "iteration 200 / 1500: loss 21.421005\n",
      "iteration 300 / 1500: loss 6.054389\n",
      "iteration 400 / 1500: loss 2.864638\n",
      "iteration 500 / 1500: loss 2.198897\n",
      "iteration 600 / 1500: loss 2.068541\n",
      "iteration 700 / 1500: loss 1.999446\n",
      "iteration 800 / 1500: loss 2.079177\n",
      "iteration 900 / 1500: loss 1.994996\n",
      "iteration 1000 / 1500: loss 2.061446\n",
      "iteration 1100 / 1500: loss 2.014169\n",
      "iteration 1200 / 1500: loss 2.077542\n",
      "iteration 1300 / 1500: loss 2.002023\n",
      "iteration 1400 / 1500: loss 2.060964\n",
      "iteration 0 / 1500: loss 416.856166\n",
      "iteration 100 / 1500: loss 101.010711\n",
      "iteration 200 / 1500: loss 25.736362\n",
      "iteration 300 / 1500: loss 7.719365\n",
      "iteration 400 / 1500: loss 3.424264\n",
      "iteration 500 / 1500: loss 2.394632\n",
      "iteration 600 / 1500: loss 2.106523\n",
      "iteration 700 / 1500: loss 2.054634\n",
      "iteration 800 / 1500: loss 2.050075\n",
      "iteration 900 / 1500: loss 2.077011\n",
      "iteration 1000 / 1500: loss 2.065091\n",
      "iteration 1100 / 1500: loss 1.965411\n",
      "iteration 1200 / 1500: loss 2.108936\n",
      "iteration 1300 / 1500: loss 2.046220\n",
      "iteration 1400 / 1500: loss 2.051511\n",
      "iteration 0 / 1500: loss 549.985522\n",
      "iteration 100 / 1500: loss 83.256666\n",
      "iteration 200 / 1500: loss 14.157716\n",
      "iteration 300 / 1500: loss 3.856283\n",
      "iteration 400 / 1500: loss 2.344046\n",
      "iteration 500 / 1500: loss 2.003185\n",
      "iteration 600 / 1500: loss 2.074572\n",
      "iteration 700 / 1500: loss 2.063848\n",
      "iteration 800 / 1500: loss 2.020768\n",
      "iteration 900 / 1500: loss 2.067041\n",
      "iteration 1000 / 1500: loss 2.127629\n",
      "iteration 1100 / 1500: loss 2.105262\n",
      "iteration 1200 / 1500: loss 2.039705\n",
      "iteration 1300 / 1500: loss 2.091850\n",
      "iteration 1400 / 1500: loss 2.082075\n",
      "iteration 0 / 1500: loss 1548.354982\n",
      "iteration 100 / 1500: loss 9.530190\n",
      "iteration 200 / 1500: loss 2.203066\n",
      "iteration 300 / 1500: loss 2.175451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 1500: loss 2.123129\n",
      "iteration 500 / 1500: loss 2.136358\n",
      "iteration 600 / 1500: loss 2.115884\n",
      "iteration 700 / 1500: loss 2.153752\n",
      "iteration 800 / 1500: loss 2.152174\n",
      "iteration 900 / 1500: loss 2.138752\n",
      "iteration 1000 / 1500: loss 2.178200\n",
      "iteration 1100 / 1500: loss 2.146350\n",
      "iteration 1200 / 1500: loss 2.151964\n",
      "iteration 1300 / 1500: loss 2.142930\n",
      "iteration 1400 / 1500: loss 2.192010\n",
      "iteration 0 / 1500: loss 1519.030275\n",
      "iteration 100 / 1500: loss 10.095753\n",
      "iteration 200 / 1500: loss 2.206194\n",
      "iteration 300 / 1500: loss 2.164437\n",
      "iteration 400 / 1500: loss 2.158309\n",
      "iteration 500 / 1500: loss 2.098695\n",
      "iteration 600 / 1500: loss 2.114842\n",
      "iteration 700 / 1500: loss 2.155334\n",
      "iteration 800 / 1500: loss 2.162662\n",
      "iteration 900 / 1500: loss 2.110659\n",
      "iteration 1000 / 1500: loss 2.151008\n",
      "iteration 1100 / 1500: loss 2.166587\n",
      "iteration 1200 / 1500: loss 2.100341\n",
      "iteration 1300 / 1500: loss 2.156782\n",
      "iteration 1400 / 1500: loss 2.173942\n",
      "iteration 0 / 1500: loss 802.980722\n",
      "iteration 100 / 1500: loss 50.106935\n",
      "iteration 200 / 1500: loss 4.990320\n",
      "iteration 300 / 1500: loss 2.306287\n",
      "iteration 400 / 1500: loss 2.033939\n",
      "iteration 500 / 1500: loss 2.135390\n",
      "iteration 600 / 1500: loss 2.105577\n",
      "iteration 700 / 1500: loss 2.107414\n",
      "iteration 800 / 1500: loss 2.109387\n",
      "iteration 900 / 1500: loss 2.059816\n",
      "iteration 1000 / 1500: loss 2.093739\n",
      "iteration 1100 / 1500: loss 2.066450\n",
      "iteration 1200 / 1500: loss 2.126599\n",
      "iteration 1300 / 1500: loss 2.057324\n",
      "iteration 1400 / 1500: loss 2.084272\n",
      "iteration 0 / 1500: loss 979.048445\n",
      "iteration 100 / 1500: loss 35.427344\n",
      "iteration 200 / 1500: loss 3.239450\n",
      "iteration 300 / 1500: loss 2.150122\n",
      "iteration 400 / 1500: loss 2.109468\n",
      "iteration 500 / 1500: loss 2.062751\n",
      "iteration 600 / 1500: loss 2.042729\n",
      "iteration 700 / 1500: loss 2.076816\n",
      "iteration 800 / 1500: loss 2.105408\n",
      "iteration 900 / 1500: loss 2.153886\n",
      "iteration 1000 / 1500: loss 2.117419\n",
      "iteration 1100 / 1500: loss 2.127421\n",
      "iteration 1200 / 1500: loss 2.112294\n",
      "iteration 1300 / 1500: loss 2.055864\n",
      "iteration 1400 / 1500: loss 2.083960\n",
      "iteration 0 / 1500: loss 1062.241417\n",
      "iteration 100 / 1500: loss 28.317945\n",
      "iteration 200 / 1500: loss 2.793515\n",
      "iteration 300 / 1500: loss 2.165909\n",
      "iteration 400 / 1500: loss 2.129694\n",
      "iteration 500 / 1500: loss 2.148652\n",
      "iteration 600 / 1500: loss 2.131949\n",
      "iteration 700 / 1500: loss 2.053100\n",
      "iteration 800 / 1500: loss 2.125853\n",
      "iteration 900 / 1500: loss 2.116603\n",
      "iteration 1000 / 1500: loss 2.070278\n",
      "iteration 1100 / 1500: loss 2.187313\n",
      "iteration 1200 / 1500: loss 2.073246\n",
      "iteration 1300 / 1500: loss 2.096155\n",
      "iteration 1400 / 1500: loss 2.094385\n",
      "iteration 0 / 1500: loss 1437.356468\n",
      "iteration 100 / 1500: loss 11.277210\n",
      "iteration 200 / 1500: loss 2.263107\n",
      "iteration 300 / 1500: loss 2.132335\n",
      "iteration 400 / 1500: loss 2.175256\n",
      "iteration 500 / 1500: loss 2.146103\n",
      "iteration 600 / 1500: loss 2.146070\n",
      "iteration 700 / 1500: loss 2.116626\n",
      "iteration 800 / 1500: loss 2.167859\n",
      "iteration 900 / 1500: loss 2.084648\n",
      "iteration 1000 / 1500: loss 2.148977\n",
      "iteration 1100 / 1500: loss 2.158518\n",
      "iteration 1200 / 1500: loss 2.161834\n",
      "iteration 1300 / 1500: loss 2.136564\n",
      "iteration 1400 / 1500: loss 2.185259\n",
      "iteration 0 / 1500: loss 470.866708\n",
      "iteration 100 / 1500: loss 94.408834\n",
      "iteration 200 / 1500: loss 20.269428\n",
      "iteration 300 / 1500: loss 5.661034\n",
      "iteration 400 / 1500: loss 2.734371\n",
      "iteration 500 / 1500: loss 2.266136\n",
      "iteration 600 / 1500: loss 2.086483\n",
      "iteration 700 / 1500: loss 2.036872\n",
      "iteration 800 / 1500: loss 2.046077\n",
      "iteration 900 / 1500: loss 1.958988\n",
      "iteration 1000 / 1500: loss 2.089943\n",
      "iteration 1100 / 1500: loss 2.038382\n",
      "iteration 1200 / 1500: loss 2.072686\n",
      "iteration 1300 / 1500: loss 2.054175\n",
      "iteration 1400 / 1500: loss 2.135030\n",
      "iteration 0 / 1500: loss 325.100071\n",
      "iteration 100 / 1500: loss 107.077697\n",
      "iteration 200 / 1500: loss 36.428077\n",
      "iteration 300 / 1500: loss 13.174303\n",
      "iteration 400 / 1500: loss 5.712839\n",
      "iteration 500 / 1500: loss 3.155330\n",
      "iteration 600 / 1500: loss 2.367370\n",
      "iteration 700 / 1500: loss 2.132280\n",
      "iteration 800 / 1500: loss 1.977972\n",
      "iteration 900 / 1500: loss 1.998811\n",
      "iteration 1000 / 1500: loss 2.042690\n",
      "iteration 1100 / 1500: loss 1.991601\n",
      "iteration 1200 / 1500: loss 1.984577\n",
      "iteration 1300 / 1500: loss 2.038515\n",
      "iteration 1400 / 1500: loss 2.008674\n",
      "iteration 0 / 1500: loss 360.483145\n",
      "iteration 100 / 1500: loss 106.556299\n",
      "iteration 200 / 1500: loss 32.696631\n",
      "iteration 300 / 1500: loss 11.036286\n",
      "iteration 400 / 1500: loss 4.655064\n",
      "iteration 500 / 1500: loss 2.799062\n",
      "iteration 600 / 1500: loss 2.243509\n",
      "iteration 700 / 1500: loss 2.104733\n",
      "iteration 800 / 1500: loss 2.098668\n",
      "iteration 900 / 1500: loss 2.048097\n",
      "iteration 1000 / 1500: loss 2.093498\n",
      "iteration 1100 / 1500: loss 2.079282\n",
      "iteration 1200 / 1500: loss 2.073939\n",
      "iteration 1300 / 1500: loss 2.065221\n",
      "iteration 1400 / 1500: loss 1.987834\n",
      "iteration 0 / 1500: loss 653.186979\n",
      "iteration 100 / 1500: loss 70.527929\n",
      "iteration 200 / 1500: loss 9.320496\n",
      "iteration 300 / 1500: loss 2.796722\n",
      "iteration 400 / 1500: loss 2.140370\n",
      "iteration 500 / 1500: loss 2.111908\n",
      "iteration 600 / 1500: loss 1.970084\n",
      "iteration 700 / 1500: loss 2.096668\n",
      "iteration 800 / 1500: loss 2.008048\n",
      "iteration 900 / 1500: loss 2.075064\n",
      "iteration 1000 / 1500: loss 2.085157\n",
      "iteration 1100 / 1500: loss 2.069857\n",
      "iteration 1200 / 1500: loss 2.096979\n",
      "iteration 1300 / 1500: loss 2.007585\n",
      "iteration 1400 / 1500: loss 2.057468\n",
      "iteration 0 / 1500: loss 392.278325\n",
      "iteration 100 / 1500: loss 101.119105\n",
      "iteration 200 / 1500: loss 27.343472\n",
      "iteration 300 / 1500: loss 8.466877\n",
      "iteration 400 / 1500: loss 3.637013\n",
      "iteration 500 / 1500: loss 2.459756\n",
      "iteration 600 / 1500: loss 2.054789\n",
      "iteration 700 / 1500: loss 2.082513\n",
      "iteration 800 / 1500: loss 1.969106\n",
      "iteration 900 / 1500: loss 2.025368\n",
      "iteration 1000 / 1500: loss 2.052268\n",
      "iteration 1100 / 1500: loss 2.039690\n",
      "iteration 1200 / 1500: loss 2.037476\n",
      "iteration 1300 / 1500: loss 2.027873\n",
      "iteration 1400 / 1500: loss 1.995938\n",
      "iteration 0 / 1500: loss 1082.433409\n",
      "iteration 100 / 1500: loss 26.599914\n",
      "iteration 200 / 1500: loss 2.667674\n",
      "iteration 300 / 1500: loss 2.082434\n",
      "iteration 400 / 1500: loss 2.135068\n",
      "iteration 500 / 1500: loss 2.131163\n",
      "iteration 600 / 1500: loss 2.113800\n",
      "iteration 700 / 1500: loss 2.090246\n",
      "iteration 800 / 1500: loss 2.099930\n",
      "iteration 900 / 1500: loss 2.090978\n",
      "iteration 1000 / 1500: loss 2.103536\n",
      "iteration 1100 / 1500: loss 2.074628\n",
      "iteration 1200 / 1500: loss 2.155662\n",
      "iteration 1300 / 1500: loss 2.096406\n",
      "iteration 1400 / 1500: loss 2.156469\n",
      "iteration 0 / 1500: loss 385.944712\n",
      "iteration 100 / 1500: loss 102.595605\n",
      "iteration 200 / 1500: loss 28.480993\n",
      "iteration 300 / 1500: loss 8.950316\n",
      "iteration 400 / 1500: loss 3.828604\n",
      "iteration 500 / 1500: loss 2.560583\n",
      "iteration 600 / 1500: loss 2.132943\n",
      "iteration 700 / 1500: loss 2.099922\n",
      "iteration 800 / 1500: loss 2.019360\n",
      "iteration 900 / 1500: loss 2.022976\n",
      "iteration 1000 / 1500: loss 1.990160\n",
      "iteration 1100 / 1500: loss 2.023551\n",
      "iteration 1200 / 1500: loss 2.019423\n",
      "iteration 1300 / 1500: loss 2.045941\n",
      "iteration 1400 / 1500: loss 2.091109\n",
      "iteration 0 / 1500: loss 1230.771680\n",
      "iteration 100 / 1500: loss 17.878011\n",
      "iteration 200 / 1500: loss 2.325248\n",
      "iteration 300 / 1500: loss 2.157297\n",
      "iteration 400 / 1500: loss 2.138771\n",
      "iteration 500 / 1500: loss 2.098393\n",
      "iteration 600 / 1500: loss 2.180425\n",
      "iteration 700 / 1500: loss 2.173713\n",
      "iteration 800 / 1500: loss 2.172769\n",
      "iteration 900 / 1500: loss 2.118492\n",
      "iteration 1000 / 1500: loss 2.148186\n",
      "iteration 1100 / 1500: loss 2.143129\n",
      "iteration 1200 / 1500: loss 2.149283\n",
      "iteration 1300 / 1500: loss 2.154420\n",
      "iteration 1400 / 1500: loss 2.136273\n",
      "iteration 0 / 1500: loss 1206.181164\n",
      "iteration 100 / 1500: loss 20.452829\n",
      "iteration 200 / 1500: loss 2.453837\n",
      "iteration 300 / 1500: loss 2.089085\n",
      "iteration 400 / 1500: loss 2.152940\n",
      "iteration 500 / 1500: loss 2.162007\n",
      "iteration 600 / 1500: loss 2.102350\n",
      "iteration 700 / 1500: loss 2.149707\n",
      "iteration 800 / 1500: loss 2.139182\n",
      "iteration 900 / 1500: loss 2.123067\n",
      "iteration 1000 / 1500: loss 2.105182\n",
      "iteration 1100 / 1500: loss 2.148594\n",
      "iteration 1200 / 1500: loss 2.140514\n",
      "iteration 1300 / 1500: loss 2.157370\n",
      "iteration 1400 / 1500: loss 2.157355\n",
      "iteration 0 / 1500: loss 388.690981\n",
      "iteration 100 / 1500: loss 103.976569\n",
      "iteration 200 / 1500: loss 29.080898\n",
      "iteration 300 / 1500: loss 9.101847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 1500: loss 3.882865\n",
      "iteration 500 / 1500: loss 2.531648\n",
      "iteration 600 / 1500: loss 2.158657\n",
      "iteration 700 / 1500: loss 2.105290\n",
      "iteration 800 / 1500: loss 1.993230\n",
      "iteration 900 / 1500: loss 2.022560\n",
      "iteration 1000 / 1500: loss 2.008689\n",
      "iteration 1100 / 1500: loss 2.046501\n",
      "iteration 1200 / 1500: loss 1.924894\n",
      "iteration 1300 / 1500: loss 1.983538\n",
      "iteration 1400 / 1500: loss 2.061286\n",
      "iteration 0 / 1500: loss 1285.161846\n",
      "iteration 100 / 1500: loss 17.363435\n",
      "iteration 200 / 1500: loss 2.349429\n",
      "iteration 300 / 1500: loss 2.115615\n",
      "iteration 400 / 1500: loss 2.134763\n",
      "iteration 500 / 1500: loss 2.100461\n",
      "iteration 600 / 1500: loss 2.167945\n",
      "iteration 700 / 1500: loss 2.121397\n",
      "iteration 800 / 1500: loss 2.089324\n",
      "iteration 900 / 1500: loss 2.147472\n",
      "iteration 1000 / 1500: loss 2.130631\n",
      "iteration 1100 / 1500: loss 2.137421\n",
      "iteration 1200 / 1500: loss 2.157707\n",
      "iteration 1300 / 1500: loss 2.065068\n",
      "iteration 1400 / 1500: loss 2.097744\n",
      "iteration 0 / 1500: loss 457.964889\n",
      "iteration 100 / 1500: loss 212.556497\n",
      "iteration 200 / 1500: loss 99.625778\n",
      "iteration 300 / 1500: loss 47.201228\n",
      "iteration 400 / 1500: loss 23.107795\n",
      "iteration 500 / 1500: loss 11.855052\n",
      "iteration 600 / 1500: loss 6.590572\n",
      "iteration 700 / 1500: loss 4.078537\n",
      "iteration 800 / 1500: loss 3.049637\n",
      "iteration 900 / 1500: loss 2.458928\n",
      "iteration 1000 / 1500: loss 2.194622\n",
      "iteration 1100 / 1500: loss 2.104599\n",
      "iteration 1200 / 1500: loss 2.149397\n",
      "iteration 1300 / 1500: loss 2.132748\n",
      "iteration 1400 / 1500: loss 2.082165\n",
      "iteration 0 / 1500: loss 414.181269\n",
      "iteration 100 / 1500: loss 207.124975\n",
      "iteration 200 / 1500: loss 104.370881\n",
      "iteration 300 / 1500: loss 53.207503\n",
      "iteration 400 / 1500: loss 27.653831\n",
      "iteration 500 / 1500: loss 14.823073\n",
      "iteration 600 / 1500: loss 8.480342\n",
      "iteration 700 / 1500: loss 5.219713\n",
      "iteration 800 / 1500: loss 3.552343\n",
      "iteration 900 / 1500: loss 2.843608\n",
      "iteration 1000 / 1500: loss 2.407348\n",
      "iteration 1100 / 1500: loss 2.259249\n",
      "iteration 1200 / 1500: loss 2.090357\n",
      "iteration 1300 / 1500: loss 2.039503\n",
      "iteration 1400 / 1500: loss 2.067066\n",
      "iteration 0 / 1500: loss 564.273424\n",
      "iteration 100 / 1500: loss 224.639922\n",
      "iteration 200 / 1500: loss 90.328170\n",
      "iteration 300 / 1500: loss 37.058626\n",
      "iteration 400 / 1500: loss 15.976364\n",
      "iteration 500 / 1500: loss 7.575519\n",
      "iteration 600 / 1500: loss 4.271245\n",
      "iteration 700 / 1500: loss 2.953661\n",
      "iteration 800 / 1500: loss 2.427903\n",
      "iteration 900 / 1500: loss 2.144806\n",
      "iteration 1000 / 1500: loss 2.039326\n",
      "iteration 1100 / 1500: loss 2.100275\n",
      "iteration 1200 / 1500: loss 2.062070\n",
      "iteration 1300 / 1500: loss 2.121737\n",
      "iteration 1400 / 1500: loss 2.075973\n",
      "iteration 0 / 1500: loss 1541.037553\n",
      "iteration 100 / 1500: loss 118.999256\n",
      "iteration 200 / 1500: loss 11.037896\n",
      "iteration 300 / 1500: loss 2.785037\n",
      "iteration 400 / 1500: loss 2.129638\n",
      "iteration 500 / 1500: loss 2.130655\n",
      "iteration 600 / 1500: loss 2.107868\n",
      "iteration 700 / 1500: loss 2.135733\n",
      "iteration 800 / 1500: loss 2.156945\n",
      "iteration 900 / 1500: loss 2.168940\n",
      "iteration 1000 / 1500: loss 2.166483\n",
      "iteration 1100 / 1500: loss 2.171640\n",
      "iteration 1200 / 1500: loss 2.175109\n",
      "iteration 1300 / 1500: loss 2.132646\n",
      "iteration 1400 / 1500: loss 2.137212\n",
      "iteration 0 / 1500: loss 1489.096597\n",
      "iteration 100 / 1500: loss 119.776368\n",
      "iteration 200 / 1500: loss 11.491923\n",
      "iteration 300 / 1500: loss 2.849833\n",
      "iteration 400 / 1500: loss 2.184748\n",
      "iteration 500 / 1500: loss 2.119423\n",
      "iteration 600 / 1500: loss 2.147659\n",
      "iteration 700 / 1500: loss 2.122947\n",
      "iteration 800 / 1500: loss 2.160274\n",
      "iteration 900 / 1500: loss 2.148441\n",
      "iteration 1000 / 1500: loss 2.138120\n",
      "iteration 1100 / 1500: loss 2.161274\n",
      "iteration 1200 / 1500: loss 2.136204\n",
      "iteration 1300 / 1500: loss 2.100569\n",
      "iteration 1400 / 1500: loss 2.149293\n",
      "iteration 0 / 1500: loss 798.850360\n",
      "iteration 100 / 1500: loss 205.971761\n",
      "iteration 200 / 1500: loss 54.361330\n",
      "iteration 300 / 1500: loss 15.529812\n",
      "iteration 400 / 1500: loss 5.511581\n",
      "iteration 500 / 1500: loss 3.035937\n",
      "iteration 600 / 1500: loss 2.316775\n",
      "iteration 700 / 1500: loss 2.107261\n",
      "iteration 800 / 1500: loss 2.132543\n",
      "iteration 900 / 1500: loss 2.086584\n",
      "iteration 1000 / 1500: loss 2.091916\n",
      "iteration 1100 / 1500: loss 2.054923\n",
      "iteration 1200 / 1500: loss 2.146184\n",
      "iteration 1300 / 1500: loss 2.054634\n",
      "iteration 1400 / 1500: loss 2.042734\n",
      "iteration 0 / 1500: loss 971.277258\n",
      "iteration 100 / 1500: loss 190.895997\n",
      "iteration 200 / 1500: loss 38.977948\n",
      "iteration 300 / 1500: loss 9.253105\n",
      "iteration 400 / 1500: loss 3.524964\n",
      "iteration 500 / 1500: loss 2.327496\n",
      "iteration 600 / 1500: loss 2.163881\n",
      "iteration 700 / 1500: loss 2.085812\n",
      "iteration 800 / 1500: loss 2.118690\n",
      "iteration 900 / 1500: loss 2.102973\n",
      "iteration 1000 / 1500: loss 2.063685\n",
      "iteration 1100 / 1500: loss 2.125630\n",
      "iteration 1200 / 1500: loss 2.127586\n",
      "iteration 1300 / 1500: loss 2.095584\n",
      "iteration 1400 / 1500: loss 2.119613\n",
      "iteration 0 / 1500: loss 1059.207672\n",
      "iteration 100 / 1500: loss 178.747717\n",
      "iteration 200 / 1500: loss 31.650606\n",
      "iteration 300 / 1500: loss 7.098751\n",
      "iteration 400 / 1500: loss 2.978673\n",
      "iteration 500 / 1500: loss 2.277099\n",
      "iteration 600 / 1500: loss 2.129792\n",
      "iteration 700 / 1500: loss 2.092798\n",
      "iteration 800 / 1500: loss 2.131513\n",
      "iteration 900 / 1500: loss 2.063673\n",
      "iteration 1000 / 1500: loss 2.095629\n",
      "iteration 1100 / 1500: loss 2.102836\n",
      "iteration 1200 / 1500: loss 2.129119\n",
      "iteration 1300 / 1500: loss 2.091767\n",
      "iteration 1400 / 1500: loss 2.098829\n",
      "iteration 0 / 1500: loss 1440.490275\n",
      "iteration 100 / 1500: loss 127.458748\n",
      "iteration 200 / 1500: loss 13.013754\n",
      "iteration 300 / 1500: loss 3.065725\n",
      "iteration 400 / 1500: loss 2.205175\n",
      "iteration 500 / 1500: loss 2.085710\n",
      "iteration 600 / 1500: loss 2.106230\n",
      "iteration 700 / 1500: loss 2.195387\n",
      "iteration 800 / 1500: loss 2.087312\n",
      "iteration 900 / 1500: loss 2.111104\n",
      "iteration 1000 / 1500: loss 2.116244\n",
      "iteration 1100 / 1500: loss 2.189638\n",
      "iteration 1200 / 1500: loss 2.144358\n",
      "iteration 1300 / 1500: loss 2.121670\n",
      "iteration 1400 / 1500: loss 2.160841\n",
      "iteration 0 / 1500: loss 470.081487\n",
      "iteration 100 / 1500: loss 214.371706\n",
      "iteration 200 / 1500: loss 98.801659\n",
      "iteration 300 / 1500: loss 46.197839\n",
      "iteration 400 / 1500: loss 22.195494\n",
      "iteration 500 / 1500: loss 11.215570\n",
      "iteration 600 / 1500: loss 6.265572\n",
      "iteration 700 / 1500: loss 3.951883\n",
      "iteration 800 / 1500: loss 2.914482\n",
      "iteration 900 / 1500: loss 2.403222\n",
      "iteration 1000 / 1500: loss 2.236510\n",
      "iteration 1100 / 1500: loss 2.204121\n",
      "iteration 1200 / 1500: loss 2.111419\n",
      "iteration 1300 / 1500: loss 2.109860\n",
      "iteration 1400 / 1500: loss 2.048681\n",
      "iteration 0 / 1500: loss 326.794174\n",
      "iteration 100 / 1500: loss 189.876331\n",
      "iteration 200 / 1500: loss 111.138369\n",
      "iteration 300 / 1500: loss 65.356206\n",
      "iteration 400 / 1500: loss 38.819471\n",
      "iteration 500 / 1500: loss 23.419040\n",
      "iteration 600 / 1500: loss 14.488100\n",
      "iteration 700 / 1500: loss 9.209572\n",
      "iteration 800 / 1500: loss 6.239986\n",
      "iteration 900 / 1500: loss 4.442548\n",
      "iteration 1000 / 1500: loss 3.429512\n",
      "iteration 1100 / 1500: loss 2.820817\n",
      "iteration 1200 / 1500: loss 2.491664\n",
      "iteration 1300 / 1500: loss 2.285544\n",
      "iteration 1400 / 1500: loss 2.197173\n",
      "iteration 0 / 1500: loss 359.372037\n",
      "iteration 100 / 1500: loss 198.329220\n",
      "iteration 200 / 1500: loss 110.356787\n",
      "iteration 300 / 1500: loss 61.730650\n",
      "iteration 400 / 1500: loss 34.932731\n",
      "iteration 500 / 1500: loss 20.167015\n",
      "iteration 600 / 1500: loss 12.065026\n",
      "iteration 700 / 1500: loss 7.541991\n",
      "iteration 800 / 1500: loss 5.119813\n",
      "iteration 900 / 1500: loss 3.708407\n",
      "iteration 1000 / 1500: loss 2.873571\n",
      "iteration 1100 / 1500: loss 2.485506\n",
      "iteration 1200 / 1500: loss 2.361637\n",
      "iteration 1300 / 1500: loss 2.135847\n",
      "iteration 1400 / 1500: loss 2.056471\n",
      "iteration 0 / 1500: loss 653.930955\n",
      "iteration 100 / 1500: loss 221.225603\n",
      "iteration 200 / 1500: loss 75.741778\n",
      "iteration 300 / 1500: loss 26.896943\n",
      "iteration 400 / 1500: loss 10.403046\n",
      "iteration 500 / 1500: loss 4.891459\n",
      "iteration 600 / 1500: loss 2.983331\n",
      "iteration 700 / 1500: loss 2.418160\n",
      "iteration 800 / 1500: loss 2.230661\n",
      "iteration 900 / 1500: loss 2.078182\n",
      "iteration 1000 / 1500: loss 1.972245\n",
      "iteration 1100 / 1500: loss 1.982143\n",
      "iteration 1200 / 1500: loss 2.109776\n",
      "iteration 1300 / 1500: loss 2.157905\n",
      "iteration 1400 / 1500: loss 2.089417\n",
      "iteration 0 / 1500: loss 396.518686\n",
      "iteration 100 / 1500: loss 204.043101\n",
      "iteration 200 / 1500: loss 106.122051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 1500: loss 55.610435\n",
      "iteration 400 / 1500: loss 29.691276\n",
      "iteration 500 / 1500: loss 16.238798\n",
      "iteration 600 / 1500: loss 9.318768\n",
      "iteration 700 / 1500: loss 5.820675\n",
      "iteration 800 / 1500: loss 4.011819\n",
      "iteration 900 / 1500: loss 2.974545\n",
      "iteration 1000 / 1500: loss 2.535557\n",
      "iteration 1100 / 1500: loss 2.255354\n",
      "iteration 1200 / 1500: loss 2.149930\n",
      "iteration 1300 / 1500: loss 2.131107\n",
      "iteration 1400 / 1500: loss 2.110166\n",
      "iteration 0 / 1500: loss 1077.113473\n",
      "iteration 100 / 1500: loss 174.148754\n",
      "iteration 200 / 1500: loss 29.684318\n",
      "iteration 300 / 1500: loss 6.567152\n",
      "iteration 400 / 1500: loss 2.803287\n",
      "iteration 500 / 1500: loss 2.170621\n",
      "iteration 600 / 1500: loss 2.149420\n",
      "iteration 700 / 1500: loss 2.131309\n",
      "iteration 800 / 1500: loss 2.112397\n",
      "iteration 900 / 1500: loss 2.084010\n",
      "iteration 1000 / 1500: loss 2.059834\n",
      "iteration 1100 / 1500: loss 2.130008\n",
      "iteration 1200 / 1500: loss 2.135148\n",
      "iteration 1300 / 1500: loss 2.100451\n",
      "iteration 1400 / 1500: loss 2.143594\n",
      "iteration 0 / 1500: loss 391.251716\n",
      "iteration 100 / 1500: loss 205.044157\n",
      "iteration 200 / 1500: loss 108.202630\n",
      "iteration 300 / 1500: loss 57.607924\n",
      "iteration 400 / 1500: loss 31.120313\n",
      "iteration 500 / 1500: loss 17.284690\n",
      "iteration 600 / 1500: loss 9.975870\n",
      "iteration 700 / 1500: loss 6.188949\n",
      "iteration 800 / 1500: loss 4.179560\n",
      "iteration 900 / 1500: loss 3.174778\n",
      "iteration 1000 / 1500: loss 2.614968\n",
      "iteration 1100 / 1500: loss 2.455715\n",
      "iteration 1200 / 1500: loss 2.230727\n",
      "iteration 1300 / 1500: loss 2.118213\n",
      "iteration 1400 / 1500: loss 2.137077\n",
      "iteration 0 / 1500: loss 1245.078229\n",
      "iteration 100 / 1500: loss 153.465373\n",
      "iteration 200 / 1500: loss 20.638011\n",
      "iteration 300 / 1500: loss 4.401318\n",
      "iteration 400 / 1500: loss 2.436515\n",
      "iteration 500 / 1500: loss 2.181728\n",
      "iteration 600 / 1500: loss 2.167088\n",
      "iteration 700 / 1500: loss 2.145351\n",
      "iteration 800 / 1500: loss 2.115833\n",
      "iteration 900 / 1500: loss 2.109187\n",
      "iteration 1000 / 1500: loss 2.145739\n",
      "iteration 1100 / 1500: loss 2.096032\n",
      "iteration 1200 / 1500: loss 2.114364\n",
      "iteration 1300 / 1500: loss 2.174800\n",
      "iteration 1400 / 1500: loss 2.157107\n",
      "iteration 0 / 1500: loss 1195.854027\n",
      "iteration 100 / 1500: loss 160.066114\n",
      "iteration 200 / 1500: loss 23.041748\n",
      "iteration 300 / 1500: loss 4.874142\n",
      "iteration 400 / 1500: loss 2.463880\n",
      "iteration 500 / 1500: loss 2.183474\n",
      "iteration 600 / 1500: loss 2.160019\n",
      "iteration 700 / 1500: loss 2.116545\n",
      "iteration 800 / 1500: loss 2.087212\n",
      "iteration 900 / 1500: loss 2.136453\n",
      "iteration 1000 / 1500: loss 2.147770\n",
      "iteration 1100 / 1500: loss 2.115422\n",
      "iteration 1200 / 1500: loss 2.104990\n",
      "iteration 1300 / 1500: loss 2.081574\n",
      "iteration 1400 / 1500: loss 2.157012\n",
      "iteration 0 / 1500: loss 386.813828\n",
      "iteration 100 / 1500: loss 203.064411\n",
      "iteration 200 / 1500: loss 107.595149\n",
      "iteration 300 / 1500: loss 57.356399\n",
      "iteration 400 / 1500: loss 31.046381\n",
      "iteration 500 / 1500: loss 17.273586\n",
      "iteration 600 / 1500: loss 9.988352\n",
      "iteration 700 / 1500: loss 6.200695\n",
      "iteration 800 / 1500: loss 4.316905\n",
      "iteration 900 / 1500: loss 3.158523\n",
      "iteration 1000 / 1500: loss 2.663485\n",
      "iteration 1100 / 1500: loss 2.361820\n",
      "iteration 1200 / 1500: loss 2.250357\n",
      "iteration 1300 / 1500: loss 2.095219\n",
      "iteration 1400 / 1500: loss 2.080653\n",
      "iteration 0 / 1500: loss 1277.319225\n",
      "iteration 100 / 1500: loss 151.857544\n",
      "iteration 200 / 1500: loss 19.726241\n",
      "iteration 300 / 1500: loss 4.220334\n",
      "iteration 400 / 1500: loss 2.385984\n",
      "iteration 500 / 1500: loss 2.167343\n",
      "iteration 600 / 1500: loss 2.121448\n",
      "iteration 700 / 1500: loss 2.184574\n",
      "iteration 800 / 1500: loss 2.120122\n",
      "iteration 900 / 1500: loss 2.131743\n",
      "iteration 1000 / 1500: loss 2.130316\n",
      "iteration 1100 / 1500: loss 2.097713\n",
      "iteration 1200 / 1500: loss 2.086459\n",
      "iteration 1300 / 1500: loss 2.132170\n",
      "iteration 1400 / 1500: loss 2.154780\n",
      "iteration 0 / 1500: loss 462.246886\n",
      "iteration 100 / 1500: loss 336.572868\n",
      "iteration 200 / 1500: loss 245.339969\n",
      "iteration 300 / 1500: loss 179.248163\n",
      "iteration 400 / 1500: loss 131.104313\n",
      "iteration 500 / 1500: loss 96.182031\n",
      "iteration 600 / 1500: loss 70.653162\n",
      "iteration 700 / 1500: loss 52.112015\n",
      "iteration 800 / 1500: loss 38.439676\n",
      "iteration 900 / 1500: loss 28.588666\n",
      "iteration 1000 / 1500: loss 21.366294\n",
      "iteration 1100 / 1500: loss 16.214206\n",
      "iteration 1200 / 1500: loss 12.336767\n",
      "iteration 1300 / 1500: loss 9.564871\n",
      "iteration 1400 / 1500: loss 7.517864\n",
      "iteration 0 / 1500: loss 415.662777\n",
      "iteration 100 / 1500: loss 312.344774\n",
      "iteration 200 / 1500: loss 234.872236\n",
      "iteration 300 / 1500: loss 176.974427\n",
      "iteration 400 / 1500: loss 133.743934\n",
      "iteration 500 / 1500: loss 100.830697\n",
      "iteration 600 / 1500: loss 76.321328\n",
      "iteration 700 / 1500: loss 57.900446\n",
      "iteration 800 / 1500: loss 43.935113\n",
      "iteration 900 / 1500: loss 33.657031\n",
      "iteration 1000 / 1500: loss 25.717586\n",
      "iteration 1100 / 1500: loss 19.816478\n",
      "iteration 1200 / 1500: loss 15.398931\n",
      "iteration 1300 / 1500: loss 12.158875\n",
      "iteration 1400 / 1500: loss 9.620377\n",
      "iteration 0 / 1500: loss 552.084836\n",
      "iteration 100 / 1500: loss 377.249437\n",
      "iteration 200 / 1500: loss 258.445152\n",
      "iteration 300 / 1500: loss 177.336793\n",
      "iteration 400 / 1500: loss 121.885717\n",
      "iteration 500 / 1500: loss 83.747892\n",
      "iteration 600 / 1500: loss 57.883849\n",
      "iteration 700 / 1500: loss 40.202872\n",
      "iteration 800 / 1500: loss 28.130344\n",
      "iteration 900 / 1500: loss 19.906363\n",
      "iteration 1000 / 1500: loss 14.250309\n",
      "iteration 1100 / 1500: loss 10.381665\n",
      "iteration 1200 / 1500: loss 7.795182\n",
      "iteration 1300 / 1500: loss 5.961183\n",
      "iteration 1400 / 1500: loss 4.703677\n",
      "iteration 0 / 1500: loss 1531.105752\n",
      "iteration 100 / 1500: loss 532.045399\n",
      "iteration 200 / 1500: loss 185.963225\n",
      "iteration 300 / 1500: loss 65.905644\n",
      "iteration 400 / 1500: loss 24.328739\n",
      "iteration 500 / 1500: loss 9.788465\n",
      "iteration 600 / 1500: loss 4.822749\n",
      "iteration 700 / 1500: loss 3.037329\n",
      "iteration 800 / 1500: loss 2.444093\n",
      "iteration 900 / 1500: loss 2.207145\n",
      "iteration 1000 / 1500: loss 2.149407\n",
      "iteration 1100 / 1500: loss 2.189303\n",
      "iteration 1200 / 1500: loss 2.128220\n",
      "iteration 1300 / 1500: loss 2.115510\n",
      "iteration 1400 / 1500: loss 2.183688\n",
      "iteration 0 / 1500: loss 1515.403844\n",
      "iteration 100 / 1500: loss 535.933744\n",
      "iteration 200 / 1500: loss 190.433174\n",
      "iteration 300 / 1500: loss 68.516612\n",
      "iteration 400 / 1500: loss 25.555668\n",
      "iteration 500 / 1500: loss 10.384143\n",
      "iteration 600 / 1500: loss 5.066981\n",
      "iteration 700 / 1500: loss 3.152975\n",
      "iteration 800 / 1500: loss 2.481424\n",
      "iteration 900 / 1500: loss 2.273271\n",
      "iteration 1000 / 1500: loss 2.191391\n",
      "iteration 1100 / 1500: loss 2.171027\n",
      "iteration 1200 / 1500: loss 2.141648\n",
      "iteration 1300 / 1500: loss 2.110178\n",
      "iteration 1400 / 1500: loss 2.182792\n",
      "iteration 0 / 1500: loss 807.321685\n",
      "iteration 100 / 1500: loss 461.193067\n",
      "iteration 200 / 1500: loss 264.046681\n",
      "iteration 300 / 1500: loss 151.653093\n",
      "iteration 400 / 1500: loss 87.513677\n",
      "iteration 500 / 1500: loss 50.952151\n",
      "iteration 600 / 1500: loss 29.973423\n",
      "iteration 700 / 1500: loss 17.971399\n",
      "iteration 800 / 1500: loss 11.232830\n",
      "iteration 900 / 1500: loss 7.276074\n",
      "iteration 1000 / 1500: loss 4.973915\n",
      "iteration 1100 / 1500: loss 3.747711\n",
      "iteration 1200 / 1500: loss 3.040596\n",
      "iteration 1300 / 1500: loss 2.658487\n",
      "iteration 1400 / 1500: loss 2.473043\n",
      "iteration 0 / 1500: loss 976.530316\n",
      "iteration 100 / 1500: loss 499.312119\n",
      "iteration 200 / 1500: loss 255.987183\n",
      "iteration 300 / 1500: loss 131.717784\n",
      "iteration 400 / 1500: loss 68.331330\n",
      "iteration 500 / 1500: loss 35.910242\n",
      "iteration 600 / 1500: loss 19.348918\n",
      "iteration 700 / 1500: loss 10.974409\n",
      "iteration 800 / 1500: loss 6.613084\n",
      "iteration 900 / 1500: loss 4.415618\n",
      "iteration 1000 / 1500: loss 3.280267\n",
      "iteration 1100 / 1500: loss 2.700330\n",
      "iteration 1200 / 1500: loss 2.442574\n",
      "iteration 1300 / 1500: loss 2.299218\n",
      "iteration 1400 / 1500: loss 2.141406\n",
      "iteration 0 / 1500: loss 1058.111299\n",
      "iteration 100 / 1500: loss 508.258844\n",
      "iteration 200 / 1500: loss 244.636049\n",
      "iteration 300 / 1500: loss 118.316954\n",
      "iteration 400 / 1500: loss 57.797885\n",
      "iteration 500 / 1500: loss 28.789069\n",
      "iteration 600 / 1500: loss 14.932442\n",
      "iteration 700 / 1500: loss 8.266417\n",
      "iteration 800 / 1500: loss 5.090296\n",
      "iteration 900 / 1500: loss 3.505923\n",
      "iteration 1000 / 1500: loss 2.788433\n",
      "iteration 1100 / 1500: loss 2.406859\n",
      "iteration 1200 / 1500: loss 2.260517\n",
      "iteration 1300 / 1500: loss 2.175438\n",
      "iteration 1400 / 1500: loss 2.137600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 1453.314638\n",
      "iteration 100 / 1500: loss 534.453102\n",
      "iteration 200 / 1500: loss 197.583689\n",
      "iteration 300 / 1500: loss 73.813704\n",
      "iteration 400 / 1500: loss 28.518786\n",
      "iteration 500 / 1500: loss 11.880446\n",
      "iteration 600 / 1500: loss 5.681632\n",
      "iteration 700 / 1500: loss 3.399950\n",
      "iteration 800 / 1500: loss 2.660565\n",
      "iteration 900 / 1500: loss 2.310251\n",
      "iteration 1000 / 1500: loss 2.193906\n",
      "iteration 1100 / 1500: loss 2.192822\n",
      "iteration 1200 / 1500: loss 2.183607\n",
      "iteration 1300 / 1500: loss 2.153816\n",
      "iteration 1400 / 1500: loss 2.110536\n",
      "iteration 0 / 1500: loss 471.867396\n",
      "iteration 100 / 1500: loss 341.436061\n",
      "iteration 200 / 1500: loss 247.667399\n",
      "iteration 300 / 1500: loss 179.614016\n",
      "iteration 400 / 1500: loss 130.569062\n",
      "iteration 500 / 1500: loss 95.184561\n",
      "iteration 600 / 1500: loss 69.215116\n",
      "iteration 700 / 1500: loss 50.750061\n",
      "iteration 800 / 1500: loss 37.300025\n",
      "iteration 900 / 1500: loss 27.540915\n",
      "iteration 1000 / 1500: loss 20.566114\n",
      "iteration 1100 / 1500: loss 15.471559\n",
      "iteration 1200 / 1500: loss 11.723253\n",
      "iteration 1300 / 1500: loss 9.074635\n",
      "iteration 1400 / 1500: loss 7.120427\n",
      "iteration 0 / 1500: loss 323.334508\n",
      "iteration 100 / 1500: loss 257.468087\n",
      "iteration 200 / 1500: loss 205.922696\n",
      "iteration 300 / 1500: loss 164.982865\n",
      "iteration 400 / 1500: loss 132.308164\n",
      "iteration 500 / 1500: loss 106.469904\n",
      "iteration 600 / 1500: loss 85.218870\n",
      "iteration 700 / 1500: loss 68.546331\n",
      "iteration 800 / 1500: loss 55.229965\n",
      "iteration 900 / 1500: loss 44.545373\n",
      "iteration 1000 / 1500: loss 36.079287\n",
      "iteration 1100 / 1500: loss 29.255663\n",
      "iteration 1200 / 1500: loss 23.800296\n",
      "iteration 1300 / 1500: loss 19.412881\n",
      "iteration 1400 / 1500: loss 15.960256\n",
      "iteration 0 / 1500: loss 359.201452\n",
      "iteration 100 / 1500: loss 280.428268\n",
      "iteration 200 / 1500: loss 219.598804\n",
      "iteration 300 / 1500: loss 172.273890\n",
      "iteration 400 / 1500: loss 135.195977\n",
      "iteration 500 / 1500: loss 106.335788\n",
      "iteration 600 / 1500: loss 83.415973\n",
      "iteration 700 / 1500: loss 65.883240\n",
      "iteration 800 / 1500: loss 51.947186\n",
      "iteration 900 / 1500: loss 41.084297\n",
      "iteration 1000 / 1500: loss 32.487489\n",
      "iteration 1100 / 1500: loss 25.829220\n",
      "iteration 1200 / 1500: loss 20.721881\n",
      "iteration 1300 / 1500: loss 16.732194\n",
      "iteration 1400 / 1500: loss 13.474247\n",
      "iteration 0 / 1500: loss 663.935788\n",
      "iteration 100 / 1500: loss 424.037717\n",
      "iteration 200 / 1500: loss 271.249939\n",
      "iteration 300 / 1500: loss 173.830830\n",
      "iteration 400 / 1500: loss 111.845070\n",
      "iteration 500 / 1500: loss 72.090779\n",
      "iteration 600 / 1500: loss 46.837766\n",
      "iteration 700 / 1500: loss 30.712049\n",
      "iteration 800 / 1500: loss 20.373083\n",
      "iteration 900 / 1500: loss 13.680415\n",
      "iteration 1000 / 1500: loss 9.520552\n",
      "iteration 1100 / 1500: loss 6.821781\n",
      "iteration 1200 / 1500: loss 5.152371\n",
      "iteration 1300 / 1500: loss 4.009943\n",
      "iteration 1400 / 1500: loss 3.298073\n",
      "iteration 0 / 1500: loss 397.290982\n",
      "iteration 100 / 1500: loss 301.969954\n",
      "iteration 200 / 1500: loss 230.264168\n",
      "iteration 300 / 1500: loss 175.415281\n",
      "iteration 400 / 1500: loss 134.132548\n",
      "iteration 500 / 1500: loss 102.533193\n",
      "iteration 600 / 1500: loss 78.575924\n",
      "iteration 700 / 1500: loss 60.303228\n",
      "iteration 800 / 1500: loss 46.410725\n",
      "iteration 900 / 1500: loss 35.732641\n",
      "iteration 1000 / 1500: loss 27.784507\n",
      "iteration 1100 / 1500: loss 21.461269\n",
      "iteration 1200 / 1500: loss 16.839364\n",
      "iteration 1300 / 1500: loss 13.324604\n",
      "iteration 1400 / 1500: loss 10.695592\n",
      "iteration 0 / 1500: loss 1107.060000\n",
      "iteration 100 / 1500: loss 522.003137\n",
      "iteration 200 / 1500: loss 246.825377\n",
      "iteration 300 / 1500: loss 117.427720\n",
      "iteration 400 / 1500: loss 56.377395\n",
      "iteration 500 / 1500: loss 27.638058\n",
      "iteration 600 / 1500: loss 14.178643\n",
      "iteration 700 / 1500: loss 7.847545\n",
      "iteration 800 / 1500: loss 4.828064\n",
      "iteration 900 / 1500: loss 3.339695\n",
      "iteration 1000 / 1500: loss 2.649291\n",
      "iteration 1100 / 1500: loss 2.454172\n",
      "iteration 1200 / 1500: loss 2.256228\n",
      "iteration 1300 / 1500: loss 2.155159\n",
      "iteration 1400 / 1500: loss 2.077460\n",
      "iteration 0 / 1500: loss 392.695489\n",
      "iteration 100 / 1500: loss 300.457597\n",
      "iteration 200 / 1500: loss 230.594283\n",
      "iteration 300 / 1500: loss 177.008388\n",
      "iteration 400 / 1500: loss 135.805155\n",
      "iteration 500 / 1500: loss 104.553914\n",
      "iteration 600 / 1500: loss 80.478543\n",
      "iteration 700 / 1500: loss 62.078542\n",
      "iteration 800 / 1500: loss 48.072669\n",
      "iteration 900 / 1500: loss 37.304445\n",
      "iteration 1000 / 1500: loss 28.878916\n",
      "iteration 1100 / 1500: loss 22.758543\n",
      "iteration 1200 / 1500: loss 17.888957\n",
      "iteration 1300 / 1500: loss 14.107755\n",
      "iteration 1400 / 1500: loss 11.315294\n",
      "iteration 0 / 1500: loss 1242.061191\n",
      "iteration 100 / 1500: loss 523.615948\n",
      "iteration 200 / 1500: loss 221.672402\n",
      "iteration 300 / 1500: loss 94.471745\n",
      "iteration 400 / 1500: loss 41.037638\n",
      "iteration 500 / 1500: loss 18.490921\n",
      "iteration 600 / 1500: loss 9.013788\n",
      "iteration 700 / 1500: loss 5.038133\n",
      "iteration 800 / 1500: loss 3.331733\n",
      "iteration 900 / 1500: loss 2.621715\n",
      "iteration 1000 / 1500: loss 2.362853\n",
      "iteration 1100 / 1500: loss 2.217886\n",
      "iteration 1200 / 1500: loss 2.145757\n",
      "iteration 1300 / 1500: loss 2.171389\n",
      "iteration 1400 / 1500: loss 2.130769\n",
      "iteration 0 / 1500: loss 1201.691493\n",
      "iteration 100 / 1500: loss 524.624771\n",
      "iteration 200 / 1500: loss 229.640596\n",
      "iteration 300 / 1500: loss 101.214717\n",
      "iteration 400 / 1500: loss 45.255861\n",
      "iteration 500 / 1500: loss 20.970638\n",
      "iteration 600 / 1500: loss 10.341445\n",
      "iteration 700 / 1500: loss 5.707381\n",
      "iteration 800 / 1500: loss 3.702860\n",
      "iteration 900 / 1500: loss 2.797897\n",
      "iteration 1000 / 1500: loss 2.461020\n",
      "iteration 1100 / 1500: loss 2.311003\n",
      "iteration 1200 / 1500: loss 2.185057\n",
      "iteration 1300 / 1500: loss 2.149247\n",
      "iteration 1400 / 1500: loss 2.159154\n",
      "iteration 0 / 1500: loss 391.586163\n",
      "iteration 100 / 1500: loss 299.669543\n",
      "iteration 200 / 1500: loss 230.166715\n",
      "iteration 300 / 1500: loss 176.706976\n",
      "iteration 400 / 1500: loss 136.175554\n",
      "iteration 500 / 1500: loss 104.796863\n",
      "iteration 600 / 1500: loss 80.833927\n",
      "iteration 700 / 1500: loss 62.467092\n",
      "iteration 800 / 1500: loss 48.529722\n",
      "iteration 900 / 1500: loss 37.691358\n",
      "iteration 1000 / 1500: loss 29.340646\n",
      "iteration 1100 / 1500: loss 22.922715\n",
      "iteration 1200 / 1500: loss 18.015314\n",
      "iteration 1300 / 1500: loss 14.373338\n",
      "iteration 1400 / 1500: loss 11.501553\n",
      "iteration 0 / 1500: loss 1269.775654\n",
      "iteration 100 / 1500: loss 527.858782\n",
      "iteration 200 / 1500: loss 220.071970\n",
      "iteration 300 / 1500: loss 92.514485\n",
      "iteration 400 / 1500: loss 39.611733\n",
      "iteration 500 / 1500: loss 17.674088\n",
      "iteration 600 / 1500: loss 8.546428\n",
      "iteration 700 / 1500: loss 4.827471\n",
      "iteration 800 / 1500: loss 3.240360\n",
      "iteration 900 / 1500: loss 2.601883\n",
      "iteration 1000 / 1500: loss 2.325506\n",
      "iteration 1100 / 1500: loss 2.203409\n",
      "iteration 1200 / 1500: loss 2.096993\n",
      "iteration 1300 / 1500: loss 2.123240\n",
      "iteration 1400 / 1500: loss 2.172778\n",
      "iteration 0 / 1500: loss 459.790376\n",
      "iteration 100 / 1500: loss 186.326972\n",
      "iteration 200 / 1500: loss 76.457312\n",
      "iteration 300 / 1500: loss 32.246063\n",
      "iteration 400 / 1500: loss 14.297096\n",
      "iteration 500 / 1500: loss 7.001270\n",
      "iteration 600 / 1500: loss 4.014691\n",
      "iteration 700 / 1500: loss 2.823004\n",
      "iteration 800 / 1500: loss 2.375097\n",
      "iteration 900 / 1500: loss 2.142791\n",
      "iteration 1000 / 1500: loss 2.060665\n",
      "iteration 1100 / 1500: loss 2.098736\n",
      "iteration 1200 / 1500: loss 2.141729\n",
      "iteration 1300 / 1500: loss 2.155690\n",
      "iteration 1400 / 1500: loss 1.999188\n",
      "iteration 0 / 1500: loss 418.485124\n",
      "iteration 100 / 1500: loss 185.408153\n",
      "iteration 200 / 1500: loss 83.082060\n",
      "iteration 300 / 1500: loss 37.921499\n",
      "iteration 400 / 1500: loss 17.825859\n",
      "iteration 500 / 1500: loss 8.973882\n",
      "iteration 600 / 1500: loss 5.145978\n",
      "iteration 700 / 1500: loss 3.423570\n",
      "iteration 800 / 1500: loss 2.671145\n",
      "iteration 900 / 1500: loss 2.303708\n",
      "iteration 1000 / 1500: loss 2.140991\n",
      "iteration 1100 / 1500: loss 2.057524\n",
      "iteration 1200 / 1500: loss 2.035800\n",
      "iteration 1300 / 1500: loss 2.084540\n",
      "iteration 1400 / 1500: loss 1.985046\n",
      "iteration 0 / 1500: loss 548.735530\n",
      "iteration 100 / 1500: loss 185.851930\n",
      "iteration 200 / 1500: loss 63.877846\n",
      "iteration 300 / 1500: loss 22.856470\n",
      "iteration 400 / 1500: loss 9.115879\n",
      "iteration 500 / 1500: loss 4.391240\n",
      "iteration 600 / 1500: loss 2.877929\n",
      "iteration 700 / 1500: loss 2.323291\n",
      "iteration 800 / 1500: loss 2.135858\n",
      "iteration 900 / 1500: loss 2.127389\n",
      "iteration 1000 / 1500: loss 2.044723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 1500: loss 2.030763\n",
      "iteration 1200 / 1500: loss 2.051618\n",
      "iteration 1300 / 1500: loss 2.053750\n",
      "iteration 1400 / 1500: loss 2.047491\n",
      "iteration 0 / 1500: loss 1537.428783\n",
      "iteration 100 / 1500: loss 75.629491\n",
      "iteration 200 / 1500: loss 5.637297\n",
      "iteration 300 / 1500: loss 2.316329\n",
      "iteration 400 / 1500: loss 2.129265\n",
      "iteration 500 / 1500: loss 2.108470\n",
      "iteration 600 / 1500: loss 2.195013\n",
      "iteration 700 / 1500: loss 2.092492\n",
      "iteration 800 / 1500: loss 2.135160\n",
      "iteration 900 / 1500: loss 2.191181\n",
      "iteration 1000 / 1500: loss 2.102871\n",
      "iteration 1100 / 1500: loss 2.103000\n",
      "iteration 1200 / 1500: loss 2.122771\n",
      "iteration 1300 / 1500: loss 2.100189\n",
      "iteration 1400 / 1500: loss 2.153339\n",
      "iteration 0 / 1500: loss 1501.657333\n",
      "iteration 100 / 1500: loss 77.395653\n",
      "iteration 200 / 1500: loss 5.910716\n",
      "iteration 300 / 1500: loss 2.342557\n",
      "iteration 400 / 1500: loss 2.127559\n",
      "iteration 500 / 1500: loss 2.131920\n",
      "iteration 600 / 1500: loss 2.119366\n",
      "iteration 700 / 1500: loss 2.152078\n",
      "iteration 800 / 1500: loss 2.145814\n",
      "iteration 900 / 1500: loss 2.135472\n",
      "iteration 1000 / 1500: loss 2.150761\n",
      "iteration 1100 / 1500: loss 2.168974\n",
      "iteration 1200 / 1500: loss 2.150926\n",
      "iteration 1300 / 1500: loss 2.168616\n",
      "iteration 1400 / 1500: loss 2.145454\n",
      "iteration 0 / 1500: loss 815.208498\n",
      "iteration 100 / 1500: loss 165.301730\n",
      "iteration 200 / 1500: loss 34.900064\n",
      "iteration 300 / 1500: loss 8.691306\n",
      "iteration 400 / 1500: loss 3.430782\n",
      "iteration 500 / 1500: loss 2.389293\n",
      "iteration 600 / 1500: loss 2.155800\n",
      "iteration 700 / 1500: loss 2.062886\n",
      "iteration 800 / 1500: loss 2.119376\n",
      "iteration 900 / 1500: loss 2.099120\n",
      "iteration 1000 / 1500: loss 2.064179\n",
      "iteration 1100 / 1500: loss 2.156785\n",
      "iteration 1200 / 1500: loss 2.111865\n",
      "iteration 1300 / 1500: loss 2.099269\n",
      "iteration 1400 / 1500: loss 2.088413\n",
      "iteration 0 / 1500: loss 968.976109\n",
      "iteration 100 / 1500: loss 142.766281\n",
      "iteration 200 / 1500: loss 22.611315\n",
      "iteration 300 / 1500: loss 5.110274\n",
      "iteration 400 / 1500: loss 2.553746\n",
      "iteration 500 / 1500: loss 2.153040\n",
      "iteration 600 / 1500: loss 2.114434\n",
      "iteration 700 / 1500: loss 2.116593\n",
      "iteration 800 / 1500: loss 2.094431\n",
      "iteration 900 / 1500: loss 2.151735\n",
      "iteration 1000 / 1500: loss 2.134229\n",
      "iteration 1100 / 1500: loss 2.193117\n",
      "iteration 1200 / 1500: loss 2.107075\n",
      "iteration 1300 / 1500: loss 2.107247\n",
      "iteration 1400 / 1500: loss 2.172272\n",
      "iteration 0 / 1500: loss 1078.712616\n",
      "iteration 100 / 1500: loss 132.819902\n",
      "iteration 200 / 1500: loss 18.002412\n",
      "iteration 300 / 1500: loss 4.064237\n",
      "iteration 400 / 1500: loss 2.433684\n",
      "iteration 500 / 1500: loss 2.114076\n",
      "iteration 600 / 1500: loss 2.033759\n",
      "iteration 700 / 1500: loss 2.136729\n",
      "iteration 800 / 1500: loss 2.166205\n",
      "iteration 900 / 1500: loss 2.150284\n",
      "iteration 1000 / 1500: loss 2.128292\n",
      "iteration 1100 / 1500: loss 2.136331\n",
      "iteration 1200 / 1500: loss 2.061304\n",
      "iteration 1300 / 1500: loss 2.075779\n",
      "iteration 1400 / 1500: loss 2.064532\n",
      "iteration 0 / 1500: loss 1452.943244\n",
      "iteration 100 / 1500: loss 83.923788\n",
      "iteration 200 / 1500: loss 6.794946\n",
      "iteration 300 / 1500: loss 2.359903\n",
      "iteration 400 / 1500: loss 2.148555\n",
      "iteration 500 / 1500: loss 2.112501\n",
      "iteration 600 / 1500: loss 2.184436\n",
      "iteration 700 / 1500: loss 2.126419\n",
      "iteration 800 / 1500: loss 2.108732\n",
      "iteration 900 / 1500: loss 2.191383\n",
      "iteration 1000 / 1500: loss 2.130567\n",
      "iteration 1100 / 1500: loss 2.112429\n",
      "iteration 1200 / 1500: loss 2.129782\n",
      "iteration 1300 / 1500: loss 2.109907\n",
      "iteration 1400 / 1500: loss 2.181823\n",
      "iteration 0 / 1500: loss 478.682232\n",
      "iteration 100 / 1500: loss 190.218070\n",
      "iteration 200 / 1500: loss 76.653283\n",
      "iteration 300 / 1500: loss 31.669905\n",
      "iteration 400 / 1500: loss 13.750128\n",
      "iteration 500 / 1500: loss 6.650992\n",
      "iteration 600 / 1500: loss 3.945408\n",
      "iteration 700 / 1500: loss 2.834337\n",
      "iteration 800 / 1500: loss 2.284853\n",
      "iteration 900 / 1500: loss 2.190908\n",
      "iteration 1000 / 1500: loss 2.136358\n",
      "iteration 1100 / 1500: loss 2.068630\n",
      "iteration 1200 / 1500: loss 2.079430\n",
      "iteration 1300 / 1500: loss 2.089595\n",
      "iteration 1400 / 1500: loss 2.030253\n",
      "iteration 0 / 1500: loss 328.668108\n",
      "iteration 100 / 1500: loss 173.608095\n",
      "iteration 200 / 1500: loss 92.434118\n",
      "iteration 300 / 1500: loss 49.851335\n",
      "iteration 400 / 1500: loss 27.140714\n",
      "iteration 500 / 1500: loss 15.280308\n",
      "iteration 600 / 1500: loss 9.050119\n",
      "iteration 700 / 1500: loss 5.677292\n",
      "iteration 800 / 1500: loss 4.028566\n",
      "iteration 900 / 1500: loss 3.099537\n",
      "iteration 1000 / 1500: loss 2.528680\n",
      "iteration 1100 / 1500: loss 2.333315\n",
      "iteration 1200 / 1500: loss 2.163650\n",
      "iteration 1300 / 1500: loss 2.038850\n",
      "iteration 1400 / 1500: loss 2.021186\n",
      "iteration 0 / 1500: loss 358.193285\n",
      "iteration 100 / 1500: loss 177.713085\n",
      "iteration 200 / 1500: loss 88.899584\n",
      "iteration 300 / 1500: loss 45.179556\n",
      "iteration 400 / 1500: loss 23.500463\n",
      "iteration 500 / 1500: loss 12.622659\n",
      "iteration 600 / 1500: loss 7.282169\n",
      "iteration 700 / 1500: loss 4.589172\n",
      "iteration 800 / 1500: loss 3.339916\n",
      "iteration 900 / 1500: loss 2.663634\n",
      "iteration 1000 / 1500: loss 2.370894\n",
      "iteration 1100 / 1500: loss 2.160563\n",
      "iteration 1200 / 1500: loss 2.071204\n",
      "iteration 1300 / 1500: loss 2.055469\n",
      "iteration 1400 / 1500: loss 2.052440\n",
      "iteration 0 / 1500: loss 647.012636\n",
      "iteration 100 / 1500: loss 180.563165\n",
      "iteration 200 / 1500: loss 51.397817\n",
      "iteration 300 / 1500: loss 15.775191\n",
      "iteration 400 / 1500: loss 5.919882\n",
      "iteration 500 / 1500: loss 3.140475\n",
      "iteration 600 / 1500: loss 2.299346\n",
      "iteration 700 / 1500: loss 2.112181\n",
      "iteration 800 / 1500: loss 2.128581\n",
      "iteration 900 / 1500: loss 2.055281\n",
      "iteration 1000 / 1500: loss 2.073199\n",
      "iteration 1100 / 1500: loss 1.985517\n",
      "iteration 1200 / 1500: loss 2.034394\n",
      "iteration 1300 / 1500: loss 2.131549\n",
      "iteration 1400 / 1500: loss 2.131626\n",
      "iteration 0 / 1500: loss 400.569510\n",
      "iteration 100 / 1500: loss 183.647939\n",
      "iteration 200 / 1500: loss 85.145135\n",
      "iteration 300 / 1500: loss 40.153187\n",
      "iteration 400 / 1500: loss 19.519037\n",
      "iteration 500 / 1500: loss 10.119028\n",
      "iteration 600 / 1500: loss 5.705344\n",
      "iteration 700 / 1500: loss 3.680152\n",
      "iteration 800 / 1500: loss 2.796494\n",
      "iteration 900 / 1500: loss 2.358584\n",
      "iteration 1000 / 1500: loss 2.187550\n",
      "iteration 1100 / 1500: loss 2.058134\n",
      "iteration 1200 / 1500: loss 2.053818\n",
      "iteration 1300 / 1500: loss 1.947983\n",
      "iteration 1400 / 1500: loss 2.028983\n",
      "iteration 0 / 1500: loss 1088.317161\n",
      "iteration 100 / 1500: loss 127.541226\n",
      "iteration 200 / 1500: loss 16.577237\n",
      "iteration 300 / 1500: loss 3.795154\n",
      "iteration 400 / 1500: loss 2.369554\n",
      "iteration 500 / 1500: loss 2.120749\n",
      "iteration 600 / 1500: loss 2.102792\n",
      "iteration 700 / 1500: loss 2.076985\n",
      "iteration 800 / 1500: loss 2.063222\n",
      "iteration 900 / 1500: loss 2.151172\n",
      "iteration 1000 / 1500: loss 2.122463\n",
      "iteration 1100 / 1500: loss 2.130964\n",
      "iteration 1200 / 1500: loss 2.104937\n",
      "iteration 1300 / 1500: loss 2.095732\n",
      "iteration 1400 / 1500: loss 2.124109\n",
      "iteration 0 / 1500: loss 392.597012\n",
      "iteration 100 / 1500: loss 183.524238\n",
      "iteration 200 / 1500: loss 86.551152\n",
      "iteration 300 / 1500: loss 41.487299\n",
      "iteration 400 / 1500: loss 20.473905\n",
      "iteration 500 / 1500: loss 10.609217\n",
      "iteration 600 / 1500: loss 5.997255\n",
      "iteration 700 / 1500: loss 3.865050\n",
      "iteration 800 / 1500: loss 2.949326\n",
      "iteration 900 / 1500: loss 2.388488\n",
      "iteration 1000 / 1500: loss 2.252856\n",
      "iteration 1100 / 1500: loss 2.179724\n",
      "iteration 1200 / 1500: loss 1.984898\n",
      "iteration 1300 / 1500: loss 1.999837\n",
      "iteration 1400 / 1500: loss 2.004900\n",
      "iteration 0 / 1500: loss 1243.306962\n",
      "iteration 100 / 1500: loss 105.881567\n",
      "iteration 200 / 1500: loss 10.797215\n",
      "iteration 300 / 1500: loss 2.853914\n",
      "iteration 400 / 1500: loss 2.210817\n",
      "iteration 500 / 1500: loss 2.186854\n",
      "iteration 600 / 1500: loss 2.126408\n",
      "iteration 700 / 1500: loss 2.138070\n",
      "iteration 800 / 1500: loss 2.083498\n",
      "iteration 900 / 1500: loss 2.153390\n",
      "iteration 1000 / 1500: loss 2.109120\n",
      "iteration 1100 / 1500: loss 2.109438\n",
      "iteration 1200 / 1500: loss 2.127346\n",
      "iteration 1300 / 1500: loss 2.142996\n",
      "iteration 1400 / 1500: loss 2.119013\n",
      "iteration 0 / 1500: loss 1182.513849\n",
      "iteration 100 / 1500: loss 111.081932\n",
      "iteration 200 / 1500: loss 12.205689\n",
      "iteration 300 / 1500: loss 2.980037\n",
      "iteration 400 / 1500: loss 2.194740\n",
      "iteration 500 / 1500: loss 2.067021\n",
      "iteration 600 / 1500: loss 2.105436\n",
      "iteration 700 / 1500: loss 2.115743\n",
      "iteration 800 / 1500: loss 2.143880\n",
      "iteration 900 / 1500: loss 2.153490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 1500: loss 2.147399\n",
      "iteration 1100 / 1500: loss 2.075266\n",
      "iteration 1200 / 1500: loss 2.143979\n",
      "iteration 1300 / 1500: loss 2.122992\n",
      "iteration 1400 / 1500: loss 2.097987\n",
      "iteration 0 / 1500: loss 388.374985\n",
      "iteration 100 / 1500: loss 182.392966\n",
      "iteration 200 / 1500: loss 86.494017\n",
      "iteration 300 / 1500: loss 41.536125\n",
      "iteration 400 / 1500: loss 20.603831\n",
      "iteration 500 / 1500: loss 10.645209\n",
      "iteration 600 / 1500: loss 6.090017\n",
      "iteration 700 / 1500: loss 3.947324\n",
      "iteration 800 / 1500: loss 2.925954\n",
      "iteration 900 / 1500: loss 2.488941\n",
      "iteration 1000 / 1500: loss 2.168985\n",
      "iteration 1100 / 1500: loss 2.156797\n",
      "iteration 1200 / 1500: loss 2.060741\n",
      "iteration 1300 / 1500: loss 2.093971\n",
      "iteration 1400 / 1500: loss 1.993887\n",
      "iteration 0 / 1500: loss 1260.927418\n",
      "iteration 100 / 1500: loss 102.802763\n",
      "iteration 200 / 1500: loss 10.205142\n",
      "iteration 300 / 1500: loss 2.760728\n",
      "iteration 400 / 1500: loss 2.146604\n",
      "iteration 500 / 1500: loss 2.126266\n",
      "iteration 600 / 1500: loss 2.124969\n",
      "iteration 700 / 1500: loss 2.129026\n",
      "iteration 800 / 1500: loss 2.089886\n",
      "iteration 900 / 1500: loss 2.209851\n",
      "iteration 1000 / 1500: loss 2.157946\n",
      "iteration 1100 / 1500: loss 2.111686\n",
      "iteration 1200 / 1500: loss 2.172939\n",
      "iteration 1300 / 1500: loss 2.084195\n",
      "iteration 1400 / 1500: loss 2.138830\n",
      "iteration 0 / 1500: loss 461.407611\n",
      "iteration 100 / 1500: loss 323.451876\n",
      "iteration 200 / 1500: loss 227.083359\n",
      "iteration 300 / 1500: loss 159.696802\n",
      "iteration 400 / 1500: loss 112.535302\n",
      "iteration 500 / 1500: loss 79.292989\n",
      "iteration 600 / 1500: loss 56.161655\n",
      "iteration 700 / 1500: loss 39.927230\n",
      "iteration 800 / 1500: loss 28.562529\n",
      "iteration 900 / 1500: loss 20.648709\n",
      "iteration 1000 / 1500: loss 15.152030\n",
      "iteration 1100 / 1500: loss 11.245526\n",
      "iteration 1200 / 1500: loss 8.443287\n",
      "iteration 1300 / 1500: loss 6.548896\n",
      "iteration 1400 / 1500: loss 5.222605\n",
      "iteration 0 / 1500: loss 416.958604\n",
      "iteration 100 / 1500: loss 301.524455\n",
      "iteration 200 / 1500: loss 219.082949\n",
      "iteration 300 / 1500: loss 159.276647\n",
      "iteration 400 / 1500: loss 116.080055\n",
      "iteration 500 / 1500: loss 84.710399\n",
      "iteration 600 / 1500: loss 61.917286\n",
      "iteration 700 / 1500: loss 45.544901\n",
      "iteration 800 / 1500: loss 33.642812\n",
      "iteration 900 / 1500: loss 24.932749\n",
      "iteration 1000 / 1500: loss 18.688695\n",
      "iteration 1100 / 1500: loss 14.026109\n",
      "iteration 1200 / 1500: loss 10.734067\n",
      "iteration 1300 / 1500: loss 8.376151\n",
      "iteration 1400 / 1500: loss 6.592636\n",
      "iteration 0 / 1500: loss 550.492974\n",
      "iteration 100 / 1500: loss 358.462240\n",
      "iteration 200 / 1500: loss 233.942574\n",
      "iteration 300 / 1500: loss 153.143484\n",
      "iteration 400 / 1500: loss 100.479077\n",
      "iteration 500 / 1500: loss 66.378454\n",
      "iteration 600 / 1500: loss 43.975441\n",
      "iteration 700 / 1500: loss 29.334452\n",
      "iteration 800 / 1500: loss 19.825970\n",
      "iteration 900 / 1500: loss 13.659978\n",
      "iteration 1000 / 1500: loss 9.655114\n",
      "iteration 1100 / 1500: loss 7.027710\n",
      "iteration 1200 / 1500: loss 5.284190\n",
      "iteration 1300 / 1500: loss 4.134624\n",
      "iteration 1400 / 1500: loss 3.436247\n",
      "iteration 0 / 1500: loss 1513.949578\n",
      "iteration 100 / 1500: loss 461.150231\n",
      "iteration 200 / 1500: loss 141.497610\n",
      "iteration 300 / 1500: loss 44.520686\n",
      "iteration 400 / 1500: loss 15.013493\n",
      "iteration 500 / 1500: loss 6.045635\n",
      "iteration 600 / 1500: loss 3.342884\n",
      "iteration 700 / 1500: loss 2.526914\n",
      "iteration 800 / 1500: loss 2.257247\n",
      "iteration 900 / 1500: loss 2.179304\n",
      "iteration 1000 / 1500: loss 2.153407\n",
      "iteration 1100 / 1500: loss 2.068381\n",
      "iteration 1200 / 1500: loss 2.189076\n",
      "iteration 1300 / 1500: loss 2.132147\n",
      "iteration 1400 / 1500: loss 2.188639\n",
      "iteration 0 / 1500: loss 1494.670628\n",
      "iteration 100 / 1500: loss 464.032481\n",
      "iteration 200 / 1500: loss 145.040038\n",
      "iteration 300 / 1500: loss 46.357014\n",
      "iteration 400 / 1500: loss 15.829443\n",
      "iteration 500 / 1500: loss 6.381542\n",
      "iteration 600 / 1500: loss 3.446472\n",
      "iteration 700 / 1500: loss 2.493197\n",
      "iteration 800 / 1500: loss 2.276908\n",
      "iteration 900 / 1500: loss 2.186075\n",
      "iteration 1000 / 1500: loss 2.164519\n",
      "iteration 1100 / 1500: loss 2.178391\n",
      "iteration 1200 / 1500: loss 2.101315\n",
      "iteration 1300 / 1500: loss 2.109603\n",
      "iteration 1400 / 1500: loss 2.149137\n",
      "iteration 0 / 1500: loss 811.590578\n",
      "iteration 100 / 1500: loss 433.002556\n",
      "iteration 200 / 1500: loss 231.379454\n",
      "iteration 300 / 1500: loss 124.209329\n",
      "iteration 400 / 1500: loss 66.956866\n",
      "iteration 500 / 1500: loss 36.723771\n",
      "iteration 600 / 1500: loss 20.466305\n",
      "iteration 700 / 1500: loss 11.957559\n",
      "iteration 800 / 1500: loss 7.283556\n",
      "iteration 900 / 1500: loss 4.863491\n",
      "iteration 1000 / 1500: loss 3.535332\n",
      "iteration 1100 / 1500: loss 2.868838\n",
      "iteration 1200 / 1500: loss 2.476457\n",
      "iteration 1300 / 1500: loss 2.321447\n",
      "iteration 1400 / 1500: loss 2.161012\n",
      "iteration 0 / 1500: loss 979.329105\n",
      "iteration 100 / 1500: loss 459.962323\n",
      "iteration 200 / 1500: loss 216.771874\n",
      "iteration 300 / 1500: loss 102.936555\n",
      "iteration 400 / 1500: loss 49.350245\n",
      "iteration 500 / 1500: loss 24.352978\n",
      "iteration 600 / 1500: loss 12.536508\n",
      "iteration 700 / 1500: loss 6.948129\n",
      "iteration 800 / 1500: loss 4.390094\n",
      "iteration 900 / 1500: loss 3.203790\n",
      "iteration 1000 / 1500: loss 2.656050\n",
      "iteration 1100 / 1500: loss 2.361993\n",
      "iteration 1200 / 1500: loss 2.228498\n",
      "iteration 1300 / 1500: loss 2.099993\n",
      "iteration 1400 / 1500: loss 2.147347\n",
      "iteration 0 / 1500: loss 1076.152465\n",
      "iteration 100 / 1500: loss 471.711835\n",
      "iteration 200 / 1500: loss 207.387420\n",
      "iteration 300 / 1500: loss 91.844337\n",
      "iteration 400 / 1500: loss 41.371644\n",
      "iteration 500 / 1500: loss 19.290070\n",
      "iteration 600 / 1500: loss 9.663591\n",
      "iteration 700 / 1500: loss 5.411190\n",
      "iteration 800 / 1500: loss 3.545297\n",
      "iteration 900 / 1500: loss 2.732659\n",
      "iteration 1000 / 1500: loss 2.387530\n",
      "iteration 1100 / 1500: loss 2.230003\n",
      "iteration 1200 / 1500: loss 2.187782\n",
      "iteration 1300 / 1500: loss 2.135919\n",
      "iteration 1400 / 1500: loss 2.133532\n",
      "iteration 0 / 1500: loss 1460.529874\n",
      "iteration 100 / 1500: loss 474.033786\n",
      "iteration 200 / 1500: loss 154.889870\n",
      "iteration 300 / 1500: loss 51.597567\n",
      "iteration 400 / 1500: loss 18.133272\n",
      "iteration 500 / 1500: loss 7.299285\n",
      "iteration 600 / 1500: loss 3.809079\n",
      "iteration 700 / 1500: loss 2.670316\n",
      "iteration 800 / 1500: loss 2.323160\n",
      "iteration 900 / 1500: loss 2.180803\n",
      "iteration 1000 / 1500: loss 2.142442\n",
      "iteration 1100 / 1500: loss 2.133641\n",
      "iteration 1200 / 1500: loss 2.132295\n",
      "iteration 1300 / 1500: loss 2.150865\n",
      "iteration 1400 / 1500: loss 2.160908\n",
      "iteration 0 / 1500: loss 474.351806\n",
      "iteration 100 / 1500: loss 329.708570\n",
      "iteration 200 / 1500: loss 229.665366\n",
      "iteration 300 / 1500: loss 160.122371\n",
      "iteration 400 / 1500: loss 111.901067\n",
      "iteration 500 / 1500: loss 78.478190\n",
      "iteration 600 / 1500: loss 55.174500\n",
      "iteration 700 / 1500: loss 38.976107\n",
      "iteration 800 / 1500: loss 27.752452\n",
      "iteration 900 / 1500: loss 19.844147\n",
      "iteration 1000 / 1500: loss 14.468974\n",
      "iteration 1100 / 1500: loss 10.635039\n",
      "iteration 1200 / 1500: loss 8.085189\n",
      "iteration 1300 / 1500: loss 6.206828\n",
      "iteration 1400 / 1500: loss 4.972181\n",
      "iteration 0 / 1500: loss 328.282154\n",
      "iteration 100 / 1500: loss 254.059114\n",
      "iteration 200 / 1500: loss 197.713731\n",
      "iteration 300 / 1500: loss 154.114596\n",
      "iteration 400 / 1500: loss 120.125839\n",
      "iteration 500 / 1500: loss 94.141440\n",
      "iteration 600 / 1500: loss 73.473519\n",
      "iteration 700 / 1500: loss 57.680566\n",
      "iteration 800 / 1500: loss 45.262472\n",
      "iteration 900 / 1500: loss 35.566121\n",
      "iteration 1000 / 1500: loss 28.077688\n",
      "iteration 1100 / 1500: loss 22.346954\n",
      "iteration 1200 / 1500: loss 17.893653\n",
      "iteration 1300 / 1500: loss 14.269071\n",
      "iteration 1400 / 1500: loss 11.607076\n",
      "iteration 0 / 1500: loss 361.862801\n",
      "iteration 100 / 1500: loss 274.438497\n",
      "iteration 200 / 1500: loss 208.639254\n",
      "iteration 300 / 1500: loss 158.832426\n",
      "iteration 400 / 1500: loss 120.784012\n",
      "iteration 500 / 1500: loss 92.235615\n",
      "iteration 600 / 1500: loss 70.561827\n",
      "iteration 700 / 1500: loss 53.990280\n",
      "iteration 800 / 1500: loss 41.472088\n",
      "iteration 900 / 1500: loss 31.915768\n",
      "iteration 1000 / 1500: loss 24.704060\n",
      "iteration 1100 / 1500: loss 19.289530\n",
      "iteration 1200 / 1500: loss 15.081646\n",
      "iteration 1300 / 1500: loss 11.998352\n",
      "iteration 1400 / 1500: loss 9.482372\n",
      "iteration 0 / 1500: loss 648.410658\n",
      "iteration 100 / 1500: loss 391.433621\n",
      "iteration 200 / 1500: loss 236.865710\n",
      "iteration 300 / 1500: loss 143.792718\n",
      "iteration 400 / 1500: loss 87.569622\n",
      "iteration 500 / 1500: loss 53.632802\n",
      "iteration 600 / 1500: loss 33.284879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 1500: loss 20.862300\n",
      "iteration 800 / 1500: loss 13.466227\n",
      "iteration 900 / 1500: loss 8.931857\n",
      "iteration 1000 / 1500: loss 6.185941\n",
      "iteration 1100 / 1500: loss 4.619103\n",
      "iteration 1200 / 1500: loss 3.605289\n",
      "iteration 1300 / 1500: loss 2.973744\n",
      "iteration 1400 / 1500: loss 2.609164\n",
      "iteration 0 / 1500: loss 397.893828\n",
      "iteration 100 / 1500: loss 292.676579\n",
      "iteration 200 / 1500: loss 215.652020\n",
      "iteration 300 / 1500: loss 159.170686\n",
      "iteration 400 / 1500: loss 117.553434\n",
      "iteration 500 / 1500: loss 87.021690\n",
      "iteration 600 / 1500: loss 64.523517\n",
      "iteration 700 / 1500: loss 47.983999\n",
      "iteration 800 / 1500: loss 35.717498\n",
      "iteration 900 / 1500: loss 26.924237\n",
      "iteration 1000 / 1500: loss 20.385026\n",
      "iteration 1100 / 1500: loss 15.526871\n",
      "iteration 1200 / 1500: loss 11.973030\n",
      "iteration 1300 / 1500: loss 9.295675\n",
      "iteration 1400 / 1500: loss 7.442721\n",
      "iteration 0 / 1500: loss 1083.064570\n",
      "iteration 100 / 1500: loss 465.233726\n",
      "iteration 200 / 1500: loss 200.744487\n",
      "iteration 300 / 1500: loss 87.205700\n",
      "iteration 400 / 1500: loss 38.579412\n",
      "iteration 500 / 1500: loss 17.741277\n",
      "iteration 600 / 1500: loss 8.822948\n",
      "iteration 700 / 1500: loss 4.950676\n",
      "iteration 800 / 1500: loss 3.280301\n",
      "iteration 900 / 1500: loss 2.647302\n",
      "iteration 1000 / 1500: loss 2.341675\n",
      "iteration 1100 / 1500: loss 2.189566\n",
      "iteration 1200 / 1500: loss 2.113644\n",
      "iteration 1300 / 1500: loss 2.149495\n",
      "iteration 1400 / 1500: loss 2.164611\n",
      "iteration 0 / 1500: loss 391.257036\n",
      "iteration 100 / 1500: loss 289.206926\n",
      "iteration 200 / 1500: loss 214.052988\n",
      "iteration 300 / 1500: loss 159.410554\n",
      "iteration 400 / 1500: loss 118.392093\n",
      "iteration 500 / 1500: loss 88.035384\n",
      "iteration 600 / 1500: loss 65.915978\n",
      "iteration 700 / 1500: loss 49.369010\n",
      "iteration 800 / 1500: loss 37.002860\n",
      "iteration 900 / 1500: loss 27.910153\n",
      "iteration 1000 / 1500: loss 21.229303\n",
      "iteration 1100 / 1500: loss 16.313874\n",
      "iteration 1200 / 1500: loss 12.586531\n",
      "iteration 1300 / 1500: loss 9.857502\n",
      "iteration 1400 / 1500: loss 7.777443\n",
      "iteration 0 / 1500: loss 1240.028725\n",
      "iteration 100 / 1500: loss 469.932498\n",
      "iteration 200 / 1500: loss 178.683008\n",
      "iteration 300 / 1500: loss 68.799829\n",
      "iteration 400 / 1500: loss 27.323637\n",
      "iteration 500 / 1500: loss 11.674720\n",
      "iteration 600 / 1500: loss 5.746673\n",
      "iteration 700 / 1500: loss 3.533730\n",
      "iteration 800 / 1500: loss 2.638555\n",
      "iteration 900 / 1500: loss 2.304614\n",
      "iteration 1000 / 1500: loss 2.200449\n",
      "iteration 1100 / 1500: loss 2.138472\n",
      "iteration 1200 / 1500: loss 2.131876\n",
      "iteration 1300 / 1500: loss 2.145581\n",
      "iteration 1400 / 1500: loss 2.149217\n",
      "iteration 0 / 1500: loss 1196.107080\n",
      "iteration 100 / 1500: loss 470.309032\n",
      "iteration 200 / 1500: loss 185.813703\n",
      "iteration 300 / 1500: loss 74.273770\n",
      "iteration 400 / 1500: loss 30.469873\n",
      "iteration 500 / 1500: loss 13.233401\n",
      "iteration 600 / 1500: loss 6.505814\n",
      "iteration 700 / 1500: loss 3.863030\n",
      "iteration 800 / 1500: loss 2.767252\n",
      "iteration 900 / 1500: loss 2.370024\n",
      "iteration 1000 / 1500: loss 2.225400\n",
      "iteration 1100 / 1500: loss 2.156597\n",
      "iteration 1200 / 1500: loss 2.147285\n",
      "iteration 1300 / 1500: loss 2.134972\n",
      "iteration 1400 / 1500: loss 2.119407\n",
      "iteration 0 / 1500: loss 387.469584\n",
      "iteration 100 / 1500: loss 286.782713\n",
      "iteration 200 / 1500: loss 213.035022\n",
      "iteration 300 / 1500: loss 158.401892\n",
      "iteration 400 / 1500: loss 118.095846\n",
      "iteration 500 / 1500: loss 88.171895\n",
      "iteration 600 / 1500: loss 65.880037\n",
      "iteration 700 / 1500: loss 49.357692\n",
      "iteration 800 / 1500: loss 37.182498\n",
      "iteration 900 / 1500: loss 28.111792\n",
      "iteration 1000 / 1500: loss 21.402383\n",
      "iteration 1100 / 1500: loss 16.398637\n",
      "iteration 1200 / 1500: loss 12.670099\n",
      "iteration 1300 / 1500: loss 9.997978\n",
      "iteration 1400 / 1500: loss 7.940861\n",
      "iteration 0 / 1500: loss 1292.067340\n",
      "iteration 100 / 1500: loss 481.348980\n",
      "iteration 200 / 1500: loss 180.100045\n",
      "iteration 300 / 1500: loss 68.291323\n",
      "iteration 400 / 1500: loss 26.677804\n",
      "iteration 500 / 1500: loss 11.269272\n",
      "iteration 600 / 1500: loss 5.525625\n",
      "iteration 700 / 1500: loss 3.440956\n",
      "iteration 800 / 1500: loss 2.604108\n",
      "iteration 900 / 1500: loss 2.295181\n",
      "iteration 1000 / 1500: loss 2.181165\n",
      "iteration 1100 / 1500: loss 2.172754\n",
      "iteration 1200 / 1500: loss 2.152202\n",
      "iteration 1300 / 1500: loss 2.111547\n",
      "iteration 1400 / 1500: loss 2.143765\n",
      "iteration 0 / 1500: loss 467.693539\n",
      "iteration 100 / 1500: loss 263.032222\n",
      "iteration 200 / 1500: loss 148.759103\n",
      "iteration 300 / 1500: loss 84.571708\n",
      "iteration 400 / 1500: loss 48.463660\n",
      "iteration 500 / 1500: loss 28.093256\n",
      "iteration 600 / 1500: loss 16.728249\n",
      "iteration 700 / 1500: loss 10.362404\n",
      "iteration 800 / 1500: loss 6.665268\n",
      "iteration 900 / 1500: loss 4.650275\n",
      "iteration 1000 / 1500: loss 3.482315\n",
      "iteration 1100 / 1500: loss 2.881584\n",
      "iteration 1200 / 1500: loss 2.503640\n",
      "iteration 1300 / 1500: loss 2.321608\n",
      "iteration 1400 / 1500: loss 2.281594\n",
      "iteration 0 / 1500: loss 414.438408\n",
      "iteration 100 / 1500: loss 247.011325\n",
      "iteration 200 / 1500: loss 147.495438\n",
      "iteration 300 / 1500: loss 88.543632\n",
      "iteration 400 / 1500: loss 53.639031\n",
      "iteration 500 / 1500: loss 32.759419\n",
      "iteration 600 / 1500: loss 20.214483\n",
      "iteration 700 / 1500: loss 12.836963\n",
      "iteration 800 / 1500: loss 8.518663\n",
      "iteration 900 / 1500: loss 5.934755\n",
      "iteration 1000 / 1500: loss 4.372698\n",
      "iteration 1100 / 1500: loss 3.429727\n",
      "iteration 1200 / 1500: loss 2.792384\n",
      "iteration 1300 / 1500: loss 2.476527\n",
      "iteration 1400 / 1500: loss 2.402036\n",
      "iteration 0 / 1500: loss 552.640687\n",
      "iteration 100 / 1500: loss 276.880584\n",
      "iteration 200 / 1500: loss 139.584680\n",
      "iteration 300 / 1500: loss 70.845628\n",
      "iteration 400 / 1500: loss 36.641599\n",
      "iteration 500 / 1500: loss 19.361283\n",
      "iteration 600 / 1500: loss 10.742616\n",
      "iteration 700 / 1500: loss 6.365882\n",
      "iteration 800 / 1500: loss 4.189870\n",
      "iteration 900 / 1500: loss 3.157397\n",
      "iteration 1000 / 1500: loss 2.629466\n",
      "iteration 1100 / 1500: loss 2.370064\n",
      "iteration 1200 / 1500: loss 2.112210\n",
      "iteration 1300 / 1500: loss 2.185614\n",
      "iteration 1400 / 1500: loss 2.154589\n",
      "iteration 0 / 1500: loss 1524.509064\n",
      "iteration 100 / 1500: loss 223.289708\n",
      "iteration 200 / 1500: loss 34.231493\n",
      "iteration 300 / 1500: loss 6.776076\n",
      "iteration 400 / 1500: loss 2.776400\n",
      "iteration 500 / 1500: loss 2.267836\n",
      "iteration 600 / 1500: loss 2.142496\n",
      "iteration 700 / 1500: loss 2.157706\n",
      "iteration 800 / 1500: loss 2.163417\n",
      "iteration 900 / 1500: loss 2.124814\n",
      "iteration 1000 / 1500: loss 2.170628\n",
      "iteration 1100 / 1500: loss 2.165469\n",
      "iteration 1200 / 1500: loss 2.136867\n",
      "iteration 1300 / 1500: loss 2.090434\n",
      "iteration 1400 / 1500: loss 2.169498\n",
      "iteration 0 / 1500: loss 1499.887098\n",
      "iteration 100 / 1500: loss 226.279344\n",
      "iteration 200 / 1500: loss 35.699905\n",
      "iteration 300 / 1500: loss 7.131753\n",
      "iteration 400 / 1500: loss 2.898691\n",
      "iteration 500 / 1500: loss 2.241584\n",
      "iteration 600 / 1500: loss 2.156843\n",
      "iteration 700 / 1500: loss 2.154473\n",
      "iteration 800 / 1500: loss 2.159993\n",
      "iteration 900 / 1500: loss 2.157074\n",
      "iteration 1000 / 1500: loss 2.120174\n",
      "iteration 1100 / 1500: loss 2.120996\n",
      "iteration 1200 / 1500: loss 2.161708\n",
      "iteration 1300 / 1500: loss 2.135011\n",
      "iteration 1400 / 1500: loss 2.166836\n",
      "iteration 0 / 1500: loss 822.593024\n",
      "iteration 100 / 1500: loss 297.770879\n",
      "iteration 200 / 1500: loss 108.634812\n",
      "iteration 300 / 1500: loss 40.502831\n",
      "iteration 400 / 1500: loss 15.963783\n",
      "iteration 500 / 1500: loss 7.105863\n",
      "iteration 600 / 1500: loss 3.893612\n",
      "iteration 700 / 1500: loss 2.698294\n",
      "iteration 800 / 1500: loss 2.306500\n",
      "iteration 900 / 1500: loss 2.203951\n",
      "iteration 1000 / 1500: loss 2.154280\n",
      "iteration 1100 / 1500: loss 2.045650\n",
      "iteration 1200 / 1500: loss 2.108888\n",
      "iteration 1300 / 1500: loss 2.106693\n",
      "iteration 1400 / 1500: loss 2.073976\n",
      "iteration 0 / 1500: loss 971.347691\n",
      "iteration 100 / 1500: loss 286.820436\n",
      "iteration 200 / 1500: loss 85.822382\n",
      "iteration 300 / 1500: loss 26.703965\n",
      "iteration 400 / 1500: loss 9.312460\n",
      "iteration 500 / 1500: loss 4.276842\n",
      "iteration 600 / 1500: loss 2.729996\n",
      "iteration 700 / 1500: loss 2.318229\n",
      "iteration 800 / 1500: loss 2.197325\n",
      "iteration 900 / 1500: loss 2.101526\n",
      "iteration 1000 / 1500: loss 2.099790\n",
      "iteration 1100 / 1500: loss 2.098460\n",
      "iteration 1200 / 1500: loss 2.119167\n",
      "iteration 1300 / 1500: loss 2.128738\n",
      "iteration 1400 / 1500: loss 2.071034\n",
      "iteration 0 / 1500: loss 1057.237089\n",
      "iteration 100 / 1500: loss 278.289605\n",
      "iteration 200 / 1500: loss 74.437378\n",
      "iteration 300 / 1500: loss 21.080467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 1500: loss 7.074164\n",
      "iteration 500 / 1500: loss 3.395373\n",
      "iteration 600 / 1500: loss 2.472401\n",
      "iteration 700 / 1500: loss 2.248090\n",
      "iteration 800 / 1500: loss 2.146613\n",
      "iteration 900 / 1500: loss 2.098435\n",
      "iteration 1000 / 1500: loss 2.090082\n",
      "iteration 1100 / 1500: loss 2.123152\n",
      "iteration 1200 / 1500: loss 2.115095\n",
      "iteration 1300 / 1500: loss 2.138880\n",
      "iteration 1400 / 1500: loss 2.086939\n",
      "iteration 0 / 1500: loss 1434.583295\n",
      "iteration 100 / 1500: loss 232.663961\n",
      "iteration 200 / 1500: loss 39.237882\n",
      "iteration 300 / 1500: loss 8.106242\n",
      "iteration 400 / 1500: loss 3.160919\n",
      "iteration 500 / 1500: loss 2.266959\n",
      "iteration 600 / 1500: loss 2.180871\n",
      "iteration 700 / 1500: loss 2.169182\n",
      "iteration 800 / 1500: loss 2.132935\n",
      "iteration 900 / 1500: loss 2.138659\n",
      "iteration 1000 / 1500: loss 2.138828\n",
      "iteration 1100 / 1500: loss 2.137394\n",
      "iteration 1200 / 1500: loss 2.123612\n",
      "iteration 1300 / 1500: loss 2.144752\n",
      "iteration 1400 / 1500: loss 2.127770\n",
      "iteration 0 / 1500: loss 475.165054\n",
      "iteration 100 / 1500: loss 263.178312\n",
      "iteration 200 / 1500: loss 147.028250\n",
      "iteration 300 / 1500: loss 82.452100\n",
      "iteration 400 / 1500: loss 46.709817\n",
      "iteration 500 / 1500: loss 26.867218\n",
      "iteration 600 / 1500: loss 15.765894\n",
      "iteration 700 / 1500: loss 9.697626\n",
      "iteration 800 / 1500: loss 6.274694\n",
      "iteration 900 / 1500: loss 4.421195\n",
      "iteration 1000 / 1500: loss 3.360266\n",
      "iteration 1100 / 1500: loss 2.794772\n",
      "iteration 1200 / 1500: loss 2.436326\n",
      "iteration 1300 / 1500: loss 2.308727\n",
      "iteration 1400 / 1500: loss 2.152745\n",
      "iteration 0 / 1500: loss 325.253122\n",
      "iteration 100 / 1500: loss 216.265325\n",
      "iteration 200 / 1500: loss 144.448490\n",
      "iteration 300 / 1500: loss 96.834278\n",
      "iteration 400 / 1500: loss 65.130636\n",
      "iteration 500 / 1500: loss 44.061613\n",
      "iteration 600 / 1500: loss 29.964433\n",
      "iteration 700 / 1500: loss 20.600917\n",
      "iteration 800 / 1500: loss 14.473384\n",
      "iteration 900 / 1500: loss 10.325050\n",
      "iteration 1000 / 1500: loss 7.507469\n",
      "iteration 1100 / 1500: loss 5.635744\n",
      "iteration 1200 / 1500: loss 4.442771\n",
      "iteration 1300 / 1500: loss 3.674270\n",
      "iteration 1400 / 1500: loss 3.095560\n",
      "iteration 0 / 1500: loss 363.889864\n",
      "iteration 100 / 1500: loss 232.638965\n",
      "iteration 200 / 1500: loss 149.354368\n",
      "iteration 300 / 1500: loss 96.285853\n",
      "iteration 400 / 1500: loss 62.357383\n",
      "iteration 500 / 1500: loss 40.682970\n",
      "iteration 600 / 1500: loss 26.815286\n",
      "iteration 700 / 1500: loss 17.837844\n",
      "iteration 800 / 1500: loss 12.187247\n",
      "iteration 900 / 1500: loss 8.556712\n",
      "iteration 1000 / 1500: loss 6.155635\n",
      "iteration 1100 / 1500: loss 4.645513\n",
      "iteration 1200 / 1500: loss 3.752599\n",
      "iteration 1300 / 1500: loss 3.142488\n",
      "iteration 1400 / 1500: loss 2.682879\n",
      "iteration 0 / 1500: loss 648.669913\n",
      "iteration 100 / 1500: loss 287.400422\n",
      "iteration 200 / 1500: loss 128.235772\n",
      "iteration 300 / 1500: loss 57.901083\n",
      "iteration 400 / 1500: loss 26.753677\n",
      "iteration 500 / 1500: loss 12.964929\n",
      "iteration 600 / 1500: loss 6.870244\n",
      "iteration 700 / 1500: loss 4.159699\n",
      "iteration 800 / 1500: loss 3.004838\n",
      "iteration 900 / 1500: loss 2.489376\n",
      "iteration 1000 / 1500: loss 2.238765\n",
      "iteration 1100 / 1500: loss 2.162979\n",
      "iteration 1200 / 1500: loss 2.132250\n",
      "iteration 1300 / 1500: loss 2.103941\n",
      "iteration 1400 / 1500: loss 2.068828\n",
      "iteration 0 / 1500: loss 400.730042\n",
      "iteration 100 / 1500: loss 243.484731\n",
      "iteration 200 / 1500: loss 149.103909\n",
      "iteration 300 / 1500: loss 91.657451\n",
      "iteration 400 / 1500: loss 56.378554\n",
      "iteration 500 / 1500: loss 35.163791\n",
      "iteration 600 / 1500: loss 22.296198\n",
      "iteration 700 / 1500: loss 14.360421\n",
      "iteration 800 / 1500: loss 9.540337\n",
      "iteration 900 / 1500: loss 6.562533\n",
      "iteration 1000 / 1500: loss 4.818513\n",
      "iteration 1100 / 1500: loss 3.711682\n",
      "iteration 1200 / 1500: loss 3.115100\n",
      "iteration 1300 / 1500: loss 2.652261\n",
      "iteration 1400 / 1500: loss 2.422132\n",
      "iteration 0 / 1500: loss 1101.234835\n",
      "iteration 100 / 1500: loss 280.618060\n",
      "iteration 200 / 1500: loss 72.745123\n",
      "iteration 300 / 1500: loss 20.007526\n",
      "iteration 400 / 1500: loss 6.681385\n",
      "iteration 500 / 1500: loss 3.265559\n",
      "iteration 600 / 1500: loss 2.378417\n",
      "iteration 700 / 1500: loss 2.229520\n",
      "iteration 800 / 1500: loss 2.124225\n",
      "iteration 900 / 1500: loss 2.054986\n",
      "iteration 1000 / 1500: loss 2.101872\n",
      "iteration 1100 / 1500: loss 2.082509\n",
      "iteration 1200 / 1500: loss 2.147310\n",
      "iteration 1300 / 1500: loss 2.100903\n",
      "iteration 1400 / 1500: loss 2.129431\n",
      "iteration 0 / 1500: loss 393.235629\n",
      "iteration 100 / 1500: loss 242.097016\n",
      "iteration 200 / 1500: loss 149.638526\n",
      "iteration 300 / 1500: loss 92.720354\n",
      "iteration 400 / 1500: loss 57.933640\n",
      "iteration 500 / 1500: loss 36.366203\n",
      "iteration 600 / 1500: loss 23.176402\n",
      "iteration 700 / 1500: loss 15.092463\n",
      "iteration 800 / 1500: loss 9.974487\n",
      "iteration 900 / 1500: loss 7.034914\n",
      "iteration 1000 / 1500: loss 5.020090\n",
      "iteration 1100 / 1500: loss 3.825905\n",
      "iteration 1200 / 1500: loss 3.133578\n",
      "iteration 1300 / 1500: loss 2.767862\n",
      "iteration 1400 / 1500: loss 2.461090\n",
      "iteration 0 / 1500: loss 1262.494715\n",
      "iteration 100 / 1500: loss 262.748019\n",
      "iteration 200 / 1500: loss 55.970332\n",
      "iteration 300 / 1500: loss 13.263670\n",
      "iteration 400 / 1500: loss 4.412006\n",
      "iteration 500 / 1500: loss 2.574866\n",
      "iteration 600 / 1500: loss 2.249006\n",
      "iteration 700 / 1500: loss 2.154870\n",
      "iteration 800 / 1500: loss 2.159901\n",
      "iteration 900 / 1500: loss 2.131409\n",
      "iteration 1000 / 1500: loss 2.141439\n",
      "iteration 1100 / 1500: loss 2.122717\n",
      "iteration 1200 / 1500: loss 2.177797\n",
      "iteration 1300 / 1500: loss 2.131973\n",
      "iteration 1400 / 1500: loss 2.173772\n",
      "iteration 0 / 1500: loss 1193.767707\n",
      "iteration 100 / 1500: loss 264.311011\n",
      "iteration 200 / 1500: loss 59.837578\n",
      "iteration 300 / 1500: loss 14.831552\n",
      "iteration 400 / 1500: loss 4.914659\n",
      "iteration 500 / 1500: loss 2.725451\n",
      "iteration 600 / 1500: loss 2.258341\n",
      "iteration 700 / 1500: loss 2.158359\n",
      "iteration 800 / 1500: loss 2.133108\n",
      "iteration 900 / 1500: loss 2.090201\n",
      "iteration 1000 / 1500: loss 2.125717\n",
      "iteration 1100 / 1500: loss 2.149708\n",
      "iteration 1200 / 1500: loss 2.121179\n",
      "iteration 1300 / 1500: loss 2.140021\n",
      "iteration 1400 / 1500: loss 2.113628\n",
      "iteration 0 / 1500: loss 391.487711\n",
      "iteration 100 / 1500: loss 242.108925\n",
      "iteration 200 / 1500: loss 149.799896\n",
      "iteration 300 / 1500: loss 93.236368\n",
      "iteration 400 / 1500: loss 58.344063\n",
      "iteration 500 / 1500: loss 36.725097\n",
      "iteration 600 / 1500: loss 23.465599\n",
      "iteration 700 / 1500: loss 15.291490\n",
      "iteration 800 / 1500: loss 10.151141\n",
      "iteration 900 / 1500: loss 7.076762\n",
      "iteration 1000 / 1500: loss 5.143188\n",
      "iteration 1100 / 1500: loss 4.016398\n",
      "iteration 1200 / 1500: loss 3.117878\n",
      "iteration 1300 / 1500: loss 2.725153\n",
      "iteration 1400 / 1500: loss 2.458742\n",
      "iteration 0 / 1500: loss 1259.371224\n",
      "iteration 100 / 1500: loss 254.790199\n",
      "iteration 200 / 1500: loss 52.976881\n",
      "iteration 300 / 1500: loss 12.368405\n",
      "iteration 400 / 1500: loss 4.212808\n",
      "iteration 500 / 1500: loss 2.541891\n",
      "iteration 600 / 1500: loss 2.198796\n",
      "iteration 700 / 1500: loss 2.108350\n",
      "iteration 800 / 1500: loss 2.138163\n",
      "iteration 900 / 1500: loss 2.137525\n",
      "iteration 1000 / 1500: loss 2.116820\n",
      "iteration 1100 / 1500: loss 2.167069\n",
      "iteration 1200 / 1500: loss 2.135499\n",
      "iteration 1300 / 1500: loss 2.142793\n",
      "iteration 1400 / 1500: loss 2.094656\n",
      "iteration 0 / 1500: loss 460.703467\n",
      "iteration 100 / 1500: loss 33.869582\n",
      "iteration 200 / 1500: loss 4.319613\n",
      "iteration 300 / 1500: loss 2.151092\n",
      "iteration 400 / 1500: loss 2.046324\n",
      "iteration 500 / 1500: loss 2.015482\n",
      "iteration 600 / 1500: loss 2.072264\n",
      "iteration 700 / 1500: loss 1.981926\n",
      "iteration 800 / 1500: loss 1.985602\n",
      "iteration 900 / 1500: loss 2.025980\n",
      "iteration 1000 / 1500: loss 1.977596\n",
      "iteration 1100 / 1500: loss 2.066887\n",
      "iteration 1200 / 1500: loss 2.062584\n",
      "iteration 1300 / 1500: loss 2.073127\n",
      "iteration 1400 / 1500: loss 1.989615\n",
      "iteration 0 / 1500: loss 415.411216\n",
      "iteration 100 / 1500: loss 39.177024\n",
      "iteration 200 / 1500: loss 5.398406\n",
      "iteration 300 / 1500: loss 2.342590\n",
      "iteration 400 / 1500: loss 1.985836\n",
      "iteration 500 / 1500: loss 1.977311\n",
      "iteration 600 / 1500: loss 2.018163\n",
      "iteration 700 / 1500: loss 2.075388\n",
      "iteration 800 / 1500: loss 2.001279\n",
      "iteration 900 / 1500: loss 2.017940\n",
      "iteration 1000 / 1500: loss 2.038850\n",
      "iteration 1100 / 1500: loss 1.999622\n",
      "iteration 1200 / 1500: loss 2.057023\n",
      "iteration 1300 / 1500: loss 2.101452\n",
      "iteration 1400 / 1500: loss 2.016971\n",
      "iteration 0 / 1500: loss 549.849997\n",
      "iteration 100 / 1500: loss 24.067958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 2.932304\n",
      "iteration 300 / 1500: loss 2.046355\n",
      "iteration 400 / 1500: loss 2.052071\n",
      "iteration 500 / 1500: loss 2.062995\n",
      "iteration 600 / 1500: loss 2.065017\n",
      "iteration 700 / 1500: loss 2.037004\n",
      "iteration 800 / 1500: loss 2.140242\n",
      "iteration 900 / 1500: loss 2.073365\n",
      "iteration 1000 / 1500: loss 2.040091\n",
      "iteration 1100 / 1500: loss 2.008860\n",
      "iteration 1200 / 1500: loss 2.000086\n",
      "iteration 1300 / 1500: loss 2.059543\n",
      "iteration 1400 / 1500: loss 2.077208\n",
      "iteration 0 / 1500: loss 1528.216015\n",
      "iteration 100 / 1500: loss 2.348226\n",
      "iteration 200 / 1500: loss 2.152576\n",
      "iteration 300 / 1500: loss 2.134468\n",
      "iteration 400 / 1500: loss 2.130617\n",
      "iteration 500 / 1500: loss 2.180201\n",
      "iteration 600 / 1500: loss 2.143593\n",
      "iteration 700 / 1500: loss 2.127727\n",
      "iteration 800 / 1500: loss 2.148376\n",
      "iteration 900 / 1500: loss 2.195996\n",
      "iteration 1000 / 1500: loss 2.127968\n",
      "iteration 1100 / 1500: loss 2.152109\n",
      "iteration 1200 / 1500: loss 2.167106\n",
      "iteration 1300 / 1500: loss 2.108176\n",
      "iteration 1400 / 1500: loss 2.156504\n",
      "iteration 0 / 1500: loss 1497.560238\n",
      "iteration 100 / 1500: loss 2.320852\n",
      "iteration 200 / 1500: loss 2.070846\n",
      "iteration 300 / 1500: loss 2.152578\n",
      "iteration 400 / 1500: loss 2.130661\n",
      "iteration 500 / 1500: loss 2.166792\n",
      "iteration 600 / 1500: loss 2.094238\n",
      "iteration 700 / 1500: loss 2.091179\n",
      "iteration 800 / 1500: loss 2.182895\n",
      "iteration 900 / 1500: loss 2.182441\n",
      "iteration 1000 / 1500: loss 2.138563\n",
      "iteration 1100 / 1500: loss 2.121049\n",
      "iteration 1200 / 1500: loss 2.103554\n",
      "iteration 1300 / 1500: loss 2.130770\n",
      "iteration 1400 / 1500: loss 2.177384\n",
      "iteration 0 / 1500: loss 818.174507\n",
      "iteration 100 / 1500: loss 9.176294\n",
      "iteration 200 / 1500: loss 2.117834\n",
      "iteration 300 / 1500: loss 2.107083\n",
      "iteration 400 / 1500: loss 2.068634\n",
      "iteration 500 / 1500: loss 2.055971\n",
      "iteration 600 / 1500: loss 2.138179\n",
      "iteration 700 / 1500: loss 2.146949\n",
      "iteration 800 / 1500: loss 2.100089\n",
      "iteration 900 / 1500: loss 2.037376\n",
      "iteration 1000 / 1500: loss 2.204112\n",
      "iteration 1100 / 1500: loss 2.073260\n",
      "iteration 1200 / 1500: loss 2.035975\n",
      "iteration 1300 / 1500: loss 2.059040\n",
      "iteration 1400 / 1500: loss 2.086889\n",
      "iteration 0 / 1500: loss 977.547383\n",
      "iteration 100 / 1500: loss 5.334375\n",
      "iteration 200 / 1500: loss 2.073854\n",
      "iteration 300 / 1500: loss 2.136445\n",
      "iteration 400 / 1500: loss 2.129400\n",
      "iteration 500 / 1500: loss 2.088818\n",
      "iteration 600 / 1500: loss 2.088650\n",
      "iteration 700 / 1500: loss 2.120722\n",
      "iteration 800 / 1500: loss 2.065878\n",
      "iteration 900 / 1500: loss 2.152333\n",
      "iteration 1000 / 1500: loss 2.133457\n",
      "iteration 1100 / 1500: loss 2.098242\n",
      "iteration 1200 / 1500: loss 2.103061\n",
      "iteration 1300 / 1500: loss 2.138453\n",
      "iteration 1400 / 1500: loss 2.115658\n",
      "iteration 0 / 1500: loss 1059.602809\n",
      "iteration 100 / 1500: loss 4.148729\n",
      "iteration 200 / 1500: loss 2.107130\n",
      "iteration 300 / 1500: loss 2.135331\n",
      "iteration 400 / 1500: loss 2.143744\n",
      "iteration 500 / 1500: loss 2.091558\n",
      "iteration 600 / 1500: loss 2.163706\n",
      "iteration 700 / 1500: loss 2.046017\n",
      "iteration 800 / 1500: loss 2.120999\n",
      "iteration 900 / 1500: loss 2.088062\n",
      "iteration 1000 / 1500: loss 2.127924\n",
      "iteration 1100 / 1500: loss 2.153360\n",
      "iteration 1200 / 1500: loss 2.127983\n",
      "iteration 1300 / 1500: loss 2.115955\n",
      "iteration 1400 / 1500: loss 2.115177\n",
      "iteration 0 / 1500: loss 1433.192677\n",
      "iteration 100 / 1500: loss 2.367188\n",
      "iteration 200 / 1500: loss 2.128705\n",
      "iteration 300 / 1500: loss 2.124003\n",
      "iteration 400 / 1500: loss 2.188188\n",
      "iteration 500 / 1500: loss 2.158330\n",
      "iteration 600 / 1500: loss 2.170529\n",
      "iteration 700 / 1500: loss 2.166137\n",
      "iteration 800 / 1500: loss 2.128150\n",
      "iteration 900 / 1500: loss 2.149134\n",
      "iteration 1000 / 1500: loss 2.175415\n",
      "iteration 1100 / 1500: loss 2.120218\n",
      "iteration 1200 / 1500: loss 2.149414\n",
      "iteration 1300 / 1500: loss 2.139118\n",
      "iteration 1400 / 1500: loss 2.159653\n",
      "iteration 0 / 1500: loss 476.021225\n",
      "iteration 100 / 1500: loss 33.057743\n",
      "iteration 200 / 1500: loss 4.137517\n",
      "iteration 300 / 1500: loss 2.189757\n",
      "iteration 400 / 1500: loss 2.041628\n",
      "iteration 500 / 1500: loss 2.060486\n",
      "iteration 600 / 1500: loss 1.998791\n",
      "iteration 700 / 1500: loss 2.055749\n",
      "iteration 800 / 1500: loss 2.031380\n",
      "iteration 900 / 1500: loss 2.042964\n",
      "iteration 1000 / 1500: loss 2.066121\n",
      "iteration 1100 / 1500: loss 2.072488\n",
      "iteration 1200 / 1500: loss 2.102805\n",
      "iteration 1300 / 1500: loss 2.089061\n",
      "iteration 1400 / 1500: loss 2.039836\n",
      "iteration 0 / 1500: loss 330.283807\n",
      "iteration 100 / 1500: loss 51.709430\n",
      "iteration 200 / 1500: loss 9.545068\n",
      "iteration 300 / 1500: loss 3.211160\n",
      "iteration 400 / 1500: loss 2.246525\n",
      "iteration 500 / 1500: loss 2.086475\n",
      "iteration 600 / 1500: loss 2.058159\n",
      "iteration 700 / 1500: loss 2.094441\n",
      "iteration 800 / 1500: loss 2.033175\n",
      "iteration 900 / 1500: loss 2.029352\n",
      "iteration 1000 / 1500: loss 2.051503\n",
      "iteration 1100 / 1500: loss 1.927326\n",
      "iteration 1200 / 1500: loss 2.057987\n",
      "iteration 1300 / 1500: loss 2.044641\n",
      "iteration 1400 / 1500: loss 1.996764\n",
      "iteration 0 / 1500: loss 358.900073\n",
      "iteration 100 / 1500: loss 47.225125\n",
      "iteration 200 / 1500: loss 7.775512\n",
      "iteration 300 / 1500: loss 2.782433\n",
      "iteration 400 / 1500: loss 2.146895\n",
      "iteration 500 / 1500: loss 2.054983\n",
      "iteration 600 / 1500: loss 2.040940\n",
      "iteration 700 / 1500: loss 2.027451\n",
      "iteration 800 / 1500: loss 1.992122\n",
      "iteration 900 / 1500: loss 2.027430\n",
      "iteration 1000 / 1500: loss 1.984506\n",
      "iteration 1100 / 1500: loss 1.940615\n",
      "iteration 1200 / 1500: loss 1.998107\n",
      "iteration 1300 / 1500: loss 2.037944\n",
      "iteration 1400 / 1500: loss 2.002893\n",
      "iteration 0 / 1500: loss 643.491558\n",
      "iteration 100 / 1500: loss 16.591730\n",
      "iteration 200 / 1500: loss 2.365536\n",
      "iteration 300 / 1500: loss 2.102882\n",
      "iteration 400 / 1500: loss 2.096545\n",
      "iteration 500 / 1500: loss 2.075870\n",
      "iteration 600 / 1500: loss 2.126218\n",
      "iteration 700 / 1500: loss 2.061972\n",
      "iteration 800 / 1500: loss 2.017478\n",
      "iteration 900 / 1500: loss 2.076997\n",
      "iteration 1000 / 1500: loss 2.072849\n",
      "iteration 1100 / 1500: loss 1.997343\n",
      "iteration 1200 / 1500: loss 2.113044\n",
      "iteration 1300 / 1500: loss 2.007833\n",
      "iteration 1400 / 1500: loss 2.093096\n",
      "iteration 0 / 1500: loss 392.348552\n",
      "iteration 100 / 1500: loss 41.147987\n",
      "iteration 200 / 1500: loss 6.025084\n",
      "iteration 300 / 1500: loss 2.430717\n",
      "iteration 400 / 1500: loss 2.058878\n",
      "iteration 500 / 1500: loss 2.115253\n",
      "iteration 600 / 1500: loss 2.041521\n",
      "iteration 700 / 1500: loss 2.004103\n",
      "iteration 800 / 1500: loss 1.967559\n",
      "iteration 900 / 1500: loss 2.021101\n",
      "iteration 1000 / 1500: loss 2.028727\n",
      "iteration 1100 / 1500: loss 2.020809\n",
      "iteration 1200 / 1500: loss 1.960569\n",
      "iteration 1300 / 1500: loss 2.025669\n",
      "iteration 1400 / 1500: loss 2.106575\n",
      "iteration 0 / 1500: loss 1077.684211\n",
      "iteration 100 / 1500: loss 3.878911\n",
      "iteration 200 / 1500: loss 2.201717\n",
      "iteration 300 / 1500: loss 2.136406\n",
      "iteration 400 / 1500: loss 2.151959\n",
      "iteration 500 / 1500: loss 2.089696\n",
      "iteration 600 / 1500: loss 2.073469\n",
      "iteration 700 / 1500: loss 2.079786\n",
      "iteration 800 / 1500: loss 2.129800\n",
      "iteration 900 / 1500: loss 2.112537\n",
      "iteration 1000 / 1500: loss 2.133364\n",
      "iteration 1100 / 1500: loss 2.079109\n",
      "iteration 1200 / 1500: loss 2.158484\n",
      "iteration 1300 / 1500: loss 2.126399\n",
      "iteration 1400 / 1500: loss 2.175069\n",
      "iteration 0 / 1500: loss 392.121012\n",
      "iteration 100 / 1500: loss 43.168152\n",
      "iteration 200 / 1500: loss 6.357119\n",
      "iteration 300 / 1500: loss 2.547004\n",
      "iteration 400 / 1500: loss 2.086628\n",
      "iteration 500 / 1500: loss 1.989521\n",
      "iteration 600 / 1500: loss 2.015995\n",
      "iteration 700 / 1500: loss 2.000183\n",
      "iteration 800 / 1500: loss 1.966758\n",
      "iteration 900 / 1500: loss 1.987774\n",
      "iteration 1000 / 1500: loss 2.068871\n",
      "iteration 1100 / 1500: loss 2.068125\n",
      "iteration 1200 / 1500: loss 2.062222\n",
      "iteration 1300 / 1500: loss 2.038485\n",
      "iteration 1400 / 1500: loss 2.092349\n",
      "iteration 0 / 1500: loss 1246.170646\n",
      "iteration 100 / 1500: loss 2.912171\n",
      "iteration 200 / 1500: loss 2.134644\n",
      "iteration 300 / 1500: loss 2.157422\n",
      "iteration 400 / 1500: loss 2.123290\n",
      "iteration 500 / 1500: loss 2.118655\n",
      "iteration 600 / 1500: loss 2.100240\n",
      "iteration 700 / 1500: loss 2.111000\n",
      "iteration 800 / 1500: loss 2.185085\n",
      "iteration 900 / 1500: loss 2.099444\n",
      "iteration 1000 / 1500: loss 2.107304\n",
      "iteration 1100 / 1500: loss 2.109203\n",
      "iteration 1200 / 1500: loss 2.126168\n",
      "iteration 1300 / 1500: loss 2.163706\n",
      "iteration 1400 / 1500: loss 2.082973\n",
      "iteration 0 / 1500: loss 1202.729621\n",
      "iteration 100 / 1500: loss 3.180526\n",
      "iteration 200 / 1500: loss 2.195656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 1500: loss 2.119523\n",
      "iteration 400 / 1500: loss 2.078283\n",
      "iteration 500 / 1500: loss 2.101955\n",
      "iteration 600 / 1500: loss 2.125353\n",
      "iteration 700 / 1500: loss 2.099900\n",
      "iteration 800 / 1500: loss 2.143595\n",
      "iteration 900 / 1500: loss 2.113531\n",
      "iteration 1000 / 1500: loss 2.121477\n",
      "iteration 1100 / 1500: loss 2.170154\n",
      "iteration 1200 / 1500: loss 2.137580\n",
      "iteration 1300 / 1500: loss 2.116334\n",
      "iteration 1400 / 1500: loss 2.111288\n",
      "iteration 0 / 1500: loss 395.149706\n",
      "iteration 100 / 1500: loss 44.094753\n",
      "iteration 200 / 1500: loss 6.543676\n",
      "iteration 300 / 1500: loss 2.479444\n",
      "iteration 400 / 1500: loss 2.036150\n",
      "iteration 500 / 1500: loss 2.022140\n",
      "iteration 600 / 1500: loss 2.061617\n",
      "iteration 700 / 1500: loss 1.993993\n",
      "iteration 800 / 1500: loss 1.960980\n",
      "iteration 900 / 1500: loss 2.022080\n",
      "iteration 1000 / 1500: loss 2.009737\n",
      "iteration 1100 / 1500: loss 2.011059\n",
      "iteration 1200 / 1500: loss 1.996147\n",
      "iteration 1300 / 1500: loss 2.026765\n",
      "iteration 1400 / 1500: loss 1.980267\n",
      "iteration 0 / 1500: loss 1277.713566\n",
      "iteration 100 / 1500: loss 2.853599\n",
      "iteration 200 / 1500: loss 2.150636\n",
      "iteration 300 / 1500: loss 2.131797\n",
      "iteration 400 / 1500: loss 2.146814\n",
      "iteration 500 / 1500: loss 2.131061\n",
      "iteration 600 / 1500: loss 2.172253\n",
      "iteration 700 / 1500: loss 2.184654\n",
      "iteration 800 / 1500: loss 2.150358\n",
      "iteration 900 / 1500: loss 2.120688\n",
      "iteration 1000 / 1500: loss 2.141750\n",
      "iteration 1100 / 1500: loss 2.159172\n",
      "iteration 1200 / 1500: loss 2.163672\n",
      "iteration 1300 / 1500: loss 2.156082\n",
      "iteration 1400 / 1500: loss 2.105577\n",
      "iteration 0 / 1500: loss 460.575313\n",
      "iteration 100 / 1500: loss 260.782107\n",
      "iteration 200 / 1500: loss 148.310031\n",
      "iteration 300 / 1500: loss 84.977101\n",
      "iteration 400 / 1500: loss 48.966689\n",
      "iteration 500 / 1500: loss 28.648604\n",
      "iteration 600 / 1500: loss 17.143355\n",
      "iteration 700 / 1500: loss 10.598360\n",
      "iteration 800 / 1500: loss 6.933077\n",
      "iteration 900 / 1500: loss 4.759709\n",
      "iteration 1000 / 1500: loss 3.560987\n",
      "iteration 1100 / 1500: loss 2.855179\n",
      "iteration 1200 / 1500: loss 2.654837\n",
      "iteration 1300 / 1500: loss 2.340039\n",
      "iteration 1400 / 1500: loss 2.207736\n",
      "iteration 0 / 1500: loss 413.440227\n",
      "iteration 100 / 1500: loss 247.795758\n",
      "iteration 200 / 1500: loss 149.218733\n",
      "iteration 300 / 1500: loss 90.052471\n",
      "iteration 400 / 1500: loss 54.763505\n",
      "iteration 500 / 1500: loss 33.674538\n",
      "iteration 600 / 1500: loss 20.989220\n",
      "iteration 700 / 1500: loss 13.315991\n",
      "iteration 800 / 1500: loss 8.813003\n",
      "iteration 900 / 1500: loss 6.092573\n",
      "iteration 1000 / 1500: loss 4.497150\n",
      "iteration 1100 / 1500: loss 3.533297\n",
      "iteration 1200 / 1500: loss 2.943603\n",
      "iteration 1300 / 1500: loss 2.569281\n",
      "iteration 1400 / 1500: loss 2.368585\n",
      "iteration 0 / 1500: loss 554.426701\n",
      "iteration 100 / 1500: loss 280.231788\n",
      "iteration 200 / 1500: loss 142.370848\n",
      "iteration 300 / 1500: loss 72.887312\n",
      "iteration 400 / 1500: loss 37.696249\n",
      "iteration 500 / 1500: loss 20.117868\n",
      "iteration 600 / 1500: loss 11.118943\n",
      "iteration 700 / 1500: loss 6.643453\n",
      "iteration 800 / 1500: loss 4.365948\n",
      "iteration 900 / 1500: loss 3.243483\n",
      "iteration 1000 / 1500: loss 2.606644\n",
      "iteration 1100 / 1500: loss 2.323713\n",
      "iteration 1200 / 1500: loss 2.194684\n",
      "iteration 1300 / 1500: loss 2.189740\n",
      "iteration 1400 / 1500: loss 2.068160\n",
      "iteration 0 / 1500: loss 1527.905470\n",
      "iteration 100 / 1500: loss 228.788430\n",
      "iteration 200 / 1500: loss 35.870414\n",
      "iteration 300 / 1500: loss 7.168807\n",
      "iteration 400 / 1500: loss 2.831108\n",
      "iteration 500 / 1500: loss 2.276191\n",
      "iteration 600 / 1500: loss 2.183874\n",
      "iteration 700 / 1500: loss 2.127408\n",
      "iteration 800 / 1500: loss 2.133848\n",
      "iteration 900 / 1500: loss 2.130520\n",
      "iteration 1000 / 1500: loss 2.121355\n",
      "iteration 1100 / 1500: loss 2.169233\n",
      "iteration 1200 / 1500: loss 2.114080\n",
      "iteration 1300 / 1500: loss 2.126938\n",
      "iteration 1400 / 1500: loss 2.139178\n",
      "iteration 0 / 1500: loss 1503.503611\n",
      "iteration 100 / 1500: loss 232.360791\n",
      "iteration 200 / 1500: loss 37.435155\n",
      "iteration 300 / 1500: loss 7.536250\n",
      "iteration 400 / 1500: loss 2.954775\n",
      "iteration 500 / 1500: loss 2.246949\n",
      "iteration 600 / 1500: loss 2.153647\n",
      "iteration 700 / 1500: loss 2.136789\n",
      "iteration 800 / 1500: loss 2.153856\n",
      "iteration 900 / 1500: loss 2.136372\n",
      "iteration 1000 / 1500: loss 2.168620\n",
      "iteration 1100 / 1500: loss 2.142199\n",
      "iteration 1200 / 1500: loss 2.100565\n",
      "iteration 1300 / 1500: loss 2.158489\n",
      "iteration 1400 / 1500: loss 2.136662\n",
      "iteration 0 / 1500: loss 809.839955\n",
      "iteration 100 / 1500: loss 296.287064\n",
      "iteration 200 / 1500: loss 109.442788\n",
      "iteration 300 / 1500: loss 41.364832\n",
      "iteration 400 / 1500: loss 16.380040\n",
      "iteration 500 / 1500: loss 7.278905\n",
      "iteration 600 / 1500: loss 4.044335\n",
      "iteration 700 / 1500: loss 2.732487\n",
      "iteration 800 / 1500: loss 2.367674\n",
      "iteration 900 / 1500: loss 2.175343\n",
      "iteration 1000 / 1500: loss 2.086199\n",
      "iteration 1100 / 1500: loss 2.139363\n",
      "iteration 1200 / 1500: loss 2.109395\n",
      "iteration 1300 / 1500: loss 2.091573\n",
      "iteration 1400 / 1500: loss 2.141298\n",
      "iteration 0 / 1500: loss 962.044941\n",
      "iteration 100 / 1500: loss 288.125609\n",
      "iteration 200 / 1500: loss 87.473265\n",
      "iteration 300 / 1500: loss 27.564967\n",
      "iteration 400 / 1500: loss 9.754920\n",
      "iteration 500 / 1500: loss 4.401333\n",
      "iteration 600 / 1500: loss 2.726856\n",
      "iteration 700 / 1500: loss 2.328048\n",
      "iteration 800 / 1500: loss 2.120606\n",
      "iteration 900 / 1500: loss 2.125224\n",
      "iteration 1000 / 1500: loss 2.110787\n",
      "iteration 1100 / 1500: loss 2.123686\n",
      "iteration 1200 / 1500: loss 2.072161\n",
      "iteration 1300 / 1500: loss 2.090671\n",
      "iteration 1400 / 1500: loss 2.077202\n",
      "iteration 0 / 1500: loss 1069.567043\n",
      "iteration 100 / 1500: loss 286.316778\n",
      "iteration 200 / 1500: loss 77.735102\n",
      "iteration 300 / 1500: loss 22.292347\n",
      "iteration 400 / 1500: loss 7.474532\n",
      "iteration 500 / 1500: loss 3.543770\n",
      "iteration 600 / 1500: loss 2.528317\n",
      "iteration 700 / 1500: loss 2.263801\n",
      "iteration 800 / 1500: loss 2.215641\n",
      "iteration 900 / 1500: loss 2.109096\n",
      "iteration 1000 / 1500: loss 2.131908\n",
      "iteration 1100 / 1500: loss 2.135470\n",
      "iteration 1200 / 1500: loss 2.149981\n",
      "iteration 1300 / 1500: loss 2.155703\n",
      "iteration 1400 / 1500: loss 2.092909\n",
      "iteration 0 / 1500: loss 1446.744291\n",
      "iteration 100 / 1500: loss 239.889526\n",
      "iteration 200 / 1500: loss 41.326130\n",
      "iteration 300 / 1500: loss 8.578778\n",
      "iteration 400 / 1500: loss 3.231551\n",
      "iteration 500 / 1500: loss 2.297556\n",
      "iteration 600 / 1500: loss 2.173443\n",
      "iteration 700 / 1500: loss 2.146977\n",
      "iteration 800 / 1500: loss 2.177114\n",
      "iteration 900 / 1500: loss 2.132498\n",
      "iteration 1000 / 1500: loss 2.131178\n",
      "iteration 1100 / 1500: loss 2.148130\n",
      "iteration 1200 / 1500: loss 2.155972\n",
      "iteration 1300 / 1500: loss 2.139513\n",
      "iteration 1400 / 1500: loss 2.117244\n",
      "iteration 0 / 1500: loss 473.093558\n",
      "iteration 100 / 1500: loss 264.404189\n",
      "iteration 200 / 1500: loss 148.890076\n",
      "iteration 300 / 1500: loss 84.088999\n",
      "iteration 400 / 1500: loss 47.986934\n",
      "iteration 500 / 1500: loss 27.626629\n",
      "iteration 600 / 1500: loss 16.322580\n",
      "iteration 700 / 1500: loss 10.088222\n",
      "iteration 800 / 1500: loss 6.543330\n",
      "iteration 900 / 1500: loss 4.528502\n",
      "iteration 1000 / 1500: loss 3.439738\n",
      "iteration 1100 / 1500: loss 2.837815\n",
      "iteration 1200 / 1500: loss 2.469896\n",
      "iteration 1300 / 1500: loss 2.272945\n",
      "iteration 1400 / 1500: loss 2.170526\n",
      "iteration 0 / 1500: loss 331.858148\n",
      "iteration 100 / 1500: loss 221.245987\n",
      "iteration 200 / 1500: loss 148.695343\n",
      "iteration 300 / 1500: loss 100.117255\n",
      "iteration 400 / 1500: loss 67.612462\n",
      "iteration 500 / 1500: loss 45.902090\n",
      "iteration 600 / 1500: loss 31.417588\n",
      "iteration 700 / 1500: loss 21.679473\n",
      "iteration 800 / 1500: loss 15.182085\n",
      "iteration 900 / 1500: loss 10.845750\n",
      "iteration 1000 / 1500: loss 7.892859\n",
      "iteration 1100 / 1500: loss 5.919279\n",
      "iteration 1200 / 1500: loss 4.588853\n",
      "iteration 1300 / 1500: loss 3.799384\n",
      "iteration 1400 / 1500: loss 3.168263\n",
      "iteration 0 / 1500: loss 358.321341\n",
      "iteration 100 / 1500: loss 230.044255\n",
      "iteration 200 / 1500: loss 148.662152\n",
      "iteration 300 / 1500: loss 96.426816\n",
      "iteration 400 / 1500: loss 62.664712\n",
      "iteration 500 / 1500: loss 41.122998\n",
      "iteration 600 / 1500: loss 27.209141\n",
      "iteration 700 / 1500: loss 18.181575\n",
      "iteration 800 / 1500: loss 12.468879\n",
      "iteration 900 / 1500: loss 8.736252\n",
      "iteration 1000 / 1500: loss 6.369974\n",
      "iteration 1100 / 1500: loss 4.863476\n",
      "iteration 1200 / 1500: loss 3.828745\n",
      "iteration 1300 / 1500: loss 3.166231\n",
      "iteration 1400 / 1500: loss 2.749040\n",
      "iteration 0 / 1500: loss 649.768320\n",
      "iteration 100 / 1500: loss 290.834026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 130.981088\n",
      "iteration 300 / 1500: loss 59.642662\n",
      "iteration 400 / 1500: loss 27.770825\n",
      "iteration 500 / 1500: loss 13.512063\n",
      "iteration 600 / 1500: loss 7.153061\n",
      "iteration 700 / 1500: loss 4.350810\n",
      "iteration 800 / 1500: loss 3.094670\n",
      "iteration 900 / 1500: loss 2.466410\n",
      "iteration 1000 / 1500: loss 2.273939\n",
      "iteration 1100 / 1500: loss 2.161700\n",
      "iteration 1200 / 1500: loss 2.126119\n",
      "iteration 1300 / 1500: loss 2.114654\n",
      "iteration 1400 / 1500: loss 2.114868\n",
      "iteration 0 / 1500: loss 395.937587\n",
      "iteration 100 / 1500: loss 241.944193\n",
      "iteration 200 / 1500: loss 149.096845\n",
      "iteration 300 / 1500: loss 92.103236\n",
      "iteration 400 / 1500: loss 57.153029\n",
      "iteration 500 / 1500: loss 35.796399\n",
      "iteration 600 / 1500: loss 22.613878\n",
      "iteration 700 / 1500: loss 14.765143\n",
      "iteration 800 / 1500: loss 9.768958\n",
      "iteration 900 / 1500: loss 6.780829\n",
      "iteration 1000 / 1500: loss 4.925337\n",
      "iteration 1100 / 1500: loss 3.865507\n",
      "iteration 1200 / 1500: loss 3.120700\n",
      "iteration 1300 / 1500: loss 2.742730\n",
      "iteration 1400 / 1500: loss 2.481304\n",
      "iteration 0 / 1500: loss 1101.135591\n",
      "iteration 100 / 1500: loss 285.607593\n",
      "iteration 200 / 1500: loss 75.262904\n",
      "iteration 300 / 1500: loss 21.004046\n",
      "iteration 400 / 1500: loss 6.991150\n",
      "iteration 500 / 1500: loss 3.403095\n",
      "iteration 600 / 1500: loss 2.377587\n",
      "iteration 700 / 1500: loss 2.233568\n",
      "iteration 800 / 1500: loss 2.146907\n",
      "iteration 900 / 1500: loss 2.091453\n",
      "iteration 1000 / 1500: loss 2.085136\n",
      "iteration 1100 / 1500: loss 2.151993\n",
      "iteration 1200 / 1500: loss 2.106533\n",
      "iteration 1300 / 1500: loss 2.154926\n",
      "iteration 1400 / 1500: loss 2.131420\n",
      "iteration 0 / 1500: loss 395.411489\n",
      "iteration 100 / 1500: loss 244.057748\n",
      "iteration 200 / 1500: loss 151.723325\n",
      "iteration 300 / 1500: loss 94.745071\n",
      "iteration 400 / 1500: loss 59.500299\n",
      "iteration 500 / 1500: loss 37.507763\n",
      "iteration 600 / 1500: loss 23.992344\n",
      "iteration 700 / 1500: loss 15.670463\n",
      "iteration 800 / 1500: loss 10.483793\n",
      "iteration 900 / 1500: loss 7.280604\n",
      "iteration 1000 / 1500: loss 5.266105\n",
      "iteration 1100 / 1500: loss 4.008080\n",
      "iteration 1200 / 1500: loss 3.227304\n",
      "iteration 1300 / 1500: loss 2.762284\n",
      "iteration 1400 / 1500: loss 2.472018\n",
      "iteration 0 / 1500: loss 1235.640692\n",
      "iteration 100 / 1500: loss 262.083036\n",
      "iteration 200 / 1500: loss 56.989090\n",
      "iteration 300 / 1500: loss 13.657386\n",
      "iteration 400 / 1500: loss 4.530292\n",
      "iteration 500 / 1500: loss 2.682718\n",
      "iteration 600 / 1500: loss 2.242606\n",
      "iteration 700 / 1500: loss 2.191948\n",
      "iteration 800 / 1500: loss 2.133986\n",
      "iteration 900 / 1500: loss 2.110801\n",
      "iteration 1000 / 1500: loss 2.101554\n",
      "iteration 1100 / 1500: loss 2.093261\n",
      "iteration 1200 / 1500: loss 2.099942\n",
      "iteration 1300 / 1500: loss 2.129131\n",
      "iteration 1400 / 1500: loss 2.149337\n",
      "iteration 0 / 1500: loss 1202.055361\n",
      "iteration 100 / 1500: loss 271.072801\n",
      "iteration 200 / 1500: loss 62.388219\n",
      "iteration 300 / 1500: loss 15.613515\n",
      "iteration 400 / 1500: loss 5.117583\n",
      "iteration 500 / 1500: loss 2.849489\n",
      "iteration 600 / 1500: loss 2.286395\n",
      "iteration 700 / 1500: loss 2.201776\n",
      "iteration 800 / 1500: loss 2.125974\n",
      "iteration 900 / 1500: loss 2.117785\n",
      "iteration 1000 / 1500: loss 2.120596\n",
      "iteration 1100 / 1500: loss 2.114705\n",
      "iteration 1200 / 1500: loss 2.089029\n",
      "iteration 1300 / 1500: loss 2.113988\n",
      "iteration 1400 / 1500: loss 2.146604\n",
      "iteration 0 / 1500: loss 390.129062\n",
      "iteration 100 / 1500: loss 241.831890\n",
      "iteration 200 / 1500: loss 150.766733\n",
      "iteration 300 / 1500: loss 94.386281\n",
      "iteration 400 / 1500: loss 59.365731\n",
      "iteration 500 / 1500: loss 37.567951\n",
      "iteration 600 / 1500: loss 24.127963\n",
      "iteration 700 / 1500: loss 15.733622\n",
      "iteration 800 / 1500: loss 10.539580\n",
      "iteration 900 / 1500: loss 7.288367\n",
      "iteration 1000 / 1500: loss 5.357275\n",
      "iteration 1100 / 1500: loss 4.073669\n",
      "iteration 1200 / 1500: loss 3.293075\n",
      "iteration 1300 / 1500: loss 2.789993\n",
      "iteration 1400 / 1500: loss 2.528406\n",
      "iteration 0 / 1500: loss 1280.628831\n",
      "iteration 100 / 1500: loss 264.488563\n",
      "iteration 200 / 1500: loss 55.955188\n",
      "iteration 300 / 1500: loss 13.222355\n",
      "iteration 400 / 1500: loss 4.350481\n",
      "iteration 500 / 1500: loss 2.582371\n",
      "iteration 600 / 1500: loss 2.167149\n",
      "iteration 700 / 1500: loss 2.209751\n",
      "iteration 800 / 1500: loss 2.106963\n",
      "iteration 900 / 1500: loss 2.117114\n",
      "iteration 1000 / 1500: loss 2.107267\n",
      "iteration 1100 / 1500: loss 2.176577\n",
      "iteration 1200 / 1500: loss 2.167030\n",
      "iteration 1300 / 1500: loss 2.105558\n",
      "iteration 1400 / 1500: loss 2.115116\n",
      "iteration 0 / 1500: loss 466.014614\n",
      "iteration 100 / 1500: loss 74.577681\n",
      "iteration 200 / 1500: loss 13.450636\n",
      "iteration 300 / 1500: loss 3.837690\n",
      "iteration 400 / 1500: loss 2.374264\n",
      "iteration 500 / 1500: loss 2.109380\n",
      "iteration 600 / 1500: loss 2.022693\n",
      "iteration 700 / 1500: loss 1.946782\n",
      "iteration 800 / 1500: loss 2.045655\n",
      "iteration 900 / 1500: loss 1.991259\n",
      "iteration 1000 / 1500: loss 1.998278\n",
      "iteration 1100 / 1500: loss 2.041262\n",
      "iteration 1200 / 1500: loss 1.989532\n",
      "iteration 1300 / 1500: loss 1.999846\n",
      "iteration 1400 / 1500: loss 2.061880\n",
      "iteration 0 / 1500: loss 417.495912\n",
      "iteration 100 / 1500: loss 79.883457\n",
      "iteration 200 / 1500: loss 16.680444\n",
      "iteration 300 / 1500: loss 4.907135\n",
      "iteration 400 / 1500: loss 2.539684\n",
      "iteration 500 / 1500: loss 2.098062\n",
      "iteration 600 / 1500: loss 2.005741\n",
      "iteration 700 / 1500: loss 2.018527\n",
      "iteration 800 / 1500: loss 2.053315\n",
      "iteration 900 / 1500: loss 2.039116\n",
      "iteration 1000 / 1500: loss 1.988753\n",
      "iteration 1100 / 1500: loss 2.078499\n",
      "iteration 1200 / 1500: loss 2.091999\n",
      "iteration 1300 / 1500: loss 2.036723\n",
      "iteration 1400 / 1500: loss 2.055793\n",
      "iteration 0 / 1500: loss 553.462941\n",
      "iteration 100 / 1500: loss 61.357318\n",
      "iteration 200 / 1500: loss 8.439362\n",
      "iteration 300 / 1500: loss 2.773133\n",
      "iteration 400 / 1500: loss 2.095774\n",
      "iteration 500 / 1500: loss 2.097340\n",
      "iteration 600 / 1500: loss 2.082921\n",
      "iteration 700 / 1500: loss 2.042269\n",
      "iteration 800 / 1500: loss 2.045826\n",
      "iteration 900 / 1500: loss 2.063245\n",
      "iteration 1000 / 1500: loss 2.071592\n",
      "iteration 1100 / 1500: loss 2.104595\n",
      "iteration 1200 / 1500: loss 2.107067\n",
      "iteration 1300 / 1500: loss 2.069882\n",
      "iteration 1400 / 1500: loss 1.995787\n",
      "iteration 0 / 1500: loss 1521.919129\n",
      "iteration 100 / 1500: loss 5.107747\n",
      "iteration 200 / 1500: loss 2.156018\n",
      "iteration 300 / 1500: loss 2.148673\n",
      "iteration 400 / 1500: loss 2.159323\n",
      "iteration 500 / 1500: loss 2.123100\n",
      "iteration 600 / 1500: loss 2.168193\n",
      "iteration 700 / 1500: loss 2.088830\n",
      "iteration 800 / 1500: loss 2.134727\n",
      "iteration 900 / 1500: loss 2.148027\n",
      "iteration 1000 / 1500: loss 2.147124\n",
      "iteration 1100 / 1500: loss 2.127688\n",
      "iteration 1200 / 1500: loss 2.156350\n",
      "iteration 1300 / 1500: loss 2.139520\n",
      "iteration 1400 / 1500: loss 2.165974\n",
      "iteration 0 / 1500: loss 1499.061784\n",
      "iteration 100 / 1500: loss 5.333084\n",
      "iteration 200 / 1500: loss 2.153935\n",
      "iteration 300 / 1500: loss 2.161652\n",
      "iteration 400 / 1500: loss 2.164650\n",
      "iteration 500 / 1500: loss 2.150062\n",
      "iteration 600 / 1500: loss 2.131488\n",
      "iteration 700 / 1500: loss 2.138568\n",
      "iteration 800 / 1500: loss 2.185288\n",
      "iteration 900 / 1500: loss 2.140967\n",
      "iteration 1000 / 1500: loss 2.132109\n",
      "iteration 1100 / 1500: loss 2.160628\n",
      "iteration 1200 / 1500: loss 2.167406\n",
      "iteration 1300 / 1500: loss 2.128637\n",
      "iteration 1400 / 1500: loss 2.155925\n",
      "iteration 0 / 1500: loss 817.909865\n",
      "iteration 100 / 1500: loss 32.489889\n",
      "iteration 200 / 1500: loss 3.227460\n",
      "iteration 300 / 1500: loss 2.193803\n",
      "iteration 400 / 1500: loss 2.142353\n",
      "iteration 500 / 1500: loss 2.137132\n",
      "iteration 600 / 1500: loss 2.093589\n",
      "iteration 700 / 1500: loss 2.128808\n",
      "iteration 800 / 1500: loss 2.129747\n",
      "iteration 900 / 1500: loss 2.113686\n",
      "iteration 1000 / 1500: loss 2.117392\n",
      "iteration 1100 / 1500: loss 2.084602\n",
      "iteration 1200 / 1500: loss 2.044796\n",
      "iteration 1300 / 1500: loss 2.076349\n",
      "iteration 1400 / 1500: loss 2.167910\n",
      "iteration 0 / 1500: loss 975.545893\n",
      "iteration 100 / 1500: loss 20.856287\n",
      "iteration 200 / 1500: loss 2.441225\n",
      "iteration 300 / 1500: loss 2.117396\n",
      "iteration 400 / 1500: loss 2.130413\n",
      "iteration 500 / 1500: loss 2.099791\n",
      "iteration 600 / 1500: loss 2.133124\n",
      "iteration 700 / 1500: loss 2.037116\n",
      "iteration 800 / 1500: loss 2.097355\n",
      "iteration 900 / 1500: loss 2.138129\n",
      "iteration 1000 / 1500: loss 2.103302\n",
      "iteration 1100 / 1500: loss 2.073226\n",
      "iteration 1200 / 1500: loss 2.099113\n",
      "iteration 1300 / 1500: loss 2.099927\n",
      "iteration 1400 / 1500: loss 2.073319\n",
      "iteration 0 / 1500: loss 1067.045406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 16.175887\n",
      "iteration 200 / 1500: loss 2.298504\n",
      "iteration 300 / 1500: loss 2.151042\n",
      "iteration 400 / 1500: loss 2.151735\n",
      "iteration 500 / 1500: loss 2.090309\n",
      "iteration 600 / 1500: loss 2.131933\n",
      "iteration 700 / 1500: loss 2.113816\n",
      "iteration 800 / 1500: loss 2.110133\n",
      "iteration 900 / 1500: loss 2.164281\n",
      "iteration 1000 / 1500: loss 2.158656\n",
      "iteration 1100 / 1500: loss 2.135184\n",
      "iteration 1200 / 1500: loss 2.121179\n",
      "iteration 1300 / 1500: loss 2.167724\n",
      "iteration 1400 / 1500: loss 2.124832\n",
      "iteration 0 / 1500: loss 1423.695596\n",
      "iteration 100 / 1500: loss 5.974199\n",
      "iteration 200 / 1500: loss 2.151702\n",
      "iteration 300 / 1500: loss 2.168613\n",
      "iteration 400 / 1500: loss 2.111300\n",
      "iteration 500 / 1500: loss 2.120329\n",
      "iteration 600 / 1500: loss 2.135997\n",
      "iteration 700 / 1500: loss 2.145056\n",
      "iteration 800 / 1500: loss 2.150514\n",
      "iteration 900 / 1500: loss 2.133779\n",
      "iteration 1000 / 1500: loss 2.097898\n",
      "iteration 1100 / 1500: loss 2.187714\n",
      "iteration 1200 / 1500: loss 2.154525\n",
      "iteration 1300 / 1500: loss 2.130397\n",
      "iteration 1400 / 1500: loss 2.131887\n",
      "iteration 0 / 1500: loss 467.663781\n",
      "iteration 100 / 1500: loss 71.887890\n",
      "iteration 200 / 1500: loss 12.630100\n",
      "iteration 300 / 1500: loss 3.621717\n",
      "iteration 400 / 1500: loss 2.253276\n",
      "iteration 500 / 1500: loss 2.128420\n",
      "iteration 600 / 1500: loss 2.029985\n",
      "iteration 700 / 1500: loss 2.084200\n",
      "iteration 800 / 1500: loss 2.036554\n",
      "iteration 900 / 1500: loss 2.041432\n",
      "iteration 1000 / 1500: loss 2.074921\n",
      "iteration 1100 / 1500: loss 2.044251\n",
      "iteration 1200 / 1500: loss 2.087567\n",
      "iteration 1300 / 1500: loss 2.095026\n",
      "iteration 1400 / 1500: loss 2.098364\n",
      "iteration 0 / 1500: loss 323.389644\n",
      "iteration 100 / 1500: loss 88.796612\n",
      "iteration 200 / 1500: loss 25.554564\n",
      "iteration 300 / 1500: loss 8.402431\n",
      "iteration 400 / 1500: loss 3.681768\n",
      "iteration 500 / 1500: loss 2.473187\n",
      "iteration 600 / 1500: loss 2.144649\n",
      "iteration 700 / 1500: loss 2.010591\n",
      "iteration 800 / 1500: loss 2.036483\n",
      "iteration 900 / 1500: loss 2.061869\n",
      "iteration 1000 / 1500: loss 2.017584\n",
      "iteration 1100 / 1500: loss 2.044448\n",
      "iteration 1200 / 1500: loss 2.002939\n",
      "iteration 1300 / 1500: loss 2.005071\n",
      "iteration 1400 / 1500: loss 1.946344\n",
      "iteration 0 / 1500: loss 357.826884\n",
      "iteration 100 / 1500: loss 86.501570\n",
      "iteration 200 / 1500: loss 22.156876\n",
      "iteration 300 / 1500: loss 6.874478\n",
      "iteration 400 / 1500: loss 3.228675\n",
      "iteration 500 / 1500: loss 2.271355\n",
      "iteration 600 / 1500: loss 2.124666\n",
      "iteration 700 / 1500: loss 2.168512\n",
      "iteration 800 / 1500: loss 1.991464\n",
      "iteration 900 / 1500: loss 1.984669\n",
      "iteration 1000 / 1500: loss 2.033350\n",
      "iteration 1100 / 1500: loss 2.081507\n",
      "iteration 1200 / 1500: loss 2.037542\n",
      "iteration 1300 / 1500: loss 1.980796\n",
      "iteration 1400 / 1500: loss 2.019107\n",
      "iteration 0 / 1500: loss 649.065365\n",
      "iteration 100 / 1500: loss 48.726252\n",
      "iteration 200 / 1500: loss 5.443257\n",
      "iteration 300 / 1500: loss 2.316640\n",
      "iteration 400 / 1500: loss 2.093150\n",
      "iteration 500 / 1500: loss 2.094750\n",
      "iteration 600 / 1500: loss 2.118037\n",
      "iteration 700 / 1500: loss 2.137197\n",
      "iteration 800 / 1500: loss 2.099615\n",
      "iteration 900 / 1500: loss 2.070968\n",
      "iteration 1000 / 1500: loss 2.040946\n",
      "iteration 1100 / 1500: loss 2.000840\n",
      "iteration 1200 / 1500: loss 2.116036\n",
      "iteration 1300 / 1500: loss 2.112856\n",
      "iteration 1400 / 1500: loss 2.068798\n",
      "iteration 0 / 1500: loss 398.184859\n",
      "iteration 100 / 1500: loss 82.090115\n",
      "iteration 200 / 1500: loss 18.334670\n",
      "iteration 300 / 1500: loss 5.351504\n",
      "iteration 400 / 1500: loss 2.700842\n",
      "iteration 500 / 1500: loss 2.216632\n",
      "iteration 600 / 1500: loss 2.016975\n",
      "iteration 700 / 1500: loss 1.985955\n",
      "iteration 800 / 1500: loss 2.079573\n",
      "iteration 900 / 1500: loss 1.946252\n",
      "iteration 1000 / 1500: loss 2.057661\n",
      "iteration 1100 / 1500: loss 2.030197\n",
      "iteration 1200 / 1500: loss 2.084176\n",
      "iteration 1300 / 1500: loss 2.041665\n",
      "iteration 1400 / 1500: loss 2.108789\n",
      "iteration 0 / 1500: loss 1081.420134\n",
      "iteration 100 / 1500: loss 14.985714\n",
      "iteration 200 / 1500: loss 2.247136\n",
      "iteration 300 / 1500: loss 2.161788\n",
      "iteration 400 / 1500: loss 2.130283\n",
      "iteration 500 / 1500: loss 2.117128\n",
      "iteration 600 / 1500: loss 2.081955\n",
      "iteration 700 / 1500: loss 2.091113\n",
      "iteration 800 / 1500: loss 2.123409\n",
      "iteration 900 / 1500: loss 2.094835\n",
      "iteration 1000 / 1500: loss 2.141840\n",
      "iteration 1100 / 1500: loss 2.122672\n",
      "iteration 1200 / 1500: loss 2.116411\n",
      "iteration 1300 / 1500: loss 2.078503\n",
      "iteration 1400 / 1500: loss 2.062851\n",
      "iteration 0 / 1500: loss 390.674306\n",
      "iteration 100 / 1500: loss 83.168387\n",
      "iteration 200 / 1500: loss 19.119096\n",
      "iteration 300 / 1500: loss 5.622262\n",
      "iteration 400 / 1500: loss 2.768173\n",
      "iteration 500 / 1500: loss 2.219945\n",
      "iteration 600 / 1500: loss 2.120372\n",
      "iteration 700 / 1500: loss 2.062257\n",
      "iteration 800 / 1500: loss 1.985173\n",
      "iteration 900 / 1500: loss 2.042705\n",
      "iteration 1000 / 1500: loss 1.982901\n",
      "iteration 1100 / 1500: loss 2.045495\n",
      "iteration 1200 / 1500: loss 1.977878\n",
      "iteration 1300 / 1500: loss 2.061808\n",
      "iteration 1400 / 1500: loss 2.004971\n",
      "iteration 0 / 1500: loss 1267.309912\n",
      "iteration 100 / 1500: loss 9.903545\n",
      "iteration 200 / 1500: loss 2.132699\n",
      "iteration 300 / 1500: loss 2.061836\n",
      "iteration 400 / 1500: loss 2.105296\n",
      "iteration 500 / 1500: loss 2.099691\n",
      "iteration 600 / 1500: loss 2.126833\n",
      "iteration 700 / 1500: loss 2.084053\n",
      "iteration 800 / 1500: loss 2.143906\n",
      "iteration 900 / 1500: loss 2.142855\n",
      "iteration 1000 / 1500: loss 2.158774\n",
      "iteration 1100 / 1500: loss 2.086070\n",
      "iteration 1200 / 1500: loss 2.111107\n",
      "iteration 1300 / 1500: loss 2.157695\n",
      "iteration 1400 / 1500: loss 2.124446\n",
      "iteration 0 / 1500: loss 1214.533615\n",
      "iteration 100 / 1500: loss 11.258178\n",
      "iteration 200 / 1500: loss 2.173958\n",
      "iteration 300 / 1500: loss 2.149877\n",
      "iteration 400 / 1500: loss 2.129911\n",
      "iteration 500 / 1500: loss 2.103968\n",
      "iteration 600 / 1500: loss 2.185211\n",
      "iteration 700 / 1500: loss 2.100462\n",
      "iteration 800 / 1500: loss 2.115783\n",
      "iteration 900 / 1500: loss 2.109959\n",
      "iteration 1000 / 1500: loss 2.115801\n",
      "iteration 1100 / 1500: loss 2.153940\n",
      "iteration 1200 / 1500: loss 2.119193\n",
      "iteration 1300 / 1500: loss 2.130051\n",
      "iteration 1400 / 1500: loss 2.108278\n",
      "iteration 0 / 1500: loss 389.619996\n",
      "iteration 100 / 1500: loss 83.971675\n",
      "iteration 200 / 1500: loss 19.346401\n",
      "iteration 300 / 1500: loss 5.686159\n",
      "iteration 400 / 1500: loss 2.813728\n",
      "iteration 500 / 1500: loss 2.195004\n",
      "iteration 600 / 1500: loss 2.036834\n",
      "iteration 700 / 1500: loss 2.049636\n",
      "iteration 800 / 1500: loss 2.059019\n",
      "iteration 900 / 1500: loss 2.064300\n",
      "iteration 1000 / 1500: loss 2.010951\n",
      "iteration 1100 / 1500: loss 2.050687\n",
      "iteration 1200 / 1500: loss 2.014367\n",
      "iteration 1300 / 1500: loss 2.041388\n",
      "iteration 1400 / 1500: loss 1.997567\n",
      "iteration 0 / 1500: loss 1248.708672\n",
      "iteration 100 / 1500: loss 9.070561\n",
      "iteration 200 / 1500: loss 2.127890\n",
      "iteration 300 / 1500: loss 2.132680\n",
      "iteration 400 / 1500: loss 2.128655\n",
      "iteration 500 / 1500: loss 2.093090\n",
      "iteration 600 / 1500: loss 2.158178\n",
      "iteration 700 / 1500: loss 2.110165\n",
      "iteration 800 / 1500: loss 2.110629\n",
      "iteration 900 / 1500: loss 2.119500\n",
      "iteration 1000 / 1500: loss 2.062739\n",
      "iteration 1100 / 1500: loss 2.115861\n",
      "iteration 1200 / 1500: loss 2.125112\n",
      "iteration 1300 / 1500: loss 2.111991\n",
      "iteration 1400 / 1500: loss 2.105288\n",
      "iteration 0 / 1500: loss 455.481603\n",
      "iteration 100 / 1500: loss 93.617523\n",
      "iteration 200 / 1500: loss 20.683237\n",
      "iteration 300 / 1500: loss 5.918763\n",
      "iteration 400 / 1500: loss 2.839428\n",
      "iteration 500 / 1500: loss 2.333141\n",
      "iteration 600 / 1500: loss 2.096230\n",
      "iteration 700 / 1500: loss 1.998949\n",
      "iteration 800 / 1500: loss 2.001726\n",
      "iteration 900 / 1500: loss 1.994347\n",
      "iteration 1000 / 1500: loss 2.036613\n",
      "iteration 1100 / 1500: loss 2.041981\n",
      "iteration 1200 / 1500: loss 2.058300\n",
      "iteration 1300 / 1500: loss 1.990335\n",
      "iteration 1400 / 1500: loss 2.080374\n",
      "iteration 0 / 1500: loss 416.951768\n",
      "iteration 100 / 1500: loss 100.234319\n",
      "iteration 200 / 1500: loss 25.362669\n",
      "iteration 300 / 1500: loss 7.558451\n",
      "iteration 400 / 1500: loss 3.378359\n",
      "iteration 500 / 1500: loss 2.290778\n",
      "iteration 600 / 1500: loss 2.117426\n",
      "iteration 700 / 1500: loss 2.046450\n",
      "iteration 800 / 1500: loss 1.998416\n",
      "iteration 900 / 1500: loss 2.051444\n",
      "iteration 1000 / 1500: loss 1.985314\n",
      "iteration 1100 / 1500: loss 2.062360\n",
      "iteration 1200 / 1500: loss 2.066686\n",
      "iteration 1300 / 1500: loss 2.022782\n",
      "iteration 1400 / 1500: loss 2.052418\n",
      "iteration 0 / 1500: loss 559.617867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 83.919347\n",
      "iteration 200 / 1500: loss 14.143748\n",
      "iteration 300 / 1500: loss 3.876149\n",
      "iteration 400 / 1500: loss 2.353334\n",
      "iteration 500 / 1500: loss 2.101563\n",
      "iteration 600 / 1500: loss 2.131076\n",
      "iteration 700 / 1500: loss 2.068555\n",
      "iteration 800 / 1500: loss 2.088147\n",
      "iteration 900 / 1500: loss 1.983060\n",
      "iteration 1000 / 1500: loss 2.110518\n",
      "iteration 1100 / 1500: loss 2.061377\n",
      "iteration 1200 / 1500: loss 2.098740\n",
      "iteration 1300 / 1500: loss 1.996727\n",
      "iteration 1400 / 1500: loss 2.042201\n",
      "iteration 0 / 1500: loss 1548.936613\n",
      "iteration 100 / 1500: loss 9.372990\n",
      "iteration 200 / 1500: loss 2.142782\n",
      "iteration 300 / 1500: loss 2.126133\n",
      "iteration 400 / 1500: loss 2.141123\n",
      "iteration 500 / 1500: loss 2.171764\n",
      "iteration 600 / 1500: loss 2.165656\n",
      "iteration 700 / 1500: loss 2.168773\n",
      "iteration 800 / 1500: loss 2.150603\n",
      "iteration 900 / 1500: loss 2.154082\n",
      "iteration 1000 / 1500: loss 2.093348\n",
      "iteration 1100 / 1500: loss 2.127511\n",
      "iteration 1200 / 1500: loss 2.170603\n",
      "iteration 1300 / 1500: loss 2.118626\n",
      "iteration 1400 / 1500: loss 2.119356\n",
      "iteration 0 / 1500: loss 1509.573278\n",
      "iteration 100 / 1500: loss 9.823005\n",
      "iteration 200 / 1500: loss 2.220573\n",
      "iteration 300 / 1500: loss 2.122441\n",
      "iteration 400 / 1500: loss 2.158810\n",
      "iteration 500 / 1500: loss 2.179674\n",
      "iteration 600 / 1500: loss 2.134741\n",
      "iteration 700 / 1500: loss 2.190527\n",
      "iteration 800 / 1500: loss 2.184310\n",
      "iteration 900 / 1500: loss 2.089351\n",
      "iteration 1000 / 1500: loss 2.197894\n",
      "iteration 1100 / 1500: loss 2.106724\n",
      "iteration 1200 / 1500: loss 2.172901\n",
      "iteration 1300 / 1500: loss 2.132778\n",
      "iteration 1400 / 1500: loss 2.142188\n",
      "iteration 0 / 1500: loss 799.141190\n",
      "iteration 100 / 1500: loss 49.271965\n",
      "iteration 200 / 1500: loss 4.923500\n",
      "iteration 300 / 1500: loss 2.282809\n",
      "iteration 400 / 1500: loss 2.105102\n",
      "iteration 500 / 1500: loss 2.064437\n",
      "iteration 600 / 1500: loss 2.089333\n",
      "iteration 700 / 1500: loss 2.082693\n",
      "iteration 800 / 1500: loss 2.113890\n",
      "iteration 900 / 1500: loss 2.071282\n",
      "iteration 1000 / 1500: loss 2.091713\n",
      "iteration 1100 / 1500: loss 2.027797\n",
      "iteration 1200 / 1500: loss 2.086294\n",
      "iteration 1300 / 1500: loss 2.084391\n",
      "iteration 1400 / 1500: loss 2.064055\n",
      "iteration 0 / 1500: loss 975.408742\n",
      "iteration 100 / 1500: loss 34.795210\n",
      "iteration 200 / 1500: loss 3.163473\n",
      "iteration 300 / 1500: loss 2.161110\n",
      "iteration 400 / 1500: loss 2.070448\n",
      "iteration 500 / 1500: loss 2.072318\n",
      "iteration 600 / 1500: loss 2.123712\n",
      "iteration 700 / 1500: loss 2.073212\n",
      "iteration 800 / 1500: loss 2.039870\n",
      "iteration 900 / 1500: loss 2.090691\n",
      "iteration 1000 / 1500: loss 2.092488\n",
      "iteration 1100 / 1500: loss 2.079913\n",
      "iteration 1200 / 1500: loss 2.082294\n",
      "iteration 1300 / 1500: loss 2.077918\n",
      "iteration 1400 / 1500: loss 2.096184\n",
      "iteration 0 / 1500: loss 1070.856569\n",
      "iteration 100 / 1500: loss 28.169144\n",
      "iteration 200 / 1500: loss 2.714182\n",
      "iteration 300 / 1500: loss 2.118445\n",
      "iteration 400 / 1500: loss 2.119202\n",
      "iteration 500 / 1500: loss 2.114152\n",
      "iteration 600 / 1500: loss 2.168304\n",
      "iteration 700 / 1500: loss 2.112254\n",
      "iteration 800 / 1500: loss 2.103359\n",
      "iteration 900 / 1500: loss 2.143303\n",
      "iteration 1000 / 1500: loss 2.105107\n",
      "iteration 1100 / 1500: loss 2.106191\n",
      "iteration 1200 / 1500: loss 2.094425\n",
      "iteration 1300 / 1500: loss 2.162026\n",
      "iteration 1400 / 1500: loss 2.080424\n",
      "iteration 0 / 1500: loss 1431.686866\n",
      "iteration 100 / 1500: loss 11.007988\n",
      "iteration 200 / 1500: loss 2.203679\n",
      "iteration 300 / 1500: loss 2.174182\n",
      "iteration 400 / 1500: loss 2.136213\n",
      "iteration 500 / 1500: loss 2.103101\n",
      "iteration 600 / 1500: loss 2.117668\n",
      "iteration 700 / 1500: loss 2.131451\n",
      "iteration 800 / 1500: loss 2.102812\n",
      "iteration 900 / 1500: loss 2.115903\n",
      "iteration 1000 / 1500: loss 2.138636\n",
      "iteration 1100 / 1500: loss 2.139720\n",
      "iteration 1200 / 1500: loss 2.092931\n",
      "iteration 1300 / 1500: loss 2.164663\n",
      "iteration 1400 / 1500: loss 2.153541\n",
      "iteration 0 / 1500: loss 465.323488\n",
      "iteration 100 / 1500: loss 92.565747\n",
      "iteration 200 / 1500: loss 19.806618\n",
      "iteration 300 / 1500: loss 5.538393\n",
      "iteration 400 / 1500: loss 2.760828\n",
      "iteration 500 / 1500: loss 2.197704\n",
      "iteration 600 / 1500: loss 2.037619\n",
      "iteration 700 / 1500: loss 2.040186\n",
      "iteration 800 / 1500: loss 1.970146\n",
      "iteration 900 / 1500: loss 2.013362\n",
      "iteration 1000 / 1500: loss 1.994301\n",
      "iteration 1100 / 1500: loss 2.070862\n",
      "iteration 1200 / 1500: loss 2.075904\n",
      "iteration 1300 / 1500: loss 2.020024\n",
      "iteration 1400 / 1500: loss 2.041385\n",
      "iteration 0 / 1500: loss 330.339009\n",
      "iteration 100 / 1500: loss 107.726408\n",
      "iteration 200 / 1500: loss 36.369784\n",
      "iteration 300 / 1500: loss 13.202251\n",
      "iteration 400 / 1500: loss 5.619773\n",
      "iteration 500 / 1500: loss 3.118578\n",
      "iteration 600 / 1500: loss 2.411994\n",
      "iteration 700 / 1500: loss 2.060822\n",
      "iteration 800 / 1500: loss 2.004640\n",
      "iteration 900 / 1500: loss 2.037668\n",
      "iteration 1000 / 1500: loss 1.989670\n",
      "iteration 1100 / 1500: loss 2.052965\n",
      "iteration 1200 / 1500: loss 2.021532\n",
      "iteration 1300 / 1500: loss 1.993838\n",
      "iteration 1400 / 1500: loss 2.039644\n",
      "iteration 0 / 1500: loss 357.979357\n",
      "iteration 100 / 1500: loss 105.194871\n",
      "iteration 200 / 1500: loss 32.042460\n",
      "iteration 300 / 1500: loss 10.794325\n",
      "iteration 400 / 1500: loss 4.533184\n",
      "iteration 500 / 1500: loss 2.727238\n",
      "iteration 600 / 1500: loss 2.192641\n",
      "iteration 700 / 1500: loss 2.161781\n",
      "iteration 800 / 1500: loss 2.031473\n",
      "iteration 900 / 1500: loss 2.087006\n",
      "iteration 1000 / 1500: loss 2.022289\n",
      "iteration 1100 / 1500: loss 1.944869\n",
      "iteration 1200 / 1500: loss 2.066843\n",
      "iteration 1300 / 1500: loss 2.019842\n",
      "iteration 1400 / 1500: loss 2.069352\n",
      "iteration 0 / 1500: loss 640.195912\n",
      "iteration 100 / 1500: loss 68.596053\n",
      "iteration 200 / 1500: loss 9.039730\n",
      "iteration 300 / 1500: loss 2.778444\n",
      "iteration 400 / 1500: loss 2.153822\n",
      "iteration 500 / 1500: loss 2.111884\n",
      "iteration 600 / 1500: loss 2.044252\n",
      "iteration 700 / 1500: loss 2.010127\n",
      "iteration 800 / 1500: loss 2.109575\n",
      "iteration 900 / 1500: loss 2.073249\n",
      "iteration 1000 / 1500: loss 2.102180\n",
      "iteration 1100 / 1500: loss 2.045865\n",
      "iteration 1200 / 1500: loss 2.072964\n",
      "iteration 1300 / 1500: loss 2.093076\n",
      "iteration 1400 / 1500: loss 2.029577\n",
      "iteration 0 / 1500: loss 397.845043\n",
      "iteration 100 / 1500: loss 101.972885\n",
      "iteration 200 / 1500: loss 27.396363\n",
      "iteration 300 / 1500: loss 8.447359\n",
      "iteration 400 / 1500: loss 3.644094\n",
      "iteration 500 / 1500: loss 2.386161\n",
      "iteration 600 / 1500: loss 2.164755\n",
      "iteration 700 / 1500: loss 2.020885\n",
      "iteration 800 / 1500: loss 2.059614\n",
      "iteration 900 / 1500: loss 2.018907\n",
      "iteration 1000 / 1500: loss 2.009263\n",
      "iteration 1100 / 1500: loss 2.040317\n",
      "iteration 1200 / 1500: loss 2.052422\n",
      "iteration 1300 / 1500: loss 2.016155\n",
      "iteration 1400 / 1500: loss 2.000998\n",
      "iteration 0 / 1500: loss 1084.376637\n",
      "iteration 100 / 1500: loss 26.112936\n",
      "iteration 200 / 1500: loss 2.648076\n",
      "iteration 300 / 1500: loss 2.120482\n",
      "iteration 400 / 1500: loss 2.116212\n",
      "iteration 500 / 1500: loss 2.113827\n",
      "iteration 600 / 1500: loss 2.135085\n",
      "iteration 700 / 1500: loss 2.100816\n",
      "iteration 800 / 1500: loss 2.108784\n",
      "iteration 900 / 1500: loss 2.113025\n",
      "iteration 1000 / 1500: loss 2.093349\n",
      "iteration 1100 / 1500: loss 2.138501\n",
      "iteration 1200 / 1500: loss 2.091486\n",
      "iteration 1300 / 1500: loss 2.121126\n",
      "iteration 1400 / 1500: loss 2.109241\n",
      "iteration 0 / 1500: loss 388.197145\n",
      "iteration 100 / 1500: loss 102.833866\n",
      "iteration 200 / 1500: loss 28.279851\n",
      "iteration 300 / 1500: loss 8.901973\n",
      "iteration 400 / 1500: loss 3.840734\n",
      "iteration 500 / 1500: loss 2.519373\n",
      "iteration 600 / 1500: loss 2.128833\n",
      "iteration 700 / 1500: loss 1.979798\n",
      "iteration 800 / 1500: loss 1.991768\n",
      "iteration 900 / 1500: loss 2.030196\n",
      "iteration 1000 / 1500: loss 1.975256\n",
      "iteration 1100 / 1500: loss 2.050939\n",
      "iteration 1200 / 1500: loss 2.054410\n",
      "iteration 1300 / 1500: loss 2.030365\n",
      "iteration 1400 / 1500: loss 2.004033\n",
      "iteration 0 / 1500: loss 1251.016244\n",
      "iteration 100 / 1500: loss 17.833593\n",
      "iteration 200 / 1500: loss 2.375370\n",
      "iteration 300 / 1500: loss 2.119068\n",
      "iteration 400 / 1500: loss 2.098388\n",
      "iteration 500 / 1500: loss 2.138286\n",
      "iteration 600 / 1500: loss 2.113549\n",
      "iteration 700 / 1500: loss 2.172432\n",
      "iteration 800 / 1500: loss 2.139983\n",
      "iteration 900 / 1500: loss 2.165024\n",
      "iteration 1000 / 1500: loss 2.130968\n",
      "iteration 1100 / 1500: loss 2.166179\n",
      "iteration 1200 / 1500: loss 2.163802\n",
      "iteration 1300 / 1500: loss 2.142012\n",
      "iteration 1400 / 1500: loss 2.097865\n",
      "iteration 0 / 1500: loss 1198.938938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 20.052006\n",
      "iteration 200 / 1500: loss 2.386911\n",
      "iteration 300 / 1500: loss 2.118068\n",
      "iteration 400 / 1500: loss 2.063266\n",
      "iteration 500 / 1500: loss 2.128723\n",
      "iteration 600 / 1500: loss 2.113038\n",
      "iteration 700 / 1500: loss 2.153189\n",
      "iteration 800 / 1500: loss 2.155609\n",
      "iteration 900 / 1500: loss 2.101398\n",
      "iteration 1000 / 1500: loss 2.141056\n",
      "iteration 1100 / 1500: loss 2.102669\n",
      "iteration 1200 / 1500: loss 2.095486\n",
      "iteration 1300 / 1500: loss 2.077499\n",
      "iteration 1400 / 1500: loss 2.170111\n",
      "iteration 0 / 1500: loss 387.888100\n",
      "iteration 100 / 1500: loss 103.389074\n",
      "iteration 200 / 1500: loss 28.722569\n",
      "iteration 300 / 1500: loss 9.105730\n",
      "iteration 400 / 1500: loss 3.923772\n",
      "iteration 500 / 1500: loss 2.530527\n",
      "iteration 600 / 1500: loss 2.177434\n",
      "iteration 700 / 1500: loss 2.154729\n",
      "iteration 800 / 1500: loss 2.069791\n",
      "iteration 900 / 1500: loss 2.116241\n",
      "iteration 1000 / 1500: loss 2.037463\n",
      "iteration 1100 / 1500: loss 1.977201\n",
      "iteration 1200 / 1500: loss 2.006047\n",
      "iteration 1300 / 1500: loss 2.074103\n",
      "iteration 1400 / 1500: loss 2.062144\n",
      "iteration 0 / 1500: loss 1282.055134\n",
      "iteration 100 / 1500: loss 16.996239\n",
      "iteration 200 / 1500: loss 2.269696\n",
      "iteration 300 / 1500: loss 2.102633\n",
      "iteration 400 / 1500: loss 2.102526\n",
      "iteration 500 / 1500: loss 2.110870\n",
      "iteration 600 / 1500: loss 2.092845\n",
      "iteration 700 / 1500: loss 2.108160\n",
      "iteration 800 / 1500: loss 2.139470\n",
      "iteration 900 / 1500: loss 2.175834\n",
      "iteration 1000 / 1500: loss 2.114774\n",
      "iteration 1100 / 1500: loss 2.117661\n",
      "iteration 1200 / 1500: loss 2.117227\n",
      "iteration 1300 / 1500: loss 2.178349\n",
      "iteration 1400 / 1500: loss 2.110073\n",
      "iteration 0 / 1500: loss 465.358241\n",
      "iteration 100 / 1500: loss 264.660889\n",
      "iteration 200 / 1500: loss 151.667490\n",
      "iteration 300 / 1500: loss 87.246716\n",
      "iteration 400 / 1500: loss 50.514210\n",
      "iteration 500 / 1500: loss 29.611244\n",
      "iteration 600 / 1500: loss 17.785958\n",
      "iteration 700 / 1500: loss 11.063949\n",
      "iteration 800 / 1500: loss 7.133291\n",
      "iteration 900 / 1500: loss 4.978111\n",
      "iteration 1000 / 1500: loss 3.719205\n",
      "iteration 1100 / 1500: loss 2.914013\n",
      "iteration 1200 / 1500: loss 2.587993\n",
      "iteration 1300 / 1500: loss 2.340979\n",
      "iteration 1400 / 1500: loss 2.198432\n",
      "iteration 0 / 1500: loss 416.495439\n",
      "iteration 100 / 1500: loss 250.766912\n",
      "iteration 200 / 1500: loss 151.502881\n",
      "iteration 300 / 1500: loss 92.034515\n",
      "iteration 400 / 1500: loss 56.159392\n",
      "iteration 500 / 1500: loss 34.700420\n",
      "iteration 600 / 1500: loss 21.627575\n",
      "iteration 700 / 1500: loss 13.879733\n",
      "iteration 800 / 1500: loss 9.147286\n",
      "iteration 900 / 1500: loss 6.321039\n",
      "iteration 1000 / 1500: loss 4.583915\n",
      "iteration 1100 / 1500: loss 3.659475\n",
      "iteration 1200 / 1500: loss 2.988691\n",
      "iteration 1300 / 1500: loss 2.550638\n",
      "iteration 1400 / 1500: loss 2.303008\n",
      "iteration 0 / 1500: loss 556.932595\n",
      "iteration 100 / 1500: loss 283.527196\n",
      "iteration 200 / 1500: loss 144.907857\n",
      "iteration 300 / 1500: loss 74.511822\n",
      "iteration 400 / 1500: loss 38.752491\n",
      "iteration 500 / 1500: loss 20.761673\n",
      "iteration 600 / 1500: loss 11.659284\n",
      "iteration 700 / 1500: loss 6.916225\n",
      "iteration 800 / 1500: loss 4.537326\n",
      "iteration 900 / 1500: loss 3.327568\n",
      "iteration 1000 / 1500: loss 2.702313\n",
      "iteration 1100 / 1500: loss 2.350883\n",
      "iteration 1200 / 1500: loss 2.235552\n",
      "iteration 1300 / 1500: loss 2.138418\n",
      "iteration 1400 / 1500: loss 2.122961\n",
      "iteration 0 / 1500: loss 1523.169528\n",
      "iteration 100 / 1500: loss 232.500113\n",
      "iteration 200 / 1500: loss 37.099078\n",
      "iteration 300 / 1500: loss 7.441783\n",
      "iteration 400 / 1500: loss 2.970520\n",
      "iteration 500 / 1500: loss 2.228168\n",
      "iteration 600 / 1500: loss 2.134574\n",
      "iteration 700 / 1500: loss 2.124987\n",
      "iteration 800 / 1500: loss 2.128632\n",
      "iteration 900 / 1500: loss 2.175255\n",
      "iteration 1000 / 1500: loss 2.149468\n",
      "iteration 1100 / 1500: loss 2.160219\n",
      "iteration 1200 / 1500: loss 2.150130\n",
      "iteration 1300 / 1500: loss 2.099224\n",
      "iteration 1400 / 1500: loss 2.110733\n",
      "iteration 0 / 1500: loss 1512.252615\n",
      "iteration 100 / 1500: loss 237.954240\n",
      "iteration 200 / 1500: loss 38.999569\n",
      "iteration 300 / 1500: loss 7.911457\n",
      "iteration 400 / 1500: loss 3.035997\n",
      "iteration 500 / 1500: loss 2.295594\n",
      "iteration 600 / 1500: loss 2.185411\n",
      "iteration 700 / 1500: loss 2.118490\n",
      "iteration 800 / 1500: loss 2.142499\n",
      "iteration 900 / 1500: loss 2.167216\n",
      "iteration 1000 / 1500: loss 2.125866\n",
      "iteration 1100 / 1500: loss 2.140769\n",
      "iteration 1200 / 1500: loss 2.189400\n",
      "iteration 1300 / 1500: loss 2.111582\n",
      "iteration 1400 / 1500: loss 2.154309\n",
      "iteration 0 / 1500: loss 812.453563\n",
      "iteration 100 / 1500: loss 300.548447\n",
      "iteration 200 / 1500: loss 112.242912\n",
      "iteration 300 / 1500: loss 42.672926\n",
      "iteration 400 / 1500: loss 17.146725\n",
      "iteration 500 / 1500: loss 7.661394\n",
      "iteration 600 / 1500: loss 4.130437\n",
      "iteration 700 / 1500: loss 2.863694\n",
      "iteration 800 / 1500: loss 2.386094\n",
      "iteration 900 / 1500: loss 2.203595\n",
      "iteration 1000 / 1500: loss 2.128711\n",
      "iteration 1100 / 1500: loss 2.136335\n",
      "iteration 1200 / 1500: loss 2.030377\n",
      "iteration 1300 / 1500: loss 2.069191\n",
      "iteration 1400 / 1500: loss 2.090531\n",
      "iteration 0 / 1500: loss 973.674600\n",
      "iteration 100 / 1500: loss 295.491632\n",
      "iteration 200 / 1500: loss 90.826716\n",
      "iteration 300 / 1500: loss 28.937942\n",
      "iteration 400 / 1500: loss 10.205105\n",
      "iteration 500 / 1500: loss 4.509538\n",
      "iteration 600 / 1500: loss 2.848220\n",
      "iteration 700 / 1500: loss 2.303094\n",
      "iteration 800 / 1500: loss 2.182885\n",
      "iteration 900 / 1500: loss 2.147964\n",
      "iteration 1000 / 1500: loss 2.059883\n",
      "iteration 1100 / 1500: loss 2.139030\n",
      "iteration 1200 / 1500: loss 2.087221\n",
      "iteration 1300 / 1500: loss 2.085942\n",
      "iteration 1400 / 1500: loss 2.078109\n",
      "iteration 0 / 1500: loss 1056.163728\n",
      "iteration 100 / 1500: loss 286.600886\n",
      "iteration 200 / 1500: loss 78.958334\n",
      "iteration 300 / 1500: loss 22.834153\n",
      "iteration 400 / 1500: loss 7.770366\n",
      "iteration 500 / 1500: loss 3.644825\n",
      "iteration 600 / 1500: loss 2.544008\n",
      "iteration 700 / 1500: loss 2.239295\n",
      "iteration 800 / 1500: loss 2.136079\n",
      "iteration 900 / 1500: loss 2.115137\n",
      "iteration 1000 / 1500: loss 2.060019\n",
      "iteration 1100 / 1500: loss 2.071151\n",
      "iteration 1200 / 1500: loss 2.093479\n",
      "iteration 1300 / 1500: loss 2.127137\n",
      "iteration 1400 / 1500: loss 2.095249\n",
      "iteration 0 / 1500: loss 1464.268168\n",
      "iteration 100 / 1500: loss 247.292163\n",
      "iteration 200 / 1500: loss 43.292325\n",
      "iteration 300 / 1500: loss 9.042726\n",
      "iteration 400 / 1500: loss 3.265190\n",
      "iteration 500 / 1500: loss 2.342742\n",
      "iteration 600 / 1500: loss 2.139877\n",
      "iteration 700 / 1500: loss 2.167849\n",
      "iteration 800 / 1500: loss 2.203760\n",
      "iteration 900 / 1500: loss 2.108628\n",
      "iteration 1000 / 1500: loss 2.108697\n",
      "iteration 1100 / 1500: loss 2.165884\n",
      "iteration 1200 / 1500: loss 2.090578\n",
      "iteration 1300 / 1500: loss 2.168231\n",
      "iteration 1400 / 1500: loss 2.111300\n",
      "iteration 0 / 1500: loss 466.610958\n",
      "iteration 100 / 1500: loss 262.666192\n",
      "iteration 200 / 1500: loss 148.395663\n",
      "iteration 300 / 1500: loss 84.436020\n",
      "iteration 400 / 1500: loss 48.415065\n",
      "iteration 500 / 1500: loss 28.045639\n",
      "iteration 600 / 1500: loss 16.693600\n",
      "iteration 700 / 1500: loss 10.301354\n",
      "iteration 800 / 1500: loss 6.636352\n",
      "iteration 900 / 1500: loss 4.629562\n",
      "iteration 1000 / 1500: loss 3.455356\n",
      "iteration 1100 / 1500: loss 2.901226\n",
      "iteration 1200 / 1500: loss 2.478328\n",
      "iteration 1300 / 1500: loss 2.283892\n",
      "iteration 1400 / 1500: loss 2.167359\n",
      "iteration 0 / 1500: loss 329.822975\n",
      "iteration 100 / 1500: loss 220.280691\n",
      "iteration 200 / 1500: loss 148.630121\n",
      "iteration 300 / 1500: loss 100.414776\n",
      "iteration 400 / 1500: loss 67.987376\n",
      "iteration 500 / 1500: loss 46.406059\n",
      "iteration 600 / 1500: loss 31.803157\n",
      "iteration 700 / 1500: loss 22.023908\n",
      "iteration 800 / 1500: loss 15.504218\n",
      "iteration 900 / 1500: loss 10.952889\n",
      "iteration 1000 / 1500: loss 8.094471\n",
      "iteration 1100 / 1500: loss 6.148909\n",
      "iteration 1200 / 1500: loss 4.782603\n",
      "iteration 1300 / 1500: loss 3.930863\n",
      "iteration 1400 / 1500: loss 3.274605\n",
      "iteration 0 / 1500: loss 361.689391\n",
      "iteration 100 / 1500: loss 233.558029\n",
      "iteration 200 / 1500: loss 151.610054\n",
      "iteration 300 / 1500: loss 98.621152\n",
      "iteration 400 / 1500: loss 64.542452\n",
      "iteration 500 / 1500: loss 42.493115\n",
      "iteration 600 / 1500: loss 28.161839\n",
      "iteration 700 / 1500: loss 18.977767\n",
      "iteration 800 / 1500: loss 13.023599\n",
      "iteration 900 / 1500: loss 9.143366\n",
      "iteration 1000 / 1500: loss 6.620723\n",
      "iteration 1100 / 1500: loss 4.918159\n",
      "iteration 1200 / 1500: loss 4.032634\n",
      "iteration 1300 / 1500: loss 3.358631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.835604\n",
      "iteration 0 / 1500: loss 656.706082\n",
      "iteration 100 / 1500: loss 296.367987\n",
      "iteration 200 / 1500: loss 134.433025\n",
      "iteration 300 / 1500: loss 61.698121\n",
      "iteration 400 / 1500: loss 28.905806\n",
      "iteration 500 / 1500: loss 14.173139\n",
      "iteration 600 / 1500: loss 7.495772\n",
      "iteration 700 / 1500: loss 4.548654\n",
      "iteration 800 / 1500: loss 3.157809\n",
      "iteration 900 / 1500: loss 2.541642\n",
      "iteration 1000 / 1500: loss 2.203166\n",
      "iteration 1100 / 1500: loss 2.137102\n",
      "iteration 1200 / 1500: loss 2.147195\n",
      "iteration 1300 / 1500: loss 2.098723\n",
      "iteration 1400 / 1500: loss 2.074226\n",
      "iteration 0 / 1500: loss 396.444921\n",
      "iteration 100 / 1500: loss 243.954990\n",
      "iteration 200 / 1500: loss 150.858179\n",
      "iteration 300 / 1500: loss 93.459096\n",
      "iteration 400 / 1500: loss 58.199741\n",
      "iteration 500 / 1500: loss 36.709846\n",
      "iteration 600 / 1500: loss 23.352877\n",
      "iteration 700 / 1500: loss 15.212000\n",
      "iteration 800 / 1500: loss 10.080117\n",
      "iteration 900 / 1500: loss 6.971731\n",
      "iteration 1000 / 1500: loss 5.118763\n",
      "iteration 1100 / 1500: loss 3.957447\n",
      "iteration 1200 / 1500: loss 3.194958\n",
      "iteration 1300 / 1500: loss 2.763205\n",
      "iteration 1400 / 1500: loss 2.456964\n",
      "iteration 0 / 1500: loss 1084.038364\n",
      "iteration 100 / 1500: loss 284.739342\n",
      "iteration 200 / 1500: loss 76.092217\n",
      "iteration 300 / 1500: loss 21.471315\n",
      "iteration 400 / 1500: loss 7.174924\n",
      "iteration 500 / 1500: loss 3.373172\n",
      "iteration 600 / 1500: loss 2.486288\n",
      "iteration 700 / 1500: loss 2.216558\n",
      "iteration 800 / 1500: loss 2.133561\n",
      "iteration 900 / 1500: loss 2.112123\n",
      "iteration 1000 / 1500: loss 2.155522\n",
      "iteration 1100 / 1500: loss 2.065639\n",
      "iteration 1200 / 1500: loss 2.112646\n",
      "iteration 1300 / 1500: loss 2.094754\n",
      "iteration 1400 / 1500: loss 2.128749\n",
      "iteration 0 / 1500: loss 389.353839\n",
      "iteration 100 / 1500: loss 241.961828\n",
      "iteration 200 / 1500: loss 151.419684\n",
      "iteration 300 / 1500: loss 94.911698\n",
      "iteration 400 / 1500: loss 59.847930\n",
      "iteration 500 / 1500: loss 38.001791\n",
      "iteration 600 / 1500: loss 24.380970\n",
      "iteration 700 / 1500: loss 15.903350\n",
      "iteration 800 / 1500: loss 10.673887\n",
      "iteration 900 / 1500: loss 7.442976\n",
      "iteration 1000 / 1500: loss 5.433109\n",
      "iteration 1100 / 1500: loss 4.009318\n",
      "iteration 1200 / 1500: loss 3.279447\n",
      "iteration 1300 / 1500: loss 2.841330\n",
      "iteration 1400 / 1500: loss 2.554221\n",
      "iteration 0 / 1500: loss 1245.074078\n",
      "iteration 100 / 1500: loss 268.168621\n",
      "iteration 200 / 1500: loss 59.172649\n",
      "iteration 300 / 1500: loss 14.370011\n",
      "iteration 400 / 1500: loss 4.729692\n",
      "iteration 500 / 1500: loss 2.679765\n",
      "iteration 600 / 1500: loss 2.301792\n",
      "iteration 700 / 1500: loss 2.163498\n",
      "iteration 800 / 1500: loss 2.151743\n",
      "iteration 900 / 1500: loss 2.104124\n",
      "iteration 1000 / 1500: loss 2.138054\n",
      "iteration 1100 / 1500: loss 2.150323\n",
      "iteration 1200 / 1500: loss 2.132733\n",
      "iteration 1300 / 1500: loss 2.126047\n",
      "iteration 1400 / 1500: loss 2.052748\n",
      "iteration 0 / 1500: loss 1208.243725\n",
      "iteration 100 / 1500: loss 276.243439\n",
      "iteration 200 / 1500: loss 64.571210\n",
      "iteration 300 / 1500: loss 16.333062\n",
      "iteration 400 / 1500: loss 5.360348\n",
      "iteration 500 / 1500: loss 2.811402\n",
      "iteration 600 / 1500: loss 2.302981\n",
      "iteration 700 / 1500: loss 2.183629\n",
      "iteration 800 / 1500: loss 2.136137\n",
      "iteration 900 / 1500: loss 2.134967\n",
      "iteration 1000 / 1500: loss 2.126729\n",
      "iteration 1100 / 1500: loss 2.139904\n",
      "iteration 1200 / 1500: loss 2.104159\n",
      "iteration 1300 / 1500: loss 2.107535\n",
      "iteration 1400 / 1500: loss 2.181544\n",
      "iteration 0 / 1500: loss 384.158340\n",
      "iteration 100 / 1500: loss 239.170139\n",
      "iteration 200 / 1500: loss 149.954829\n",
      "iteration 300 / 1500: loss 94.443118\n",
      "iteration 400 / 1500: loss 59.786690\n",
      "iteration 500 / 1500: loss 37.978678\n",
      "iteration 600 / 1500: loss 24.489886\n",
      "iteration 700 / 1500: loss 16.031802\n",
      "iteration 800 / 1500: loss 10.729045\n",
      "iteration 900 / 1500: loss 7.533605\n",
      "iteration 1000 / 1500: loss 5.420422\n",
      "iteration 1100 / 1500: loss 4.214654\n",
      "iteration 1200 / 1500: loss 3.363632\n",
      "iteration 1300 / 1500: loss 2.816968\n",
      "iteration 1400 / 1500: loss 2.515695\n",
      "iteration 0 / 1500: loss 1280.651577\n",
      "iteration 100 / 1500: loss 268.308627\n",
      "iteration 200 / 1500: loss 57.719396\n",
      "iteration 300 / 1500: loss 13.729657\n",
      "iteration 400 / 1500: loss 4.510707\n",
      "iteration 500 / 1500: loss 2.653863\n",
      "iteration 600 / 1500: loss 2.288935\n",
      "iteration 700 / 1500: loss 2.147275\n",
      "iteration 800 / 1500: loss 2.132951\n",
      "iteration 900 / 1500: loss 2.115751\n",
      "iteration 1000 / 1500: loss 2.058729\n",
      "iteration 1100 / 1500: loss 2.167297\n",
      "iteration 1200 / 1500: loss 2.063840\n",
      "iteration 1300 / 1500: loss 2.089827\n",
      "iteration 1400 / 1500: loss 2.145130\n",
      "iteration 0 / 1500: loss 461.347612\n",
      "iteration 100 / 1500: loss 121.734865\n",
      "iteration 200 / 1500: loss 33.364391\n",
      "iteration 300 / 1500: loss 10.284112\n",
      "iteration 400 / 1500: loss 4.177659\n",
      "iteration 500 / 1500: loss 2.586967\n",
      "iteration 600 / 1500: loss 2.204937\n",
      "iteration 700 / 1500: loss 2.113566\n",
      "iteration 800 / 1500: loss 2.108644\n",
      "iteration 900 / 1500: loss 2.018942\n",
      "iteration 1000 / 1500: loss 2.014561\n",
      "iteration 1100 / 1500: loss 2.018995\n",
      "iteration 1200 / 1500: loss 2.047919\n",
      "iteration 1300 / 1500: loss 2.015084\n",
      "iteration 1400 / 1500: loss 2.025752\n",
      "iteration 0 / 1500: loss 417.336232\n",
      "iteration 100 / 1500: loss 125.577761\n",
      "iteration 200 / 1500: loss 38.839037\n",
      "iteration 300 / 1500: loss 13.019307\n",
      "iteration 400 / 1500: loss 5.324495\n",
      "iteration 500 / 1500: loss 2.972200\n",
      "iteration 600 / 1500: loss 2.338545\n",
      "iteration 700 / 1500: loss 2.094178\n",
      "iteration 800 / 1500: loss 2.026149\n",
      "iteration 900 / 1500: loss 2.014739\n",
      "iteration 1000 / 1500: loss 2.095799\n",
      "iteration 1100 / 1500: loss 2.036682\n",
      "iteration 1200 / 1500: loss 2.043533\n",
      "iteration 1300 / 1500: loss 2.011575\n",
      "iteration 1400 / 1500: loss 2.004917\n",
      "iteration 0 / 1500: loss 551.887235\n",
      "iteration 100 / 1500: loss 111.676922\n",
      "iteration 200 / 1500: loss 23.975141\n",
      "iteration 300 / 1500: loss 6.400794\n",
      "iteration 400 / 1500: loss 2.913072\n",
      "iteration 500 / 1500: loss 2.284932\n",
      "iteration 600 / 1500: loss 2.067114\n",
      "iteration 700 / 1500: loss 2.102011\n",
      "iteration 800 / 1500: loss 2.051675\n",
      "iteration 900 / 1500: loss 1.994275\n",
      "iteration 1000 / 1500: loss 2.101487\n",
      "iteration 1100 / 1500: loss 2.054624\n",
      "iteration 1200 / 1500: loss 2.037251\n",
      "iteration 1300 / 1500: loss 2.013032\n",
      "iteration 1400 / 1500: loss 2.078656\n",
      "iteration 0 / 1500: loss 1551.606137\n",
      "iteration 100 / 1500: loss 19.189031\n",
      "iteration 200 / 1500: loss 2.357937\n",
      "iteration 300 / 1500: loss 2.149469\n",
      "iteration 400 / 1500: loss 2.109839\n",
      "iteration 500 / 1500: loss 2.110194\n",
      "iteration 600 / 1500: loss 2.118262\n",
      "iteration 700 / 1500: loss 2.137896\n",
      "iteration 800 / 1500: loss 2.141844\n",
      "iteration 900 / 1500: loss 2.153399\n",
      "iteration 1000 / 1500: loss 2.172769\n",
      "iteration 1100 / 1500: loss 2.156761\n",
      "iteration 1200 / 1500: loss 2.147946\n",
      "iteration 1300 / 1500: loss 2.173709\n",
      "iteration 1400 / 1500: loss 2.148140\n",
      "iteration 0 / 1500: loss 1509.325747\n",
      "iteration 100 / 1500: loss 19.932259\n",
      "iteration 200 / 1500: loss 2.352610\n",
      "iteration 300 / 1500: loss 2.142683\n",
      "iteration 400 / 1500: loss 2.176231\n",
      "iteration 500 / 1500: loss 2.175597\n",
      "iteration 600 / 1500: loss 2.161160\n",
      "iteration 700 / 1500: loss 2.112350\n",
      "iteration 800 / 1500: loss 2.159499\n",
      "iteration 900 / 1500: loss 2.144113\n",
      "iteration 1000 / 1500: loss 2.155417\n",
      "iteration 1100 / 1500: loss 2.175811\n",
      "iteration 1200 / 1500: loss 2.158343\n",
      "iteration 1300 / 1500: loss 2.154223\n",
      "iteration 1400 / 1500: loss 2.132465\n",
      "iteration 0 / 1500: loss 819.760351\n",
      "iteration 100 / 1500: loss 77.816003\n",
      "iteration 200 / 1500: loss 9.157198\n",
      "iteration 300 / 1500: loss 2.744250\n",
      "iteration 400 / 1500: loss 2.168741\n",
      "iteration 500 / 1500: loss 2.108842\n",
      "iteration 600 / 1500: loss 2.076532\n",
      "iteration 700 / 1500: loss 2.091667\n",
      "iteration 800 / 1500: loss 2.181166\n",
      "iteration 900 / 1500: loss 2.127825\n",
      "iteration 1000 / 1500: loss 2.114879\n",
      "iteration 1100 / 1500: loss 2.037487\n",
      "iteration 1200 / 1500: loss 2.140735\n",
      "iteration 1300 / 1500: loss 2.101217\n",
      "iteration 1400 / 1500: loss 2.134999\n",
      "iteration 0 / 1500: loss 961.466394\n",
      "iteration 100 / 1500: loss 57.352564\n",
      "iteration 200 / 1500: loss 5.293297\n",
      "iteration 300 / 1500: loss 2.251034\n",
      "iteration 400 / 1500: loss 2.125499\n",
      "iteration 500 / 1500: loss 2.039230\n",
      "iteration 600 / 1500: loss 2.092906\n",
      "iteration 700 / 1500: loss 2.071298\n",
      "iteration 800 / 1500: loss 2.068664\n",
      "iteration 900 / 1500: loss 2.112829\n",
      "iteration 1000 / 1500: loss 2.140305\n",
      "iteration 1100 / 1500: loss 2.114152\n",
      "iteration 1200 / 1500: loss 2.048162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 1500: loss 2.121743\n",
      "iteration 1400 / 1500: loss 2.075455\n",
      "iteration 0 / 1500: loss 1051.366431\n",
      "iteration 100 / 1500: loss 48.147719\n",
      "iteration 200 / 1500: loss 4.144764\n",
      "iteration 300 / 1500: loss 2.198518\n",
      "iteration 400 / 1500: loss 2.148098\n",
      "iteration 500 / 1500: loss 2.120503\n",
      "iteration 600 / 1500: loss 2.123766\n",
      "iteration 700 / 1500: loss 2.122251\n",
      "iteration 800 / 1500: loss 2.152089\n",
      "iteration 900 / 1500: loss 2.085797\n",
      "iteration 1000 / 1500: loss 2.104147\n",
      "iteration 1100 / 1500: loss 2.135280\n",
      "iteration 1200 / 1500: loss 2.111545\n",
      "iteration 1300 / 1500: loss 2.121915\n",
      "iteration 1400 / 1500: loss 2.032941\n",
      "iteration 0 / 1500: loss 1440.709682\n",
      "iteration 100 / 1500: loss 22.307247\n",
      "iteration 200 / 1500: loss 2.413253\n",
      "iteration 300 / 1500: loss 2.214395\n",
      "iteration 400 / 1500: loss 2.087827\n",
      "iteration 500 / 1500: loss 2.152968\n",
      "iteration 600 / 1500: loss 2.156939\n",
      "iteration 700 / 1500: loss 2.170153\n",
      "iteration 800 / 1500: loss 2.142792\n",
      "iteration 900 / 1500: loss 2.149580\n",
      "iteration 1000 / 1500: loss 2.164393\n",
      "iteration 1100 / 1500: loss 2.165856\n",
      "iteration 1200 / 1500: loss 2.131088\n",
      "iteration 1300 / 1500: loss 2.187071\n",
      "iteration 1400 / 1500: loss 2.071528\n",
      "iteration 0 / 1500: loss 467.182051\n",
      "iteration 100 / 1500: loss 119.656661\n",
      "iteration 200 / 1500: loss 31.997163\n",
      "iteration 300 / 1500: loss 9.656936\n",
      "iteration 400 / 1500: loss 3.868100\n",
      "iteration 500 / 1500: loss 2.570360\n",
      "iteration 600 / 1500: loss 2.192027\n",
      "iteration 700 / 1500: loss 2.129308\n",
      "iteration 800 / 1500: loss 1.993068\n",
      "iteration 900 / 1500: loss 2.079964\n",
      "iteration 1000 / 1500: loss 2.006341\n",
      "iteration 1100 / 1500: loss 2.028432\n",
      "iteration 1200 / 1500: loss 2.014997\n",
      "iteration 1300 / 1500: loss 2.013322\n",
      "iteration 1400 / 1500: loss 2.093218\n",
      "iteration 0 / 1500: loss 322.033218\n",
      "iteration 100 / 1500: loss 125.883821\n",
      "iteration 200 / 1500: loss 50.039498\n",
      "iteration 300 / 1500: loss 20.663796\n",
      "iteration 400 / 1500: loss 9.243264\n",
      "iteration 500 / 1500: loss 4.851348\n",
      "iteration 600 / 1500: loss 3.083291\n",
      "iteration 700 / 1500: loss 2.460942\n",
      "iteration 800 / 1500: loss 2.155413\n",
      "iteration 900 / 1500: loss 2.114191\n",
      "iteration 1000 / 1500: loss 1.964635\n",
      "iteration 1100 / 1500: loss 1.990442\n",
      "iteration 1200 / 1500: loss 2.026269\n",
      "iteration 1300 / 1500: loss 2.077878\n",
      "iteration 1400 / 1500: loss 1.977572\n",
      "iteration 0 / 1500: loss 361.589882\n",
      "iteration 100 / 1500: loss 128.777248\n",
      "iteration 200 / 1500: loss 46.889205\n",
      "iteration 300 / 1500: loss 17.896248\n",
      "iteration 400 / 1500: loss 7.624453\n",
      "iteration 500 / 1500: loss 3.990708\n",
      "iteration 600 / 1500: loss 2.766345\n",
      "iteration 700 / 1500: loss 2.337748\n",
      "iteration 800 / 1500: loss 2.148146\n",
      "iteration 900 / 1500: loss 2.086262\n",
      "iteration 1000 / 1500: loss 1.966829\n",
      "iteration 1100 / 1500: loss 1.965199\n",
      "iteration 1200 / 1500: loss 2.059993\n",
      "iteration 1300 / 1500: loss 1.990298\n",
      "iteration 1400 / 1500: loss 1.995394\n",
      "iteration 0 / 1500: loss 651.148923\n",
      "iteration 100 / 1500: loss 98.824400\n",
      "iteration 200 / 1500: loss 16.513803\n",
      "iteration 300 / 1500: loss 4.259930\n",
      "iteration 400 / 1500: loss 2.383409\n",
      "iteration 500 / 1500: loss 2.167129\n",
      "iteration 600 / 1500: loss 2.041534\n",
      "iteration 700 / 1500: loss 2.073164\n",
      "iteration 800 / 1500: loss 2.118647\n",
      "iteration 900 / 1500: loss 2.067151\n",
      "iteration 1000 / 1500: loss 2.029548\n",
      "iteration 1100 / 1500: loss 2.023846\n",
      "iteration 1200 / 1500: loss 2.017378\n",
      "iteration 1300 / 1500: loss 2.053326\n",
      "iteration 1400 / 1500: loss 2.144778\n",
      "iteration 0 / 1500: loss 397.112343\n",
      "iteration 100 / 1500: loss 125.784381\n",
      "iteration 200 / 1500: loss 40.996942\n",
      "iteration 300 / 1500: loss 14.362986\n",
      "iteration 400 / 1500: loss 5.921886\n",
      "iteration 500 / 1500: loss 3.284495\n",
      "iteration 600 / 1500: loss 2.440858\n",
      "iteration 700 / 1500: loss 2.177422\n",
      "iteration 800 / 1500: loss 2.105905\n",
      "iteration 900 / 1500: loss 2.144285\n",
      "iteration 1000 / 1500: loss 1.997817\n",
      "iteration 1100 / 1500: loss 2.020130\n",
      "iteration 1200 / 1500: loss 2.071286\n",
      "iteration 1300 / 1500: loss 2.061916\n",
      "iteration 1400 / 1500: loss 1.983960\n",
      "iteration 0 / 1500: loss 1089.128704\n",
      "iteration 100 / 1500: loss 46.363059\n",
      "iteration 200 / 1500: loss 3.929708\n",
      "iteration 300 / 1500: loss 2.099599\n",
      "iteration 400 / 1500: loss 2.121281\n",
      "iteration 500 / 1500: loss 2.127079\n",
      "iteration 600 / 1500: loss 2.106939\n",
      "iteration 700 / 1500: loss 2.081197\n",
      "iteration 800 / 1500: loss 2.104979\n",
      "iteration 900 / 1500: loss 2.075859\n",
      "iteration 1000 / 1500: loss 2.095460\n",
      "iteration 1100 / 1500: loss 2.081074\n",
      "iteration 1200 / 1500: loss 2.138397\n",
      "iteration 1300 / 1500: loss 2.094170\n",
      "iteration 1400 / 1500: loss 2.157400\n",
      "iteration 0 / 1500: loss 390.697559\n",
      "iteration 100 / 1500: loss 127.376963\n",
      "iteration 200 / 1500: loss 42.496900\n",
      "iteration 300 / 1500: loss 15.122922\n",
      "iteration 400 / 1500: loss 6.213263\n",
      "iteration 500 / 1500: loss 3.370480\n",
      "iteration 600 / 1500: loss 2.488899\n",
      "iteration 700 / 1500: loss 2.170673\n",
      "iteration 800 / 1500: loss 2.045348\n",
      "iteration 900 / 1500: loss 2.024508\n",
      "iteration 1000 / 1500: loss 2.019852\n",
      "iteration 1100 / 1500: loss 2.008343\n",
      "iteration 1200 / 1500: loss 2.017513\n",
      "iteration 1300 / 1500: loss 2.057893\n",
      "iteration 1400 / 1500: loss 2.000924\n",
      "iteration 0 / 1500: loss 1258.109355\n",
      "iteration 100 / 1500: loss 33.838035\n",
      "iteration 200 / 1500: loss 2.961569\n",
      "iteration 300 / 1500: loss 2.146404\n",
      "iteration 400 / 1500: loss 2.130288\n",
      "iteration 500 / 1500: loss 2.132278\n",
      "iteration 600 / 1500: loss 2.088084\n",
      "iteration 700 / 1500: loss 2.142557\n",
      "iteration 800 / 1500: loss 2.148057\n",
      "iteration 900 / 1500: loss 2.148168\n",
      "iteration 1000 / 1500: loss 2.096149\n",
      "iteration 1100 / 1500: loss 2.154808\n",
      "iteration 1200 / 1500: loss 2.132583\n",
      "iteration 1300 / 1500: loss 2.127167\n",
      "iteration 1400 / 1500: loss 2.142322\n",
      "iteration 0 / 1500: loss 1196.650274\n",
      "iteration 100 / 1500: loss 37.010342\n",
      "iteration 200 / 1500: loss 3.122800\n",
      "iteration 300 / 1500: loss 2.139862\n",
      "iteration 400 / 1500: loss 2.147291\n",
      "iteration 500 / 1500: loss 2.160171\n",
      "iteration 600 / 1500: loss 2.140611\n",
      "iteration 700 / 1500: loss 2.063529\n",
      "iteration 800 / 1500: loss 2.099433\n",
      "iteration 900 / 1500: loss 2.086916\n",
      "iteration 1000 / 1500: loss 2.130116\n",
      "iteration 1100 / 1500: loss 2.102526\n",
      "iteration 1200 / 1500: loss 2.129839\n",
      "iteration 1300 / 1500: loss 2.155075\n",
      "iteration 1400 / 1500: loss 2.099279\n",
      "iteration 0 / 1500: loss 390.498074\n",
      "iteration 100 / 1500: loss 127.672269\n",
      "iteration 200 / 1500: loss 42.962687\n",
      "iteration 300 / 1500: loss 15.355123\n",
      "iteration 400 / 1500: loss 6.369542\n",
      "iteration 500 / 1500: loss 3.452759\n",
      "iteration 600 / 1500: loss 2.494465\n",
      "iteration 700 / 1500: loss 2.106623\n",
      "iteration 800 / 1500: loss 2.133590\n",
      "iteration 900 / 1500: loss 2.099280\n",
      "iteration 1000 / 1500: loss 2.003295\n",
      "iteration 1100 / 1500: loss 2.014711\n",
      "iteration 1200 / 1500: loss 1.993672\n",
      "iteration 1300 / 1500: loss 2.046278\n",
      "iteration 1400 / 1500: loss 1.954983\n",
      "iteration 0 / 1500: loss 1257.293502\n",
      "iteration 100 / 1500: loss 31.790616\n",
      "iteration 200 / 1500: loss 2.830795\n",
      "iteration 300 / 1500: loss 2.136734\n",
      "iteration 400 / 1500: loss 2.122265\n",
      "iteration 500 / 1500: loss 2.147749\n",
      "iteration 600 / 1500: loss 2.136862\n",
      "iteration 700 / 1500: loss 2.158724\n",
      "iteration 800 / 1500: loss 2.154552\n",
      "iteration 900 / 1500: loss 2.175462\n",
      "iteration 1000 / 1500: loss 2.121489\n",
      "iteration 1100 / 1500: loss 2.143253\n",
      "iteration 1200 / 1500: loss 2.136248\n",
      "iteration 1300 / 1500: loss 2.101997\n",
      "iteration 1400 / 1500: loss 2.137084\n",
      "lr 2.607995e-08 reg 1.045806e+04 train accuracy: 0.222041 val accuracy: 0.233000\n",
      "lr 2.607995e-08 reg 1.147450e+04 train accuracy: 0.233122 val accuracy: 0.246000\n",
      "lr 2.607995e-08 reg 1.241578e+04 train accuracy: 0.227510 val accuracy: 0.228000\n",
      "lr 2.607995e-08 reg 1.248531e+04 train accuracy: 0.239408 val accuracy: 0.240000\n",
      "lr 2.607995e-08 reg 1.277496e+04 train accuracy: 0.240735 val accuracy: 0.239000\n",
      "lr 2.607995e-08 reg 1.336581e+04 train accuracy: 0.237796 val accuracy: 0.232000\n",
      "lr 2.607995e-08 reg 1.481408e+04 train accuracy: 0.239898 val accuracy: 0.230000\n",
      "lr 2.607995e-08 reg 1.514801e+04 train accuracy: 0.247061 val accuracy: 0.264000\n",
      "lr 2.607995e-08 reg 1.782350e+04 train accuracy: 0.260163 val accuracy: 0.268000\n",
      "lr 2.607995e-08 reg 2.101221e+04 train accuracy: 0.264551 val accuracy: 0.286000\n",
      "lr 2.607995e-08 reg 2.625685e+04 train accuracy: 0.289531 val accuracy: 0.291000\n",
      "lr 2.607995e-08 reg 3.150788e+04 train accuracy: 0.300041 val accuracy: 0.295000\n",
      "lr 2.607995e-08 reg 3.446966e+04 train accuracy: 0.296633 val accuracy: 0.305000\n",
      "lr 2.607995e-08 reg 3.529470e+04 train accuracy: 0.305735 val accuracy: 0.321000\n",
      "lr 2.607995e-08 reg 3.894720e+04 train accuracy: 0.300388 val accuracy: 0.335000\n",
      "lr 2.607995e-08 reg 4.054160e+04 train accuracy: 0.303102 val accuracy: 0.317000\n",
      "lr 2.607995e-08 reg 4.125281e+04 train accuracy: 0.307429 val accuracy: 0.314000\n",
      "lr 2.607995e-08 reg 4.696066e+04 train accuracy: 0.306388 val accuracy: 0.313000\n",
      "lr 2.607995e-08 reg 4.882880e+04 train accuracy: 0.308755 val accuracy: 0.321000\n",
      "lr 2.607995e-08 reg 4.961007e+04 train accuracy: 0.299857 val accuracy: 0.317000\n",
      "lr 3.570127e-08 reg 1.045806e+04 train accuracy: 0.255265 val accuracy: 0.268000\n",
      "lr 3.570127e-08 reg 1.147450e+04 train accuracy: 0.270918 val accuracy: 0.261000\n",
      "lr 3.570127e-08 reg 1.241578e+04 train accuracy: 0.273469 val accuracy: 0.287000\n",
      "lr 3.570127e-08 reg 1.248531e+04 train accuracy: 0.266898 val accuracy: 0.272000\n",
      "lr 3.570127e-08 reg 1.277496e+04 train accuracy: 0.274449 val accuracy: 0.277000\n",
      "lr 3.570127e-08 reg 1.336581e+04 train accuracy: 0.283551 val accuracy: 0.288000\n",
      "lr 3.570127e-08 reg 1.481408e+04 train accuracy: 0.278469 val accuracy: 0.271000\n",
      "lr 3.570127e-08 reg 1.514801e+04 train accuracy: 0.288633 val accuracy: 0.300000\n",
      "lr 3.570127e-08 reg 1.782350e+04 train accuracy: 0.301020 val accuracy: 0.314000\n",
      "lr 3.570127e-08 reg 2.101221e+04 train accuracy: 0.311551 val accuracy: 0.308000\n",
      "lr 3.570127e-08 reg 2.625685e+04 train accuracy: 0.318347 val accuracy: 0.345000\n",
      "lr 3.570127e-08 reg 3.150788e+04 train accuracy: 0.321510 val accuracy: 0.332000\n",
      "lr 3.570127e-08 reg 3.446966e+04 train accuracy: 0.318306 val accuracy: 0.317000\n",
      "lr 3.570127e-08 reg 3.529470e+04 train accuracy: 0.315816 val accuracy: 0.328000\n",
      "lr 3.570127e-08 reg 3.894720e+04 train accuracy: 0.313857 val accuracy: 0.325000\n",
      "lr 3.570127e-08 reg 4.054160e+04 train accuracy: 0.317673 val accuracy: 0.333000\n",
      "lr 3.570127e-08 reg 4.125281e+04 train accuracy: 0.311816 val accuracy: 0.331000\n",
      "lr 3.570127e-08 reg 4.696066e+04 train accuracy: 0.312735 val accuracy: 0.327000\n",
      "lr 3.570127e-08 reg 4.882880e+04 train accuracy: 0.306694 val accuracy: 0.324000\n",
      "lr 3.570127e-08 reg 4.961007e+04 train accuracy: 0.310531 val accuracy: 0.328000\n",
      "lr 5.312642e-08 reg 1.045806e+04 train accuracy: 0.311898 val accuracy: 0.316000\n",
      "lr 5.312642e-08 reg 1.147450e+04 train accuracy: 0.311878 val accuracy: 0.328000\n",
      "lr 5.312642e-08 reg 1.241578e+04 train accuracy: 0.320000 val accuracy: 0.344000\n",
      "lr 5.312642e-08 reg 1.248531e+04 train accuracy: 0.326265 val accuracy: 0.331000\n",
      "lr 5.312642e-08 reg 1.277496e+04 train accuracy: 0.321694 val accuracy: 0.329000\n",
      "lr 5.312642e-08 reg 1.336581e+04 train accuracy: 0.319469 val accuracy: 0.327000\n",
      "lr 5.312642e-08 reg 1.481408e+04 train accuracy: 0.325469 val accuracy: 0.336000\n",
      "lr 5.312642e-08 reg 1.514801e+04 train accuracy: 0.330776 val accuracy: 0.344000\n",
      "lr 5.312642e-08 reg 1.782350e+04 train accuracy: 0.333592 val accuracy: 0.347000\n",
      "lr 5.312642e-08 reg 2.101221e+04 train accuracy: 0.329959 val accuracy: 0.345000\n",
      "lr 5.312642e-08 reg 2.625685e+04 train accuracy: 0.328143 val accuracy: 0.345000\n",
      "lr 5.312642e-08 reg 3.150788e+04 train accuracy: 0.322429 val accuracy: 0.336000\n",
      "lr 5.312642e-08 reg 3.446966e+04 train accuracy: 0.319816 val accuracy: 0.327000\n",
      "lr 5.312642e-08 reg 3.529470e+04 train accuracy: 0.316388 val accuracy: 0.327000\n",
      "lr 5.312642e-08 reg 3.894720e+04 train accuracy: 0.315184 val accuracy: 0.334000\n",
      "lr 5.312642e-08 reg 4.054160e+04 train accuracy: 0.313653 val accuracy: 0.328000\n",
      "lr 5.312642e-08 reg 4.125281e+04 train accuracy: 0.317020 val accuracy: 0.328000\n",
      "lr 5.312642e-08 reg 4.696066e+04 train accuracy: 0.307714 val accuracy: 0.321000\n",
      "lr 5.312642e-08 reg 4.882880e+04 train accuracy: 0.302184 val accuracy: 0.319000\n",
      "lr 5.312642e-08 reg 4.961007e+04 train accuracy: 0.312163 val accuracy: 0.321000\n",
      "lr 5.976081e-08 reg 1.045806e+04 train accuracy: 0.317245 val accuracy: 0.331000\n",
      "lr 5.976081e-08 reg 1.147450e+04 train accuracy: 0.329082 val accuracy: 0.345000\n",
      "lr 5.976081e-08 reg 1.241578e+04 train accuracy: 0.327837 val accuracy: 0.343000\n",
      "lr 5.976081e-08 reg 1.248531e+04 train accuracy: 0.326082 val accuracy: 0.344000\n",
      "lr 5.976081e-08 reg 1.277496e+04 train accuracy: 0.329224 val accuracy: 0.352000\n",
      "lr 5.976081e-08 reg 1.336581e+04 train accuracy: 0.331857 val accuracy: 0.347000\n",
      "lr 5.976081e-08 reg 1.481408e+04 train accuracy: 0.337082 val accuracy: 0.334000\n",
      "lr 5.976081e-08 reg 1.514801e+04 train accuracy: 0.337837 val accuracy: 0.345000\n",
      "lr 5.976081e-08 reg 1.782350e+04 train accuracy: 0.333082 val accuracy: 0.350000\n",
      "lr 5.976081e-08 reg 2.101221e+04 train accuracy: 0.334449 val accuracy: 0.348000\n",
      "lr 5.976081e-08 reg 2.625685e+04 train accuracy: 0.325204 val accuracy: 0.335000\n",
      "lr 5.976081e-08 reg 3.150788e+04 train accuracy: 0.326102 val accuracy: 0.341000\n",
      "lr 5.976081e-08 reg 3.446966e+04 train accuracy: 0.317694 val accuracy: 0.337000\n",
      "lr 5.976081e-08 reg 3.529470e+04 train accuracy: 0.317837 val accuracy: 0.331000\n",
      "lr 5.976081e-08 reg 3.894720e+04 train accuracy: 0.310735 val accuracy: 0.329000\n",
      "lr 5.976081e-08 reg 4.054160e+04 train accuracy: 0.313306 val accuracy: 0.328000\n",
      "lr 5.976081e-08 reg 4.125281e+04 train accuracy: 0.310388 val accuracy: 0.328000\n",
      "lr 5.976081e-08 reg 4.696066e+04 train accuracy: 0.309653 val accuracy: 0.329000\n",
      "lr 5.976081e-08 reg 4.882880e+04 train accuracy: 0.308061 val accuracy: 0.325000\n",
      "lr 5.976081e-08 reg 4.961007e+04 train accuracy: 0.308286 val accuracy: 0.326000\n",
      "lr 6.719420e-08 reg 1.045806e+04 train accuracy: 0.337898 val accuracy: 0.356000\n",
      "lr 6.719420e-08 reg 1.147450e+04 train accuracy: 0.336286 val accuracy: 0.352000\n",
      "lr 6.719420e-08 reg 1.241578e+04 train accuracy: 0.343204 val accuracy: 0.369000\n",
      "lr 6.719420e-08 reg 1.248531e+04 train accuracy: 0.338429 val accuracy: 0.341000\n",
      "lr 6.719420e-08 reg 1.277496e+04 train accuracy: 0.340306 val accuracy: 0.367000\n",
      "lr 6.719420e-08 reg 1.336581e+04 train accuracy: 0.339837 val accuracy: 0.346000\n",
      "lr 6.719420e-08 reg 1.481408e+04 train accuracy: 0.341531 val accuracy: 0.345000\n",
      "lr 6.719420e-08 reg 1.514801e+04 train accuracy: 0.338224 val accuracy: 0.349000\n",
      "lr 6.719420e-08 reg 1.782350e+04 train accuracy: 0.343061 val accuracy: 0.350000\n",
      "lr 6.719420e-08 reg 2.101221e+04 train accuracy: 0.332490 val accuracy: 0.343000\n",
      "lr 6.719420e-08 reg 2.625685e+04 train accuracy: 0.332755 val accuracy: 0.341000\n",
      "lr 6.719420e-08 reg 3.150788e+04 train accuracy: 0.323429 val accuracy: 0.342000\n",
      "lr 6.719420e-08 reg 3.446966e+04 train accuracy: 0.323265 val accuracy: 0.333000\n",
      "lr 6.719420e-08 reg 3.529470e+04 train accuracy: 0.319286 val accuracy: 0.331000\n",
      "lr 6.719420e-08 reg 3.894720e+04 train accuracy: 0.317429 val accuracy: 0.334000\n",
      "lr 6.719420e-08 reg 4.054160e+04 train accuracy: 0.309510 val accuracy: 0.332000\n",
      "lr 6.719420e-08 reg 4.125281e+04 train accuracy: 0.312327 val accuracy: 0.328000\n",
      "lr 6.719420e-08 reg 4.696066e+04 train accuracy: 0.308490 val accuracy: 0.327000\n",
      "lr 6.719420e-08 reg 4.882880e+04 train accuracy: 0.308306 val accuracy: 0.319000\n",
      "lr 6.719420e-08 reg 4.961007e+04 train accuracy: 0.308571 val accuracy: 0.320000\n",
      "lr 7.408057e-08 reg 1.045806e+04 train accuracy: 0.340837 val accuracy: 0.362000\n",
      "lr 7.408057e-08 reg 1.147450e+04 train accuracy: 0.345959 val accuracy: 0.360000\n",
      "lr 7.408057e-08 reg 1.241578e+04 train accuracy: 0.343306 val accuracy: 0.355000\n",
      "lr 7.408057e-08 reg 1.248531e+04 train accuracy: 0.342286 val accuracy: 0.360000\n",
      "lr 7.408057e-08 reg 1.277496e+04 train accuracy: 0.343796 val accuracy: 0.345000\n",
      "lr 7.408057e-08 reg 1.336581e+04 train accuracy: 0.346571 val accuracy: 0.363000\n",
      "lr 7.408057e-08 reg 1.481408e+04 train accuracy: 0.340980 val accuracy: 0.361000\n",
      "lr 7.408057e-08 reg 1.514801e+04 train accuracy: 0.342755 val accuracy: 0.354000\n",
      "lr 7.408057e-08 reg 1.782350e+04 train accuracy: 0.338020 val accuracy: 0.343000\n",
      "lr 7.408057e-08 reg 2.101221e+04 train accuracy: 0.331449 val accuracy: 0.345000\n",
      "lr 7.408057e-08 reg 2.625685e+04 train accuracy: 0.324694 val accuracy: 0.342000\n",
      "lr 7.408057e-08 reg 3.150788e+04 train accuracy: 0.326449 val accuracy: 0.333000\n",
      "lr 7.408057e-08 reg 3.446966e+04 train accuracy: 0.321592 val accuracy: 0.334000\n",
      "lr 7.408057e-08 reg 3.529470e+04 train accuracy: 0.321490 val accuracy: 0.338000\n",
      "lr 7.408057e-08 reg 3.894720e+04 train accuracy: 0.307816 val accuracy: 0.324000\n",
      "lr 7.408057e-08 reg 4.054160e+04 train accuracy: 0.323571 val accuracy: 0.335000\n",
      "lr 7.408057e-08 reg 4.125281e+04 train accuracy: 0.313816 val accuracy: 0.334000\n",
      "lr 7.408057e-08 reg 4.696066e+04 train accuracy: 0.310918 val accuracy: 0.330000\n",
      "lr 7.408057e-08 reg 4.882880e+04 train accuracy: 0.307265 val accuracy: 0.321000\n",
      "lr 7.408057e-08 reg 4.961007e+04 train accuracy: 0.302061 val accuracy: 0.322000\n",
      "lr 9.445829e-08 reg 1.045806e+04 train accuracy: 0.351612 val accuracy: 0.373000\n",
      "lr 9.445829e-08 reg 1.147450e+04 train accuracy: 0.344694 val accuracy: 0.361000\n",
      "lr 9.445829e-08 reg 1.241578e+04 train accuracy: 0.351776 val accuracy: 0.371000\n",
      "lr 9.445829e-08 reg 1.248531e+04 train accuracy: 0.347837 val accuracy: 0.364000\n",
      "lr 9.445829e-08 reg 1.277496e+04 train accuracy: 0.347551 val accuracy: 0.360000\n",
      "lr 9.445829e-08 reg 1.336581e+04 train accuracy: 0.346776 val accuracy: 0.365000\n",
      "lr 9.445829e-08 reg 1.481408e+04 train accuracy: 0.343347 val accuracy: 0.353000\n",
      "lr 9.445829e-08 reg 1.514801e+04 train accuracy: 0.343633 val accuracy: 0.343000\n",
      "lr 9.445829e-08 reg 1.782350e+04 train accuracy: 0.336939 val accuracy: 0.356000\n",
      "lr 9.445829e-08 reg 2.101221e+04 train accuracy: 0.334673 val accuracy: 0.346000\n",
      "lr 9.445829e-08 reg 2.625685e+04 train accuracy: 0.326102 val accuracy: 0.341000\n",
      "lr 9.445829e-08 reg 3.150788e+04 train accuracy: 0.323041 val accuracy: 0.331000\n",
      "lr 9.445829e-08 reg 3.446966e+04 train accuracy: 0.325571 val accuracy: 0.338000\n",
      "lr 9.445829e-08 reg 3.529470e+04 train accuracy: 0.317531 val accuracy: 0.329000\n",
      "lr 9.445829e-08 reg 3.894720e+04 train accuracy: 0.315327 val accuracy: 0.330000\n",
      "lr 9.445829e-08 reg 4.054160e+04 train accuracy: 0.313204 val accuracy: 0.327000\n",
      "lr 9.445829e-08 reg 4.125281e+04 train accuracy: 0.308061 val accuracy: 0.321000\n",
      "lr 9.445829e-08 reg 4.696066e+04 train accuracy: 0.309633 val accuracy: 0.327000\n",
      "lr 9.445829e-08 reg 4.882880e+04 train accuracy: 0.305102 val accuracy: 0.316000\n",
      "lr 9.445829e-08 reg 4.961007e+04 train accuracy: 0.304694 val accuracy: 0.327000\n",
      "lr 9.541322e-08 reg 1.045806e+04 train accuracy: 0.349959 val accuracy: 0.356000\n",
      "lr 9.541322e-08 reg 1.147450e+04 train accuracy: 0.351388 val accuracy: 0.369000\n",
      "lr 9.541322e-08 reg 1.241578e+04 train accuracy: 0.349388 val accuracy: 0.358000\n",
      "lr 9.541322e-08 reg 1.248531e+04 train accuracy: 0.351020 val accuracy: 0.364000\n",
      "lr 9.541322e-08 reg 1.277496e+04 train accuracy: 0.349796 val accuracy: 0.369000\n",
      "lr 9.541322e-08 reg 1.336581e+04 train accuracy: 0.349694 val accuracy: 0.374000\n",
      "lr 9.541322e-08 reg 1.481408e+04 train accuracy: 0.339163 val accuracy: 0.354000\n",
      "lr 9.541322e-08 reg 1.514801e+04 train accuracy: 0.337939 val accuracy: 0.358000\n",
      "lr 9.541322e-08 reg 1.782350e+04 train accuracy: 0.341367 val accuracy: 0.355000\n",
      "lr 9.541322e-08 reg 2.101221e+04 train accuracy: 0.330082 val accuracy: 0.346000\n",
      "lr 9.541322e-08 reg 2.625685e+04 train accuracy: 0.330694 val accuracy: 0.341000\n",
      "lr 9.541322e-08 reg 3.150788e+04 train accuracy: 0.321939 val accuracy: 0.340000\n",
      "lr 9.541322e-08 reg 3.446966e+04 train accuracy: 0.311082 val accuracy: 0.320000\n",
      "lr 9.541322e-08 reg 3.529470e+04 train accuracy: 0.317959 val accuracy: 0.330000\n",
      "lr 9.541322e-08 reg 3.894720e+04 train accuracy: 0.313980 val accuracy: 0.333000\n",
      "lr 9.541322e-08 reg 4.054160e+04 train accuracy: 0.316224 val accuracy: 0.319000\n",
      "lr 9.541322e-08 reg 4.125281e+04 train accuracy: 0.317020 val accuracy: 0.334000\n",
      "lr 9.541322e-08 reg 4.696066e+04 train accuracy: 0.310327 val accuracy: 0.329000\n",
      "lr 9.541322e-08 reg 4.882880e+04 train accuracy: 0.304469 val accuracy: 0.323000\n",
      "lr 9.541322e-08 reg 4.961007e+04 train accuracy: 0.305571 val accuracy: 0.320000\n",
      "lr 9.659906e-08 reg 1.045806e+04 train accuracy: 0.353082 val accuracy: 0.377000\n",
      "lr 9.659906e-08 reg 1.147450e+04 train accuracy: 0.349980 val accuracy: 0.360000\n",
      "lr 9.659906e-08 reg 1.241578e+04 train accuracy: 0.351735 val accuracy: 0.364000\n",
      "lr 9.659906e-08 reg 1.248531e+04 train accuracy: 0.346612 val accuracy: 0.375000\n",
      "lr 9.659906e-08 reg 1.277496e+04 train accuracy: 0.349551 val accuracy: 0.357000\n",
      "lr 9.659906e-08 reg 1.336581e+04 train accuracy: 0.345388 val accuracy: 0.364000\n",
      "lr 9.659906e-08 reg 1.481408e+04 train accuracy: 0.344082 val accuracy: 0.362000\n",
      "lr 9.659906e-08 reg 1.514801e+04 train accuracy: 0.344163 val accuracy: 0.359000\n",
      "lr 9.659906e-08 reg 1.782350e+04 train accuracy: 0.339592 val accuracy: 0.353000\n",
      "lr 9.659906e-08 reg 2.101221e+04 train accuracy: 0.328857 val accuracy: 0.350000\n",
      "lr 9.659906e-08 reg 2.625685e+04 train accuracy: 0.327327 val accuracy: 0.339000\n",
      "lr 9.659906e-08 reg 3.150788e+04 train accuracy: 0.324939 val accuracy: 0.344000\n",
      "lr 9.659906e-08 reg 3.446966e+04 train accuracy: 0.322245 val accuracy: 0.329000\n",
      "lr 9.659906e-08 reg 3.529470e+04 train accuracy: 0.320796 val accuracy: 0.332000\n",
      "lr 9.659906e-08 reg 3.894720e+04 train accuracy: 0.314673 val accuracy: 0.336000\n",
      "lr 9.659906e-08 reg 4.054160e+04 train accuracy: 0.314061 val accuracy: 0.332000\n",
      "lr 9.659906e-08 reg 4.125281e+04 train accuracy: 0.317143 val accuracy: 0.332000\n",
      "lr 9.659906e-08 reg 4.696066e+04 train accuracy: 0.308347 val accuracy: 0.319000\n",
      "lr 9.659906e-08 reg 4.882880e+04 train accuracy: 0.304510 val accuracy: 0.329000\n",
      "lr 9.659906e-08 reg 4.961007e+04 train accuracy: 0.310347 val accuracy: 0.329000\n",
      "lr 1.288419e-07 reg 1.045806e+04 train accuracy: 0.354061 val accuracy: 0.367000\n",
      "lr 1.288419e-07 reg 1.147450e+04 train accuracy: 0.356306 val accuracy: 0.374000\n",
      "lr 1.288419e-07 reg 1.241578e+04 train accuracy: 0.349245 val accuracy: 0.360000\n",
      "lr 1.288419e-07 reg 1.248531e+04 train accuracy: 0.349122 val accuracy: 0.359000\n",
      "lr 1.288419e-07 reg 1.277496e+04 train accuracy: 0.355571 val accuracy: 0.358000\n",
      "lr 1.288419e-07 reg 1.336581e+04 train accuracy: 0.346837 val accuracy: 0.367000\n",
      "lr 1.288419e-07 reg 1.481408e+04 train accuracy: 0.345122 val accuracy: 0.365000\n",
      "lr 1.288419e-07 reg 1.514801e+04 train accuracy: 0.346959 val accuracy: 0.363000\n",
      "lr 1.288419e-07 reg 1.782350e+04 train accuracy: 0.340714 val accuracy: 0.349000\n",
      "lr 1.288419e-07 reg 2.101221e+04 train accuracy: 0.335673 val accuracy: 0.346000\n",
      "lr 1.288419e-07 reg 2.625685e+04 train accuracy: 0.329653 val accuracy: 0.356000\n",
      "lr 1.288419e-07 reg 3.150788e+04 train accuracy: 0.322122 val accuracy: 0.336000\n",
      "lr 1.288419e-07 reg 3.446966e+04 train accuracy: 0.311510 val accuracy: 0.334000\n",
      "lr 1.288419e-07 reg 3.529470e+04 train accuracy: 0.319755 val accuracy: 0.325000\n",
      "lr 1.288419e-07 reg 3.894720e+04 train accuracy: 0.313959 val accuracy: 0.331000\n",
      "lr 1.288419e-07 reg 4.054160e+04 train accuracy: 0.310204 val accuracy: 0.333000\n",
      "lr 1.288419e-07 reg 4.125281e+04 train accuracy: 0.318082 val accuracy: 0.338000\n",
      "lr 1.288419e-07 reg 4.696066e+04 train accuracy: 0.297306 val accuracy: 0.317000\n",
      "lr 1.288419e-07 reg 4.882880e+04 train accuracy: 0.304673 val accuracy: 0.317000\n",
      "lr 1.288419e-07 reg 4.961007e+04 train accuracy: 0.314592 val accuracy: 0.334000\n",
      "lr 1.517340e-07 reg 1.045806e+04 train accuracy: 0.354408 val accuracy: 0.370000\n",
      "lr 1.517340e-07 reg 1.147450e+04 train accuracy: 0.355408 val accuracy: 0.370000\n",
      "lr 1.517340e-07 reg 1.241578e+04 train accuracy: 0.347735 val accuracy: 0.364000\n",
      "lr 1.517340e-07 reg 1.248531e+04 train accuracy: 0.350612 val accuracy: 0.360000\n",
      "lr 1.517340e-07 reg 1.277496e+04 train accuracy: 0.346633 val accuracy: 0.362000\n",
      "lr 1.517340e-07 reg 1.336581e+04 train accuracy: 0.345612 val accuracy: 0.363000\n",
      "lr 1.517340e-07 reg 1.481408e+04 train accuracy: 0.344592 val accuracy: 0.363000\n",
      "lr 1.517340e-07 reg 1.514801e+04 train accuracy: 0.344041 val accuracy: 0.345000\n",
      "lr 1.517340e-07 reg 1.782350e+04 train accuracy: 0.343510 val accuracy: 0.351000\n",
      "lr 1.517340e-07 reg 2.101221e+04 train accuracy: 0.338449 val accuracy: 0.357000\n",
      "lr 1.517340e-07 reg 2.625685e+04 train accuracy: 0.324673 val accuracy: 0.335000\n",
      "lr 1.517340e-07 reg 3.150788e+04 train accuracy: 0.324449 val accuracy: 0.336000\n",
      "lr 1.517340e-07 reg 3.446966e+04 train accuracy: 0.320204 val accuracy: 0.342000\n",
      "lr 1.517340e-07 reg 3.529470e+04 train accuracy: 0.318510 val accuracy: 0.334000\n",
      "lr 1.517340e-07 reg 3.894720e+04 train accuracy: 0.307429 val accuracy: 0.321000\n",
      "lr 1.517340e-07 reg 4.054160e+04 train accuracy: 0.318122 val accuracy: 0.332000\n",
      "lr 1.517340e-07 reg 4.125281e+04 train accuracy: 0.310939 val accuracy: 0.328000\n",
      "lr 1.517340e-07 reg 4.696066e+04 train accuracy: 0.301184 val accuracy: 0.319000\n",
      "lr 1.517340e-07 reg 4.882880e+04 train accuracy: 0.299653 val accuracy: 0.312000\n",
      "lr 1.517340e-07 reg 4.961007e+04 train accuracy: 0.302551 val accuracy: 0.323000\n",
      "lr 2.034389e-07 reg 1.045806e+04 train accuracy: 0.355592 val accuracy: 0.365000\n",
      "lr 2.034389e-07 reg 1.147450e+04 train accuracy: 0.354490 val accuracy: 0.352000\n",
      "lr 2.034389e-07 reg 1.241578e+04 train accuracy: 0.345857 val accuracy: 0.355000\n",
      "lr 2.034389e-07 reg 1.248531e+04 train accuracy: 0.354245 val accuracy: 0.361000\n",
      "lr 2.034389e-07 reg 1.277496e+04 train accuracy: 0.347857 val accuracy: 0.360000\n",
      "lr 2.034389e-07 reg 1.336581e+04 train accuracy: 0.351612 val accuracy: 0.356000\n",
      "lr 2.034389e-07 reg 1.481408e+04 train accuracy: 0.345612 val accuracy: 0.363000\n",
      "lr 2.034389e-07 reg 1.514801e+04 train accuracy: 0.345714 val accuracy: 0.355000\n",
      "lr 2.034389e-07 reg 1.782350e+04 train accuracy: 0.341857 val accuracy: 0.362000\n",
      "lr 2.034389e-07 reg 2.101221e+04 train accuracy: 0.329878 val accuracy: 0.343000\n",
      "lr 2.034389e-07 reg 2.625685e+04 train accuracy: 0.325898 val accuracy: 0.341000\n",
      "lr 2.034389e-07 reg 3.150788e+04 train accuracy: 0.327898 val accuracy: 0.340000\n",
      "lr 2.034389e-07 reg 3.446966e+04 train accuracy: 0.326041 val accuracy: 0.344000\n",
      "lr 2.034389e-07 reg 3.529470e+04 train accuracy: 0.315020 val accuracy: 0.330000\n",
      "lr 2.034389e-07 reg 3.894720e+04 train accuracy: 0.309653 val accuracy: 0.328000\n",
      "lr 2.034389e-07 reg 4.054160e+04 train accuracy: 0.314429 val accuracy: 0.326000\n",
      "lr 2.034389e-07 reg 4.125281e+04 train accuracy: 0.310041 val accuracy: 0.325000\n",
      "lr 2.034389e-07 reg 4.696066e+04 train accuracy: 0.305020 val accuracy: 0.315000\n",
      "lr 2.034389e-07 reg 4.882880e+04 train accuracy: 0.302857 val accuracy: 0.309000\n",
      "lr 2.034389e-07 reg 4.961007e+04 train accuracy: 0.311490 val accuracy: 0.327000\n",
      "lr 2.243843e-07 reg 1.045806e+04 train accuracy: 0.356592 val accuracy: 0.369000\n",
      "lr 2.243843e-07 reg 1.147450e+04 train accuracy: 0.346612 val accuracy: 0.354000\n",
      "lr 2.243843e-07 reg 1.241578e+04 train accuracy: 0.351673 val accuracy: 0.367000\n",
      "lr 2.243843e-07 reg 1.248531e+04 train accuracy: 0.350388 val accuracy: 0.365000\n",
      "lr 2.243843e-07 reg 1.277496e+04 train accuracy: 0.352224 val accuracy: 0.369000\n",
      "lr 2.243843e-07 reg 1.336581e+04 train accuracy: 0.347469 val accuracy: 0.366000\n",
      "lr 2.243843e-07 reg 1.481408e+04 train accuracy: 0.342367 val accuracy: 0.348000\n",
      "lr 2.243843e-07 reg 1.514801e+04 train accuracy: 0.347143 val accuracy: 0.353000\n",
      "lr 2.243843e-07 reg 1.782350e+04 train accuracy: 0.341224 val accuracy: 0.357000\n",
      "lr 2.243843e-07 reg 2.101221e+04 train accuracy: 0.322449 val accuracy: 0.341000\n",
      "lr 2.243843e-07 reg 2.625685e+04 train accuracy: 0.326367 val accuracy: 0.352000\n",
      "lr 2.243843e-07 reg 3.150788e+04 train accuracy: 0.314531 val accuracy: 0.329000\n",
      "lr 2.243843e-07 reg 3.446966e+04 train accuracy: 0.314551 val accuracy: 0.329000\n",
      "lr 2.243843e-07 reg 3.529470e+04 train accuracy: 0.313020 val accuracy: 0.327000\n",
      "lr 2.243843e-07 reg 3.894720e+04 train accuracy: 0.312184 val accuracy: 0.326000\n",
      "lr 2.243843e-07 reg 4.054160e+04 train accuracy: 0.322551 val accuracy: 0.328000\n",
      "lr 2.243843e-07 reg 4.125281e+04 train accuracy: 0.313286 val accuracy: 0.333000\n",
      "lr 2.243843e-07 reg 4.696066e+04 train accuracy: 0.311776 val accuracy: 0.326000\n",
      "lr 2.243843e-07 reg 4.882880e+04 train accuracy: 0.309531 val accuracy: 0.327000\n",
      "lr 2.243843e-07 reg 4.961007e+04 train accuracy: 0.303816 val accuracy: 0.321000\n",
      "lr 2.651908e-07 reg 1.045806e+04 train accuracy: 0.352857 val accuracy: 0.367000\n",
      "lr 2.651908e-07 reg 1.147450e+04 train accuracy: 0.349898 val accuracy: 0.363000\n",
      "lr 2.651908e-07 reg 1.241578e+04 train accuracy: 0.345673 val accuracy: 0.362000\n",
      "lr 2.651908e-07 reg 1.248531e+04 train accuracy: 0.344633 val accuracy: 0.358000\n",
      "lr 2.651908e-07 reg 1.277496e+04 train accuracy: 0.349041 val accuracy: 0.365000\n",
      "lr 2.651908e-07 reg 1.336581e+04 train accuracy: 0.339286 val accuracy: 0.344000\n",
      "lr 2.651908e-07 reg 1.481408e+04 train accuracy: 0.345918 val accuracy: 0.360000\n",
      "lr 2.651908e-07 reg 1.514801e+04 train accuracy: 0.338367 val accuracy: 0.355000\n",
      "lr 2.651908e-07 reg 1.782350e+04 train accuracy: 0.332449 val accuracy: 0.344000\n",
      "lr 2.651908e-07 reg 2.101221e+04 train accuracy: 0.329755 val accuracy: 0.339000\n",
      "lr 2.651908e-07 reg 2.625685e+04 train accuracy: 0.321755 val accuracy: 0.336000\n",
      "lr 2.651908e-07 reg 3.150788e+04 train accuracy: 0.312388 val accuracy: 0.327000\n",
      "lr 2.651908e-07 reg 3.446966e+04 train accuracy: 0.316020 val accuracy: 0.329000\n",
      "lr 2.651908e-07 reg 3.529470e+04 train accuracy: 0.318245 val accuracy: 0.330000\n",
      "lr 2.651908e-07 reg 3.894720e+04 train accuracy: 0.306367 val accuracy: 0.329000\n",
      "lr 2.651908e-07 reg 4.054160e+04 train accuracy: 0.311102 val accuracy: 0.336000\n",
      "lr 2.651908e-07 reg 4.125281e+04 train accuracy: 0.293776 val accuracy: 0.304000\n",
      "lr 2.651908e-07 reg 4.696066e+04 train accuracy: 0.295837 val accuracy: 0.312000\n",
      "lr 2.651908e-07 reg 4.882880e+04 train accuracy: 0.295735 val accuracy: 0.309000\n",
      "lr 2.651908e-07 reg 4.961007e+04 train accuracy: 0.306204 val accuracy: 0.321000\n",
      "lr 2.664440e-07 reg 1.045806e+04 train accuracy: 0.355551 val accuracy: 0.368000\n",
      "lr 2.664440e-07 reg 1.147450e+04 train accuracy: 0.353735 val accuracy: 0.371000\n",
      "lr 2.664440e-07 reg 1.241578e+04 train accuracy: 0.351122 val accuracy: 0.365000\n",
      "lr 2.664440e-07 reg 1.248531e+04 train accuracy: 0.348429 val accuracy: 0.357000\n",
      "lr 2.664440e-07 reg 1.277496e+04 train accuracy: 0.351878 val accuracy: 0.364000\n",
      "lr 2.664440e-07 reg 1.336581e+04 train accuracy: 0.348531 val accuracy: 0.365000\n",
      "lr 2.664440e-07 reg 1.481408e+04 train accuracy: 0.344755 val accuracy: 0.360000\n",
      "lr 2.664440e-07 reg 1.514801e+04 train accuracy: 0.348510 val accuracy: 0.364000\n",
      "lr 2.664440e-07 reg 1.782350e+04 train accuracy: 0.339510 val accuracy: 0.355000\n",
      "lr 2.664440e-07 reg 2.101221e+04 train accuracy: 0.333612 val accuracy: 0.343000\n",
      "lr 2.664440e-07 reg 2.625685e+04 train accuracy: 0.314592 val accuracy: 0.338000\n",
      "lr 2.664440e-07 reg 3.150788e+04 train accuracy: 0.322429 val accuracy: 0.332000\n",
      "lr 2.664440e-07 reg 3.446966e+04 train accuracy: 0.321204 val accuracy: 0.336000\n",
      "lr 2.664440e-07 reg 3.529470e+04 train accuracy: 0.316633 val accuracy: 0.332000\n",
      "lr 2.664440e-07 reg 3.894720e+04 train accuracy: 0.310673 val accuracy: 0.328000\n",
      "lr 2.664440e-07 reg 4.054160e+04 train accuracy: 0.305041 val accuracy: 0.330000\n",
      "lr 2.664440e-07 reg 4.125281e+04 train accuracy: 0.319776 val accuracy: 0.331000\n",
      "lr 2.664440e-07 reg 4.696066e+04 train accuracy: 0.295959 val accuracy: 0.316000\n",
      "lr 2.664440e-07 reg 4.882880e+04 train accuracy: 0.296531 val accuracy: 0.316000\n",
      "lr 2.664440e-07 reg 4.961007e+04 train accuracy: 0.308224 val accuracy: 0.320000\n",
      "lr 3.095607e-07 reg 1.045806e+04 train accuracy: 0.351755 val accuracy: 0.378000\n",
      "lr 3.095607e-07 reg 1.147450e+04 train accuracy: 0.356490 val accuracy: 0.371000\n",
      "lr 3.095607e-07 reg 1.241578e+04 train accuracy: 0.351286 val accuracy: 0.366000\n",
      "lr 3.095607e-07 reg 1.248531e+04 train accuracy: 0.351755 val accuracy: 0.371000\n",
      "lr 3.095607e-07 reg 1.277496e+04 train accuracy: 0.346510 val accuracy: 0.352000\n",
      "lr 3.095607e-07 reg 1.336581e+04 train accuracy: 0.349939 val accuracy: 0.364000\n",
      "lr 3.095607e-07 reg 1.481408e+04 train accuracy: 0.344102 val accuracy: 0.366000\n",
      "lr 3.095607e-07 reg 1.514801e+04 train accuracy: 0.339857 val accuracy: 0.353000\n",
      "lr 3.095607e-07 reg 1.782350e+04 train accuracy: 0.342429 val accuracy: 0.358000\n",
      "lr 3.095607e-07 reg 2.101221e+04 train accuracy: 0.328102 val accuracy: 0.345000\n",
      "lr 3.095607e-07 reg 2.625685e+04 train accuracy: 0.326959 val accuracy: 0.349000\n",
      "lr 3.095607e-07 reg 3.150788e+04 train accuracy: 0.325551 val accuracy: 0.347000\n",
      "lr 3.095607e-07 reg 3.446966e+04 train accuracy: 0.320490 val accuracy: 0.331000\n",
      "lr 3.095607e-07 reg 3.529470e+04 train accuracy: 0.314041 val accuracy: 0.324000\n",
      "lr 3.095607e-07 reg 3.894720e+04 train accuracy: 0.313122 val accuracy: 0.330000\n",
      "lr 3.095607e-07 reg 4.054160e+04 train accuracy: 0.320347 val accuracy: 0.337000\n",
      "lr 3.095607e-07 reg 4.125281e+04 train accuracy: 0.312449 val accuracy: 0.331000\n",
      "lr 3.095607e-07 reg 4.696066e+04 train accuracy: 0.308939 val accuracy: 0.318000\n",
      "lr 3.095607e-07 reg 4.882880e+04 train accuracy: 0.294776 val accuracy: 0.309000\n",
      "lr 3.095607e-07 reg 4.961007e+04 train accuracy: 0.289918 val accuracy: 0.300000\n",
      "lr 3.158080e-07 reg 1.045806e+04 train accuracy: 0.354673 val accuracy: 0.365000\n",
      "lr 3.158080e-07 reg 1.147450e+04 train accuracy: 0.348429 val accuracy: 0.370000\n",
      "lr 3.158080e-07 reg 1.241578e+04 train accuracy: 0.349061 val accuracy: 0.371000\n",
      "lr 3.158080e-07 reg 1.248531e+04 train accuracy: 0.352347 val accuracy: 0.366000\n",
      "lr 3.158080e-07 reg 1.277496e+04 train accuracy: 0.345796 val accuracy: 0.366000\n",
      "lr 3.158080e-07 reg 1.336581e+04 train accuracy: 0.352102 val accuracy: 0.361000\n",
      "lr 3.158080e-07 reg 1.481408e+04 train accuracy: 0.339571 val accuracy: 0.354000\n",
      "lr 3.158080e-07 reg 1.514801e+04 train accuracy: 0.338980 val accuracy: 0.359000\n",
      "lr 3.158080e-07 reg 1.782350e+04 train accuracy: 0.332388 val accuracy: 0.354000\n",
      "lr 3.158080e-07 reg 2.101221e+04 train accuracy: 0.335122 val accuracy: 0.346000\n",
      "lr 3.158080e-07 reg 2.625685e+04 train accuracy: 0.324408 val accuracy: 0.341000\n",
      "lr 3.158080e-07 reg 3.150788e+04 train accuracy: 0.319286 val accuracy: 0.333000\n",
      "lr 3.158080e-07 reg 3.446966e+04 train accuracy: 0.301510 val accuracy: 0.317000\n",
      "lr 3.158080e-07 reg 3.529470e+04 train accuracy: 0.304102 val accuracy: 0.313000\n",
      "lr 3.158080e-07 reg 3.894720e+04 train accuracy: 0.308082 val accuracy: 0.329000\n",
      "lr 3.158080e-07 reg 4.054160e+04 train accuracy: 0.313633 val accuracy: 0.332000\n",
      "lr 3.158080e-07 reg 4.125281e+04 train accuracy: 0.302041 val accuracy: 0.309000\n",
      "lr 3.158080e-07 reg 4.696066e+04 train accuracy: 0.317122 val accuracy: 0.325000\n",
      "lr 3.158080e-07 reg 4.882880e+04 train accuracy: 0.312224 val accuracy: 0.328000\n",
      "lr 3.158080e-07 reg 4.961007e+04 train accuracy: 0.303939 val accuracy: 0.312000\n",
      "lr 3.653960e-07 reg 1.045806e+04 train accuracy: 0.347327 val accuracy: 0.365000\n",
      "lr 3.653960e-07 reg 1.147450e+04 train accuracy: 0.348122 val accuracy: 0.361000\n",
      "lr 3.653960e-07 reg 1.241578e+04 train accuracy: 0.350612 val accuracy: 0.366000\n",
      "lr 3.653960e-07 reg 1.248531e+04 train accuracy: 0.350286 val accuracy: 0.372000\n",
      "lr 3.653960e-07 reg 1.277496e+04 train accuracy: 0.340388 val accuracy: 0.357000\n",
      "lr 3.653960e-07 reg 1.336581e+04 train accuracy: 0.349388 val accuracy: 0.363000\n",
      "lr 3.653960e-07 reg 1.481408e+04 train accuracy: 0.335204 val accuracy: 0.345000\n",
      "lr 3.653960e-07 reg 1.514801e+04 train accuracy: 0.339224 val accuracy: 0.356000\n",
      "lr 3.653960e-07 reg 1.782350e+04 train accuracy: 0.334878 val accuracy: 0.346000\n",
      "lr 3.653960e-07 reg 2.101221e+04 train accuracy: 0.331735 val accuracy: 0.347000\n",
      "lr 3.653960e-07 reg 2.625685e+04 train accuracy: 0.316245 val accuracy: 0.338000\n",
      "lr 3.653960e-07 reg 3.150788e+04 train accuracy: 0.312837 val accuracy: 0.326000\n",
      "lr 3.653960e-07 reg 3.446966e+04 train accuracy: 0.319204 val accuracy: 0.330000\n",
      "lr 3.653960e-07 reg 3.529470e+04 train accuracy: 0.314653 val accuracy: 0.329000\n",
      "lr 3.653960e-07 reg 3.894720e+04 train accuracy: 0.317184 val accuracy: 0.331000\n",
      "lr 3.653960e-07 reg 4.054160e+04 train accuracy: 0.317571 val accuracy: 0.327000\n",
      "lr 3.653960e-07 reg 4.125281e+04 train accuracy: 0.303286 val accuracy: 0.322000\n",
      "lr 3.653960e-07 reg 4.696066e+04 train accuracy: 0.306265 val accuracy: 0.320000\n",
      "lr 3.653960e-07 reg 4.882880e+04 train accuracy: 0.292163 val accuracy: 0.310000\n",
      "lr 3.653960e-07 reg 4.961007e+04 train accuracy: 0.298816 val accuracy: 0.310000\n",
      "lr 4.447280e-07 reg 1.045806e+04 train accuracy: 0.351429 val accuracy: 0.364000\n",
      "lr 4.447280e-07 reg 1.147450e+04 train accuracy: 0.344408 val accuracy: 0.359000\n",
      "lr 4.447280e-07 reg 1.241578e+04 train accuracy: 0.346449 val accuracy: 0.350000\n",
      "lr 4.447280e-07 reg 1.248531e+04 train accuracy: 0.350878 val accuracy: 0.358000\n",
      "lr 4.447280e-07 reg 1.277496e+04 train accuracy: 0.350571 val accuracy: 0.361000\n",
      "lr 4.447280e-07 reg 1.336581e+04 train accuracy: 0.348184 val accuracy: 0.367000\n",
      "lr 4.447280e-07 reg 1.481408e+04 train accuracy: 0.345245 val accuracy: 0.359000\n",
      "lr 4.447280e-07 reg 1.514801e+04 train accuracy: 0.341245 val accuracy: 0.343000\n",
      "lr 4.447280e-07 reg 1.782350e+04 train accuracy: 0.334633 val accuracy: 0.357000\n",
      "lr 4.447280e-07 reg 2.101221e+04 train accuracy: 0.335306 val accuracy: 0.341000\n",
      "lr 4.447280e-07 reg 2.625685e+04 train accuracy: 0.329857 val accuracy: 0.336000\n",
      "lr 4.447280e-07 reg 3.150788e+04 train accuracy: 0.320224 val accuracy: 0.333000\n",
      "lr 4.447280e-07 reg 3.446966e+04 train accuracy: 0.315796 val accuracy: 0.336000\n",
      "lr 4.447280e-07 reg 3.529470e+04 train accuracy: 0.320959 val accuracy: 0.329000\n",
      "lr 4.447280e-07 reg 3.894720e+04 train accuracy: 0.302796 val accuracy: 0.333000\n",
      "lr 4.447280e-07 reg 4.054160e+04 train accuracy: 0.303959 val accuracy: 0.324000\n",
      "lr 4.447280e-07 reg 4.125281e+04 train accuracy: 0.299245 val accuracy: 0.312000\n",
      "lr 4.447280e-07 reg 4.696066e+04 train accuracy: 0.308612 val accuracy: 0.333000\n",
      "lr 4.447280e-07 reg 4.882880e+04 train accuracy: 0.305939 val accuracy: 0.314000\n",
      "lr 4.447280e-07 reg 4.961007e+04 train accuracy: 0.308224 val accuracy: 0.317000\n",
      "lr 4.865481e-07 reg 1.045806e+04 train accuracy: 0.353837 val accuracy: 0.357000\n",
      "lr 4.865481e-07 reg 1.147450e+04 train accuracy: 0.348571 val accuracy: 0.350000\n",
      "lr 4.865481e-07 reg 1.241578e+04 train accuracy: 0.342265 val accuracy: 0.356000\n",
      "lr 4.865481e-07 reg 1.248531e+04 train accuracy: 0.347571 val accuracy: 0.358000\n",
      "lr 4.865481e-07 reg 1.277496e+04 train accuracy: 0.346490 val accuracy: 0.366000\n",
      "lr 4.865481e-07 reg 1.336581e+04 train accuracy: 0.338306 val accuracy: 0.357000\n",
      "lr 4.865481e-07 reg 1.481408e+04 train accuracy: 0.345163 val accuracy: 0.348000\n",
      "lr 4.865481e-07 reg 1.514801e+04 train accuracy: 0.335367 val accuracy: 0.353000\n",
      "lr 4.865481e-07 reg 1.782350e+04 train accuracy: 0.338347 val accuracy: 0.359000\n",
      "lr 4.865481e-07 reg 2.101221e+04 train accuracy: 0.331061 val accuracy: 0.341000\n",
      "lr 4.865481e-07 reg 2.625685e+04 train accuracy: 0.325592 val accuracy: 0.332000\n",
      "lr 4.865481e-07 reg 3.150788e+04 train accuracy: 0.319571 val accuracy: 0.340000\n",
      "lr 4.865481e-07 reg 3.446966e+04 train accuracy: 0.316592 val accuracy: 0.324000\n",
      "lr 4.865481e-07 reg 3.529470e+04 train accuracy: 0.319306 val accuracy: 0.330000\n",
      "lr 4.865481e-07 reg 3.894720e+04 train accuracy: 0.303673 val accuracy: 0.321000\n",
      "lr 4.865481e-07 reg 4.054160e+04 train accuracy: 0.304082 val accuracy: 0.334000\n",
      "lr 4.865481e-07 reg 4.125281e+04 train accuracy: 0.300531 val accuracy: 0.323000\n",
      "lr 4.865481e-07 reg 4.696066e+04 train accuracy: 0.297367 val accuracy: 0.310000\n",
      "lr 4.865481e-07 reg 4.882880e+04 train accuracy: 0.286633 val accuracy: 0.300000\n",
      "lr 4.865481e-07 reg 4.961007e+04 train accuracy: 0.305184 val accuracy: 0.322000\n",
      "best validation accuracy achieved during cross-validation: 0.378000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.random.uniform(low = 0.2e-7, high = 5e-7, size = (20, ))\n",
    "regularization_strengths = np.random.uniform(low = 1e4, high = 5e4, size = (20, ))\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "for i in range(len(learning_rates)):\n",
    "    for j in range(len(regularization_strengths)):\n",
    "        softmax = Softmax()\n",
    "        softmax.train(X_train, y_train, learning_rates[i], regularization_strengths[j], 1500, 200, True)\n",
    "        train_pred = softmax.predict(X_train)\n",
    "        train_acc = np.mean(y_train == train_pred)\n",
    "        val_pred = softmax.predict(X_val)\n",
    "        val_acc = np.mean(y_val == val_pred)\n",
    "        results[(learning_rates[i], regularization_strengths[j])] = (train_acc, val_acc)\n",
    "        if(val_acc > best_val):\n",
    "            best_val = val_acc\n",
    "            best_softmax = softmax\n",
    "pass\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.357000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20380d57ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
